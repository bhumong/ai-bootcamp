{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "450f0833",
   "metadata": {},
   "source": [
    "source: [link](https://ai-bootcamp.ruangguru.com/learn/03_deep-learning/05_pytorch-gradient-descent.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996a8338",
   "metadata": {},
   "source": [
    "# Gradient Descent with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357ffd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(5, requires_grad=True, dtype=torch.float32)\n",
    "# quadratic function\n",
    "y = x**2 + 2*x + 1\n",
    "\n",
    "# plot the function from x = 0 to x = 10\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "x_plt = np.linspace(-10, 10, 100)\n",
    "y_plt = x_plt**2 + 2*x_plt + 1\n",
    "plt.plot(x_plt, y_plt)\n",
    "plt.title('y = x^2 + 2x + 1')\n",
    "\n",
    "# Plot the x, y\n",
    "plt.plot(x.detach().numpy(), y.detach().numpy(), 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4f573d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65ec2a2f",
   "metadata": {},
   "source": [
    "What is the gradient at x = 5?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8234ef14",
   "metadata": {},
   "source": [
    "In the previous notebook, we have learned the hardway:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07b931d",
   "metadata": {},
   "source": [
    "But in PyTorch, we can do it in a much easier way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702cf819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastbook import *\n",
    "\n",
    "gv('''\n",
    "   x -> y[label=\"x^2 + 2x+1\"]\n",
    "   ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d4318c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d65436",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(5, requires_grad=True, dtype=torch.float32)\n",
    "y = x**2 + 2*x + 1\n",
    "\n",
    "# compute gradient\n",
    "y.backward()\n",
    "\n",
    "# print gradient\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783452fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor(12.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4630b552",
   "metadata": {},
   "source": [
    "backward() does backward propagation to calculate the gradient. And the result is stored in x.grad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174b7b7b",
   "metadata": {},
   "source": [
    "Notice the requires_grad=True in the definition of x. It tells PyTorch to calculate the gradient of x during backward propagation. If we don’t set it, x.grad will raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f0a900",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(5, requires_grad=False, dtype=torch.float32)\n",
    "y = x**2 + 2*x + 1\n",
    "\n",
    "# compute gradient\n",
    "y.backward()\n",
    "\n",
    "# Error\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe7ad77",
   "metadata": {},
   "source": [
    "Let’s find the minimum value like before. Remember the steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa635907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly choose x\n",
    "x = torch.tensor(5, requires_grad=True, dtype=torch.float32)\n",
    "alpha = 0.1\n",
    "\n",
    "for i in range(10):\n",
    "    y = x**2 + 2*x + 1\n",
    "    y.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x -= alpha * x.grad\n",
    "        # gradient is accumulated, so we need to zero it\n",
    "        x.grad.zero_()\n",
    "\n",
    "    print(x.detach().numpy(), y.detach().numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8f08a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "3.8 36.0\n",
    "2.84 23.039999\n",
    "2.072 14.7456\n",
    "1.4576 9.437184\n",
    "0.96607995 6.039798\n",
    "0.57286394 3.8654704\n",
    "0.25829116 2.473901\n",
    "0.006632924 1.5832967\n",
    "-0.19469367 1.0133098\n",
    "-0.35575494 0.6485183"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e28e9d6",
   "metadata": {},
   "source": [
    "Let’s visualize the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abd95eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly choose x\n",
    "x = torch.tensor(5, requires_grad=True, dtype=torch.float32)\n",
    "alpha = 0.1\n",
    "\n",
    "x_plt = np.linspace(-10, 10, 100)\n",
    "y_plt = x_plt**2 + 2*x_plt + 1\n",
    "plt.plot(x_plt, y_plt)\n",
    "plt.title('y = x^2 + 2x + 1')\n",
    "\n",
    "# Plot the \n",
    "for i in range(50):\n",
    "    y = x**2 + 2*x + 1\n",
    "    plt.plot(x.detach().numpy(), y.detach().numpy(), 'o')\n",
    "\n",
    "    y.backward()\n",
    "\n",
    "    gradient = None\n",
    "    with torch.no_grad():\n",
    "        gradient = x.grad.item()\n",
    "        x -= alpha * x.grad\n",
    "        # gradient is accumulated, so we need to zero it\n",
    "        x.grad.zero_()\n",
    "\n",
    "    print(x.detach().numpy(), y.detach().numpy(), gradient)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2e42d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "3.8 36.0 12.0\n",
    "2.84 23.039999 9.600000381469727\n",
    "2.072 14.7456 7.679999828338623\n",
    "1.4576 9.437184 6.144000053405762\n",
    "0.96607995 6.039798 4.915200233459473\n",
    "0.57286394 3.8654704 3.932159900665283\n",
    "0.25829116 2.473901 3.1457278728485107\n",
    "0.006632924 1.5832967 2.5165822505950928\n",
    "-0.19469367 1.0133098 2.01326584815979\n",
    "-0.35575494 0.6485183 1.6106126308441162\n",
    "-0.48460394 0.4150517 1.2884900569915771\n",
    "-0.58768314 0.2656331 1.0307921171188354\n",
    "-0.6701465 0.1700052 0.8246337175369263\n",
    "-0.73611724 0.10880327 0.6597069501876831\n",
    "-0.7888938 0.06963408 0.5277655124664307\n",
    "-0.83111507 0.044565797 0.4222123622894287\n",
    "-0.86489207 0.028522134 0.33776986598968506\n",
    "-0.89191365 0.01825416 0.27021586894989014\n",
    "-0.91353095 0.01168263 0.2161726951599121\n",
    "-0.93082476 0.007476926 0.17293810844421387\n",
    "-0.9446598 0.0047852397 0.1383504867553711\n",
    "-0.9557279 0.0030625463 0.11068034172058105\n",
    "-0.9645823 0.0019600391 0.08854424953460693\n",
    "-0.97166586 0.0012544394 0.07083535194396973\n",
    "-0.9773327 0.00080281496 0.05666828155517578\n",
    "-0.9818662 0.00051379204 0.045334577560424805\n",
    "-0.98549294 0.00032883883 0.036267638206481934\n",
    "-0.9883944 0.000210464 0.029014110565185547\n",
    "-0.9907155 0.0001347065 0.023211240768432617\n",
    "-0.9925724 8.6188316e-05 0.018568992614746094\n",
    "-0.99405795 5.51939e-05 0.014855146408081055\n",
    "-0.99524635 3.528595e-05 0.011884093284606934\n",
    "-0.9961971 2.259016e-05 0.009507298469543457\n",
    "-0.99695766 1.4483929e-05 0.007605791091918945\n",
    "-0.9975661 9.23872e-06 0.0060846805572509766\n",
    "-0.9980529 5.90086e-06 0.0048677921295166016\n",
    "-0.9984423 3.8146973e-06 0.003894209861755371\n",
    "-0.99875385 2.4437904e-06 0.003115415573120117\n",
    "-0.99900305 1.5497208e-06 0.0024923086166381836\n",
    "-0.99920243 1.013279e-06 0.001993894577026367\n",
    "-0.99936193 6.556511e-07 0.001595139503479004\n",
    "-0.99948955 4.172325e-07 0.0012761354446411133\n",
    "-0.99959165 2.3841858e-07 0.0010209083557128906\n",
    "-0.9996733 1.7881393e-07 0.0008167028427124023\n",
    "-0.99973863 1.1920929e-07 0.000653386116027832\n",
    "-0.9997909 5.9604645e-08 0.0005227327346801758\n",
    "-0.99983275 5.9604645e-08 0.0004181861877441406\n",
    "-0.9998662 0.0 0.0003345012664794922\n",
    "-0.99989295 0.0 0.0002676248550415039\n",
    "-0.99991435 0.0 0.00021409988403320312"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b04dc3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2082774d",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d87ffe",
   "metadata": {},
   "source": [
    "In the previous example, we have to manually update \\(x\\) with the gradient. But in PyTorch, we can use an optimizer to do it for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07f9fad",
   "metadata": {},
   "source": [
    "torch.optim.SGD is a simple optimizer that does gradient descent. It takes the parameters to optimize and the learning rate as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f5dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly choose x\n",
    "x = torch.tensor(5, requires_grad=True, dtype=torch.float32)\n",
    "alpha = 0.1\n",
    "optimizer = torch.optim.SGD([x], lr=alpha)\n",
    "\n",
    "for i in range(10):\n",
    "    y = x**2 + 2*x + 1\n",
    "    y.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(x.detach().numpy(), y.detach().numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac52eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "3.8 36.0\n",
    "2.84 23.039999\n",
    "2.072 14.7456\n",
    "1.4576 9.437184\n",
    "0.96607995 6.039798\n",
    "0.57286394 3.8654704\n",
    "0.25829116 2.473901\n",
    "0.006632924 1.5832967\n",
    "-0.19469367 1.0133098\n",
    "-0.35575494 0.6485183"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b29f13",
   "metadata": {},
   "source": [
    "We get the same result as before, but with less code (without manually updating \\(x\\))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a5ba41",
   "metadata": {},
   "source": [
    "There are some other optimizers, such as torch.optim.Adam and torch.optim.RMSprop. You can find more in the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1321d05",
   "metadata": {},
   "source": [
    "# Building Simple Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe4772f",
   "metadata": {},
   "source": [
    "We now know how to calculate the gradient of a function and use an optimizer to find the minimum value. We can use this knowledge to build a simple neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9242767c",
   "metadata": {},
   "source": [
    "Let’s build NOT gate with a single neuron, similar to what we have here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d4f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0], [1]], dtype=torch.float32)\n",
    "y = torch.tensor([1, 0], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor([[3]], requires_grad=True, dtype=torch.float32)\n",
    "b = torch.tensor([[1]], requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "a = torch.sigmoid(w@x.T + b)\n",
    "\n",
    "print(\"a = \", a)\n",
    "\n",
    "loss = torch.square(a - y)\n",
    "print(\"loss = \", loss.detach().numpy())\n",
    "\n",
    "loss = torch.mean(loss)\n",
    "print(\"sum loss = \", loss.detach().numpy())\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(\"w_grad = \", w.grad)\n",
    "print(\"b_grad = \", b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ac39b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a =  tensor([[0.7311, 0.9820]], grad_fn=<SigmoidBackward0>)\n",
    "loss =  [[0.07232948 0.96435106]]\n",
    "sum loss =  0.5183403\n",
    "w_grad =  tensor([[0.0173]])\n",
    "b_grad =  tensor([[-0.0355]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49e96c8",
   "metadata": {},
   "source": [
    "That’s similar to what we got in the Google Sheet. Do check it out if you haven’t :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f660e9",
   "metadata": {},
   "source": [
    "Now, we just need to iterate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d39c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([[0], [1]], dtype=torch.float32)\n",
    "y = torch.tensor([1, 0], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor([[3]], requires_grad=True, dtype=torch.float32)\n",
    "b = torch.tensor([[1]], requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "optimizer = torch.optim.SGD([w, b], lr=10)\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"Iteration \", i)\n",
    "    print(\"w = \", w.detach().numpy())\n",
    "    print(\"b = \", b.detach().numpy())\n",
    "\n",
    "    a = torch.sigmoid(w@x.T + b)\n",
    "\n",
    "    print(\"a = \", a)\n",
    "\n",
    "    loss = torch.square(a - y)\n",
    "    print(\"loss = \", loss.detach().numpy())\n",
    "\n",
    "    loss = torch.mean(loss)\n",
    "    print(\"mean loss = \", loss.detach().numpy())\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    print(\"w_grad = \", w.grad)\n",
    "    print(\"b_grad = \", b.grad)\n",
    "\n",
    "    # Update w, b\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d5edaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Iteration  0\n",
    "w =  [[3.]]\n",
    "b =  [[1.]]\n",
    "a =  tensor([[0.7311, 0.9820]], grad_fn=<SigmoidBackward0>)\n",
    "loss =  [[0.07232948 0.96435106]]\n",
    "mean loss =  0.5183403\n",
    "w_grad =  tensor([[0.0173]])\n",
    "b_grad =  tensor([[-0.0355]])\n",
    "\n",
    "Iteration  1\n",
    "w =  [[2.8265495]]\n",
    "b =  [[1.3553205]]\n",
    "a =  tensor([[0.7950, 0.9850]], grad_fn=<SigmoidBackward0>)\n",
    "loss =  [[0.0420258  0.97014576]]\n",
    "mean loss =  0.50608575\n",
    "w_grad =  tensor([[0.0146]])\n",
    "b_grad =  tensor([[-0.0188]])\n",
    "\n",
    "Iteration  2\n",
    "w =  [[2.6806374]]\n",
    "b =  [[1.5435127]]\n",
    "a =  tensor([[0.8240, 0.9856]], grad_fn=<SigmoidBackward0>)\n",
    "loss =  [[0.03098487 0.97135484]]\n",
    "mean loss =  0.50116986\n",
    "w_grad =  tensor([[0.0140]])\n",
    "b_grad =  tensor([[-0.0115]])\n",
    "\n",
    "Iteration  3\n",
    "w =  [[2.5405035]]\n",
    "b =  [[1.6586863]]\n",
    "a =  tensor([[0.8401, 0.9852]], grad_fn=<SigmoidBackward0>)\n",
    "loss =  [[0.02558029 0.970647  ]]\n",
    "mean loss =  0.49811363\n",
    "w_grad =  tensor([[0.0144]])\n",
    "b_grad =  tensor([[-0.0071]])\n",
    "\n",
    "Iteration  4\n",
    "w =  [[2.3969853]]\n",
    "b =  [[1.7300583]]\n",
    "a =  tensor([[0.8494, 0.9841]], grad_fn=<SigmoidBackward0>)\n",
    "loss =  [[0.02267439 0.96850324]]\n",
    "mean loss =  0.4955888\n",
    "w_grad =  tensor([[0.0154]])\n",
    "b_grad =  tensor([[-0.0039]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd440c07",
   "metadata": {},
   "source": [
    "Compare it with the Google Sheet. It’s the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da6a809",
   "metadata": {},
   "source": [
    "# Simpler Way to Build Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d528eb90",
   "metadata": {},
   "source": [
    "Our code looks much simpler than before. But we can make it even simpler with torch.nn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7b48d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([[0], [1]], dtype=torch.float32)\n",
    "y = torch.tensor([1, 0], dtype=torch.float32)\n",
    "\n",
    "class NotGate(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w = torch.nn.Parameter(torch.tensor([[3.]]))\n",
    "        self.b = torch.nn.Parameter(torch.tensor([[1.]]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.w@x.T + self.b)\n",
    "\n",
    "model = NotGate()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=10)\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"Iteration \", i)\n",
    "    print(\"w = \", model.w.detach().numpy())\n",
    "    print(\"b = \", model.b.detach().numpy())\n",
    "\n",
    "    model.train()\n",
    "    a = model(x)\n",
    "\n",
    "    print(\"a = \", a)\n",
    "\n",
    "    loss = torch.square(a - y)\n",
    "    print(\"loss = \", loss.detach().numpy())\n",
    "\n",
    "    loss = torch.mean(loss)\n",
    "    print(\"mean loss = \", loss.detach().numpy())\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    print(\"w_grad = \", model.w.grad)\n",
    "    print(\"b_grad = \", model.b.grad)\n",
    "\n",
    "    # Update w, b\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69af412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Iteration  0\n",
    "w =  [[3.]]\n",
    "b =  [[1.]]\n",
    "a =  tensor([[0.7311, 0.9820]], grad_fn=<SigmoidBackward0>)\n",
    "loss =  [[0.07232948 0.96435106]]\n",
    "mean loss =  0.5183403\n",
    "w_grad =  tensor([[0.0173]])\n",
    "b_grad =  tensor([[-0.0355]])\n",
    "\n",
    "Iteration  1\n",
    "w =  [[2.8265495]]\n",
    "b =  [[1.3553205]]\n",
    "a =  tensor([[0.7950, 0.9850]], grad_fn=<SigmoidBackward0>)\n",
    "loss =  [[0.0420258  0.97014576]]\n",
    "mean loss =  0.50608575\n",
    "w_grad =  tensor([[0.0146]])\n",
    "b_grad =  tensor([[-0.0188]])\n",
    "\n",
    "Iteration  2\n",
    "w =  [[2.6806374]]\n",
    "b =  [[1.5435127]]\n",
    "a =  tensor([[0.8240, 0.9856]], grad_fn=<SigmoidBackward0>)\n",
    "loss =  [[0.03098487 0.97135484]]\n",
    "mean loss =  0.50116986\n",
    "w_grad =  tensor([[0.0140]])\n",
    "b_grad =  tensor([[-0.0115]])\n",
    "\n",
    "Iteration  3\n",
    "w =  [[2.5405035]]\n",
    "b =  [[1.6586863]]\n",
    "a =  tensor([[0.8401, 0.9852]], grad_fn=<SigmoidBackward0>)\n",
    "loss =  [[0.02558029 0.970647  ]]\n",
    "mean loss =  0.49811363\n",
    "w_grad =  tensor([[0.0144]])\n",
    "b_grad =  tensor([[-0.0071]])\n",
    "\n",
    "Iteration  4\n",
    "w =  [[2.3969853]]\n",
    "b =  [[1.7300583]]\n",
    "a =  tensor([[0.8494, 0.9841]], grad_fn=<SigmoidBackward0>)\n",
    "loss =  [[0.02267439 0.96850324]]\n",
    "mean loss =  0.4955888\n",
    "w_grad =  tensor([[0.0154]])\n",
    "b_grad =  tensor([[-0.0039]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f19b4e",
   "metadata": {},
   "source": [
    "# Even Much More Simpler Way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5ba07b",
   "metadata": {},
   "source": [
    "We can make it even simpler with torch.nn.Linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9686e481",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastbook import *\n",
    "\n",
    "gv('''\n",
    "x -> NotGate -> y\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905ef486",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd593190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([[0], [1]], dtype=torch.float32)\n",
    "y = torch.tensor([1, 0], dtype=torch.float32)\n",
    "\n",
    "class NotGate(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(in_features=1, out_features=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))\n",
    "    \n",
    "model = NotGate()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=10)\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"Iteration \", i)\n",
    "    print(\"w = \", model.linear.weight.detach().numpy())\n",
    "    print(\"b = \", model.linear.bias.detach().numpy())\n",
    "\n",
    "    model.train()\n",
    "    a = model(x)\n",
    "\n",
    "    print(\"a = \", a)\n",
    "\n",
    "    loss = torch.square(a - y)\n",
    "    print(\"loss = \", loss.detach().numpy())\n",
    "\n",
    "    loss = torch.mean(loss)\n",
    "    print(\"mean loss = \", loss.detach().numpy())\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    print(\"w_grad = \", model.linear.weight.grad)\n",
    "    print(\"b_grad = \", model.linear.bias.grad)\n",
    "\n",
    "    # Update w, b\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dbd5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Iteration  0\n",
    "w =  [[-0.08132863]]\n",
    "b =  [0.9673029]\n",
    "a =  tensor([[0.7246],\n",
    "        [0.7081]], grad_fn=<SigmoidBackward0>)\n",
    "loss =  [[0.07585529 0.5250185 ]\n",
    " [0.08522972 0.5013471 ]]\n",
    "mean loss =  0.29686266\n",
    "w_grad =  tensor([[0.0430]])\n",
    "b_grad =  tensor([0.0878])\n",
    "\n",
    "Iteration  1\n",
    "w =  [[-0.5114101]]\n",
    "b =  [0.0890395]\n",
    "a =  tensor([[0.5222],\n",
    "        [0.3959]], grad_fn=<SigmoidBackward0>)\n",
    "loss =  [[0.22824968 0.27274   ]\n",
    " [0.3648769  0.15677612]]\n",
    "mean loss =  0.25566065\n",
    "w_grad =  tensor([[-0.0249]])\n",
    "b_grad =  tensor([-0.0193])\n",
    "\n",
    "Iteration  2\n",
    "w =  [[-0.26254913]]\n",
    "b =  [0.28239763]\n",
    "a =  tensor([[0.5701],\n",
    "        [0.5050]], grad_fn=<SigmoidBackward0>)\n",
    "loss =  [[0.18478484 0.32505268]\n",
    " [0.24506265 0.25498658]]\n",
    "mean loss =  0.2524717\n",
    "w_grad =  tensor([[0.0012]])\n",
    "b_grad =  tensor([0.0184])\n",
    "\n",
    "Iteration  3\n",
    "w =  [[-0.27495283]]\n",
    "b =  [0.09810886]\n",
    "a =  tensor([[0.5245],\n",
    "        [0.4559]], grad_fn=<SigmoidBackward0>)\n",
    "loss =  [[0.22609304 0.27510822]\n",
    " [0.29604056 0.20784836]]\n",
    "mean loss =  0.25127253\n",
    "w_grad =  tensor([[-0.0109]])\n",
    "b_grad =  tensor([-0.0048])\n",
    "\n",
    "Iteration  4\n",
    "w =  [[-0.16556998]]\n",
    "b =  [0.14636995]\n",
    "a =  tensor([[0.5365],\n",
    "        [0.4952]], grad_fn=<SigmoidBackward0>)\n",
    "loss =  [[0.21480696 0.28786153]\n",
    " [0.25482288 0.24522316]]\n",
    "mean loss =  0.25067863\n",
    "w_grad =  tensor([[-0.0012]])\n",
    "b_grad =  tensor([0.0079])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7322982d",
   "metadata": {},
   "source": [
    "Here we just need to define the input size and output size. We don’t need to define the weights and bias manually!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cb8df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastbook import *\n",
    "\n",
    "gv('''\n",
    "   x_0[label=3]\n",
    "   x_1[label=5]\n",
    "   a_0_0[label=\"b=8, ReLU\"]\n",
    "   a_0_1[label=\"b=-2, ReLU\"]\n",
    "   a_0_2[label=\"b=4, ReLU\"]\n",
    "   a_1_0[label=\"b=3, ReLU\"]\n",
    "   x_0 -> a_0_0 [label=-2]\n",
    "   x_0 -> a_0_1 [label=5]\n",
    "   x_0 -> a_0_2 [label=3]\n",
    "   x_1 -> a_0_0 [label=8]\n",
    "   x_1 -> a_0_1 [label=-2]\n",
    "   x_1 -> a_0_2 [label=4]\n",
    "   a_0_0 -> a_1_0 [label=3]\n",
    "   a_0_1 -> a_1_0 [label=2]\n",
    "   a_0_2 -> a_1_0 [label=8]\n",
    "   \n",
    "   a_1_0 -> output\n",
    "   ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81edebad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c7d9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(in_features=2, out_features=3)\n",
    "        self.linear2 = torch.nn.Linear(in_features=3, out_features=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.linear1(x))\n",
    "        x = torch.relu(self.linear2(x))\n",
    "        return x\n",
    "\n",
    "model = ComplexNetwork()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "x = torch.tensor([[3, 5]], dtype=torch.float32)\n",
    "y = torch.tensor([[10]], dtype=torch.float32)\n",
    "\n",
    "for i in range(5):\n",
    "    model.train()\n",
    "    y_hat = model(x)\n",
    "\n",
    "    loss = torch.square(y_hat - y)\n",
    "    loss = torch.mean(loss)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(\"Iteration \", i)\n",
    "    print(\"w1 = \", model.linear1.weight.detach().numpy())\n",
    "    print(\"b1 = \", model.linear1.bias.detach().numpy())\n",
    "    print(\"w2 = \", model.linear2.weight.detach().numpy())\n",
    "    print(\"b2 = \", model.linear2.bias.detach().numpy())\n",
    "\n",
    "    print(\"loss = \", loss.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a91e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Iteration  0\n",
    "w1 =  [[-1.3071573  -1.9972436 ]\n",
    " [ 2.1915283   4.3640413 ]\n",
    " [ 0.10451669 -0.33010566]]\n",
    "b1 =  [-0.28740007  0.43799853 -0.08292443]\n",
    "w2 =  [[ 4.097638    4.571283   -0.45574307]]\n",
    "b2 =  [1.728075]\n",
    "loss =  99.424774\n",
    "Iteration  1\n",
    "w1 =  [[-1.30715728e+00 -1.99724364e+00]\n",
    " [-3.36625305e+02 -5.60330750e+02]\n",
    " [ 1.04516685e-01 -3.30105662e-01]]\n",
    "b1 =  [-2.87400067e-01 -1.12500946e+02 -8.29244256e-02]\n",
    "w2 =  [[ 4.0976381e+00 -7.0777679e+02 -4.5574307e-01]]\n",
    "b2 =  [-22.978106]\n",
    "loss =  15259.883\n",
    "Iteration  2\n",
    "w1 =  [[-1.30715728e+00 -1.99724364e+00]\n",
    " [-3.36625305e+02 -5.60330750e+02]\n",
    " [ 1.04516685e-01 -3.30105662e-01]]\n",
    "b1 =  [-2.87400067e-01 -1.12500946e+02 -8.29244256e-02]\n",
    "w2 =  [[ 4.0976381e+00 -7.0777679e+02 -4.5574307e-01]]\n",
    "b2 =  [-22.978106]\n",
    "loss =  100.0\n",
    "Iteration  3\n",
    "w1 =  [[-1.30715728e+00 -1.99724364e+00]\n",
    " [-3.36625305e+02 -5.60330750e+02]\n",
    " [ 1.04516685e-01 -3.30105662e-01]]\n",
    "b1 =  [-2.87400067e-01 -1.12500946e+02 -8.29244256e-02]\n",
    "w2 =  [[ 4.0976381e+00 -7.0777679e+02 -4.5574307e-01]]\n",
    "b2 =  [-22.978106]\n",
    "loss =  100.0\n",
    "Iteration  4\n",
    "w1 =  [[-1.30715728e+00 -1.99724364e+00]\n",
    " [-3.36625305e+02 -5.60330750e+02]\n",
    " [ 1.04516685e-01 -3.30105662e-01]]\n",
    "b1 =  [-2.87400067e-01 -1.12500946e+02 -8.29244256e-02]\n",
    "w2 =  [[ 4.0976381e+00 -7.0777679e+02 -4.5574307e-01]]\n",
    "b2 =  [-22.978106]\n",
    "loss =  100.0"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
