{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e7dfcb9",
   "metadata": {},
   "source": [
    "source: [https://ai-bootcamp.ruangguru.com/learn/03_deep-learning/07_pytorch-application.html](https://ai-bootcamp.ruangguru.com/learn/03_deep-learning/07_pytorch-application.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f60c2d8",
   "metadata": {},
   "source": [
    "# Pytorch Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0148c8",
   "metadata": {},
   "source": [
    "# Simple Line Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa64310",
   "metadata": {},
   "source": [
    "Let’s define a simple Neural Network with one hidden layer and one output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7151fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super().__init__() \n",
    "        self.layer_1 = torch.nn.Linear(n_feature, n_hidden) \n",
    "        self.layer_2 = torch.nn.Linear(n_hidden, n_output) \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer_2(torch.relu(self.layer_1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b4aac2",
   "metadata": {},
   "source": [
    "Tips: to generate numbers from -1 to 1 we can use linspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf3eba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.linspace(-1, 1, 100)\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dee5873",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor([-1.0000, -0.9798, -0.9596, -0.9394, -0.9192, -0.8990, -0.8788, -0.8586,\n",
    "        -0.8384, -0.8182, -0.7980, -0.7778, -0.7576, -0.7374, -0.7172, -0.6970,\n",
    "        -0.6768, -0.6566, -0.6364, -0.6162, -0.5960, -0.5758, -0.5556, -0.5354,\n",
    "        -0.5152, -0.4949, -0.4747, -0.4545, -0.4343, -0.4141, -0.3939, -0.3737,\n",
    "        -0.3535, -0.3333, -0.3131, -0.2929, -0.2727, -0.2525, -0.2323, -0.2121,\n",
    "        -0.1919, -0.1717, -0.1515, -0.1313, -0.1111, -0.0909, -0.0707, -0.0505,\n",
    "        -0.0303, -0.0101,  0.0101,  0.0303,  0.0505,  0.0707,  0.0909,  0.1111,\n",
    "         0.1313,  0.1515,  0.1717,  0.1919,  0.2121,  0.2323,  0.2525,  0.2727,\n",
    "         0.2929,  0.3131,  0.3333,  0.3535,  0.3737,  0.3939,  0.4141,  0.4343,\n",
    "         0.4545,  0.4747,  0.4949,  0.5152,  0.5354,  0.5556,  0.5758,  0.5960,\n",
    "         0.6162,  0.6364,  0.6566,  0.6768,  0.6970,  0.7172,  0.7374,  0.7576,\n",
    "         0.7778,  0.7980,  0.8182,  0.8384,  0.8586,  0.8788,  0.8990,  0.9192,\n",
    "         0.9394,  0.9596,  0.9798,  1.0000])\n",
    "torch.Size([100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d30b4d",
   "metadata": {},
   "source": [
    "However, what we want is (100, 1) shape. Why? Because we want to model 100 samples with 1 feature each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be4712e",
   "metadata": {},
   "source": [
    "So, we need to unsqueeze it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad08d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.unsqueeze(torch.linspace(-1, 1, 100), 1)\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8025b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor([[-1.0000],\n",
    "        [-0.9798],\n",
    "        [-0.9596],\n",
    "        [-0.9394],\n",
    "        [-0.9192],\n",
    "        [-0.8990],\n",
    "        [-0.8788],\n",
    "        [-0.8586],\n",
    "        [-0.8384],\n",
    "        [-0.8182],\n",
    "        [-0.7980],\n",
    "        [-0.7778],\n",
    "        [-0.7576],\n",
    "        [-0.7374],\n",
    "        [-0.7172],\n",
    "        [-0.6970],\n",
    "        [-0.6768],\n",
    "        [-0.6566],\n",
    "        [-0.6364],\n",
    "        [-0.6162],\n",
    "        [-0.5960],\n",
    "        [-0.5758],\n",
    "        [-0.5556],\n",
    "        [-0.5354],\n",
    "        [-0.5152],\n",
    "        [-0.4949],\n",
    "        [-0.4747],\n",
    "        [-0.4545],\n",
    "        [-0.4343],\n",
    "        [-0.4141],\n",
    "        [-0.3939],\n",
    "        [-0.3737],\n",
    "        [-0.3535],\n",
    "        [-0.3333],\n",
    "        [-0.3131],\n",
    "        [-0.2929],\n",
    "        [-0.2727],\n",
    "        [-0.2525],\n",
    "        [-0.2323],\n",
    "        [-0.2121],\n",
    "        [-0.1919],\n",
    "        [-0.1717],\n",
    "        [-0.1515],\n",
    "        [-0.1313],\n",
    "        [-0.1111],\n",
    "        [-0.0909],\n",
    "        [-0.0707],\n",
    "        [-0.0505],\n",
    "        [-0.0303],\n",
    "        [-0.0101],\n",
    "        [ 0.0101],\n",
    "        [ 0.0303],\n",
    "        [ 0.0505],\n",
    "        [ 0.0707],\n",
    "        [ 0.0909],\n",
    "        [ 0.1111],\n",
    "        [ 0.1313],\n",
    "        [ 0.1515],\n",
    "        [ 0.1717],\n",
    "        [ 0.1919],\n",
    "        [ 0.2121],\n",
    "        [ 0.2323],\n",
    "        [ 0.2525],\n",
    "        [ 0.2727],\n",
    "        [ 0.2929],\n",
    "        [ 0.3131],\n",
    "        [ 0.3333],\n",
    "        [ 0.3535],\n",
    "        [ 0.3737],\n",
    "        [ 0.3939],\n",
    "        [ 0.4141],\n",
    "        [ 0.4343],\n",
    "        [ 0.4545],\n",
    "        [ 0.4747],\n",
    "        [ 0.4949],\n",
    "        [ 0.5152],\n",
    "        [ 0.5354],\n",
    "        [ 0.5556],\n",
    "        [ 0.5758],\n",
    "        [ 0.5960],\n",
    "        [ 0.6162],\n",
    "        [ 0.6364],\n",
    "        [ 0.6566],\n",
    "        [ 0.6768],\n",
    "        [ 0.6970],\n",
    "        [ 0.7172],\n",
    "        [ 0.7374],\n",
    "        [ 0.7576],\n",
    "        [ 0.7778],\n",
    "        [ 0.7980],\n",
    "        [ 0.8182],\n",
    "        [ 0.8384],\n",
    "        [ 0.8586],\n",
    "        [ 0.8788],\n",
    "        [ 0.8990],\n",
    "        [ 0.9192],\n",
    "        [ 0.9394],\n",
    "        [ 0.9596],\n",
    "        [ 0.9798],\n",
    "        [ 1.0000]])\n",
    "torch.Size([100, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f865c5",
   "metadata": {},
   "source": [
    "Let’s put them all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3378eb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(1) # make it reproducible\n",
    "\n",
    "# 1. Generate data\n",
    "x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1) # x data (tensor), shape=(100, 1)\n",
    "\n",
    "#  x^2 + 0.2*noise\n",
    "y = x.pow(2) + 0.2*torch.rand(x.size()) # noisy y data (tensor), shape=(100, 1)\n",
    "\n",
    "# 2. Build the network\n",
    "net = Net(n_feature=1, n_hidden=10, n_output=1) \n",
    "print(net) \n",
    "\n",
    "# 3. Train the network\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.2)\n",
    "\n",
    "# Mean square error\n",
    "loss_func = torch.nn.MSELoss() \n",
    "\n",
    "for t in range(200):\n",
    "    prediction = net(x) \n",
    "    loss = loss_func(prediction, y) \n",
    "    optimizer.zero_grad() # clear gradients for next train\n",
    "    loss.backward() # backpropagation, compute gradients\n",
    "    optimizer.step() # apply gradients\n",
    "    \n",
    "    if t % 20 == 0:\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy()) # plot data\n",
    "        plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5) # plot prediction\n",
    "        plt.text(0.5, 0, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 12, 'color': 'red'}) # plot loss\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53437ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Net(\n",
    "  (layer_1): Linear(in_features=1, out_features=10, bias=True)\n",
    "  (layer_2): Linear(in_features=10, out_features=1, bias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b32208",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a9ef940",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b16ab81c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f720c447",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b71d4c93",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0edecb61",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e50dcb60",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76cd211d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38e85528",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bad4392",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "453b8c26",
   "metadata": {},
   "source": [
    "# Validating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd7bfcf",
   "metadata": {},
   "source": [
    "Is it good enough? Does it overfit?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38540432",
   "metadata": {},
   "source": [
    "Let’s split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8251222",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Draw quadratic equations\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(1) # reproducible\n",
    "\n",
    "# 1. Generate data\n",
    "x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1) # x data (tensor), shape=(100, 1)\n",
    "y = x.pow(2) + 0.2*torch.rand(x.size()) # noisy y data (tensor), shape=(100, 1)\n",
    "\n",
    "# Split data into training and testing randomly\n",
    "indices = torch.randperm(x.size(0))\n",
    "x_train = torch.index_select(x, dim=0, index=indices[:80])\n",
    "y_train = torch.index_select(y, dim=0, index=indices[:80])\n",
    "x_test = torch.index_select(x, dim=0, index=indices[80:])\n",
    "y_test = torch.index_select(y, dim=0, index=indices[80:])\n",
    "\n",
    "# 2. Build the network\n",
    "net = Net(n_feature=1, n_hidden=10, n_output=1) # define the network\n",
    "print(net) # net architecture\n",
    "\n",
    "# 3. Train the network\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.2) # define optimizer\n",
    "loss_func = torch.nn.MSELoss() # define loss function\n",
    "\n",
    "for t in range(200):\n",
    "    net.train()\n",
    "    \n",
    "    prediction = net(x_train) # input x and predict based on x\n",
    "    loss = loss_func(prediction, y_train) # must be (1. nn output, 2. target)\n",
    "    optimizer.zero_grad() # clear gradients for next train\n",
    "    loss.backward() # backpropagation, compute gradients\n",
    "    optimizer.step() # apply gradients\n",
    "\n",
    "    if t % 20 == 0:\n",
    "        # plot and show learning process\n",
    "        plt.scatter(x_train.data.numpy(), y_train.data.numpy()) # plot training data\n",
    "        plt.scatter(x_test.data.numpy(), y_test.data.numpy()) # plot testing data\n",
    "\n",
    "        # plot prediction scatter\n",
    "        plt.scatter(x_train.data.numpy(), prediction.data.numpy(), color='red')\n",
    "\n",
    "        net.eval()\n",
    "        # get prediction on test data\n",
    "        with torch.inference_mode():\n",
    "            test_prediction = net(x_test)\n",
    "\n",
    "            # plot prediction scatter\n",
    "            plt.scatter(x_test.data.numpy(), test_prediction.data.numpy(), color='green')\n",
    "            \n",
    "            plt.text(0.5, 0, 'Training loss=%.4f' % loss.data.numpy(), fontdict={'size': 12, 'color': 'red'}) # plot loss\n",
    "            plt.text(0.5, 0.1, 'Testing loss=%.4f' % loss_func(test_prediction, y_test).data.numpy(), fontdict={'size': 12, 'color': 'green'})\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365133b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Net(\n",
    "  (layer_1): Linear(in_features=1, out_features=10, bias=True)\n",
    "  (layer_2): Linear(in_features=10, out_features=1, bias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7eb0ed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dab851e7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c222da0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "741bd4db",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "738f1ed0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb151103",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adaa52ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b76362e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1db3ce8a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b00c5fd0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6d4ee9c",
   "metadata": {},
   "source": [
    "To visualize testing loss and training loss, we usually plot them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec1f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Draw quadratic equations\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(1) # reproducible\n",
    "\n",
    "# 1. Generate data\n",
    "x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1) # x data (tensor), shape=(100, 1)\n",
    "y = x.pow(2) + 0.2*torch.rand(x.size()) # noisy y data (tensor), shape=(100, 1)\n",
    "\n",
    "# Split data into training and testing randomly\n",
    "indices = torch.randperm(x.size(0))\n",
    "x_train = torch.index_select(x, dim=0, index=indices[:80])\n",
    "y_train = torch.index_select(y, dim=0, index=indices[:80])\n",
    "x_test = torch.index_select(x, dim=0, index=indices[80:])\n",
    "y_test = torch.index_select(y, dim=0, index=indices[80:])\n",
    "\n",
    "# 2. Build the network\n",
    "net = Net(n_feature=1, n_hidden=10, n_output=1) # define the network\n",
    "print(net) # net architecture\n",
    "\n",
    "# 3. Train the network\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.2) # define optimizer\n",
    "loss_func = torch.nn.MSELoss() # define loss function\n",
    "\n",
    "training_losses = []\n",
    "testing_losses = []\n",
    "\n",
    "for t in range(200):\n",
    "    net.train()\n",
    "    \n",
    "    prediction = net(x_train) # input x and predict based on x\n",
    "    loss = loss_func(prediction, y_train) # must be (1. nn output, 2. target)\n",
    "    optimizer.zero_grad() # clear gradients for next train\n",
    "    loss.backward() # backpropagation, compute gradients\n",
    "    optimizer.step() # apply gradients\n",
    "\n",
    "    if t % 20 == 0:\n",
    "        net.eval()\n",
    "        with torch.inference_mode():\n",
    "            test_prediction = net(x_test)\n",
    "            test_loss = loss_func(test_prediction, y_test)\n",
    "\n",
    "            print('Training loss=%.4f' % loss.data.numpy())\n",
    "            print('Testing loss=%.4f' % test_loss.data.numpy())\n",
    "            print()\n",
    "\n",
    "            training_losses.append(loss.data.numpy())\n",
    "            testing_losses.append(test_loss.data.numpy())\n",
    "\n",
    "plt.plot(training_losses, label='Training loss')\n",
    "plt.plot(testing_losses, label='Testing loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b60c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "Net(\n",
    "  (layer_1): Linear(in_features=1, out_features=10, bias=True)\n",
    "  (layer_2): Linear(in_features=10, out_features=1, bias=True)\n",
    ")\n",
    "Training loss=0.1544\n",
    "Testing loss=0.0934\n",
    "\n",
    "Training loss=0.0654\n",
    "Testing loss=0.0573\n",
    "\n",
    "Training loss=0.0465\n",
    "Testing loss=0.0388\n",
    "\n",
    "Training loss=0.0306\n",
    "Testing loss=0.0235\n",
    "\n",
    "Training loss=0.0199\n",
    "Testing loss=0.0140\n",
    "\n",
    "Training loss=0.0138\n",
    "Testing loss=0.0092\n",
    "\n",
    "Training loss=0.0107\n",
    "Testing loss=0.0073\n",
    "\n",
    "Training loss=0.0091\n",
    "Testing loss=0.0066\n",
    "\n",
    "Training loss=0.0081\n",
    "Testing loss=0.0064\n",
    "\n",
    "Training loss=0.0076\n",
    "Testing loss=0.0061\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2027a31",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19b852dc",
   "metadata": {},
   "source": [
    "# Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9528a7f9",
   "metadata": {},
   "source": [
    "The indicator of overfitting is when the training loss is decreasing but the testing loss is increasing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92341847",
   "metadata": {},
   "source": [
    "# More Complex Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66766cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make moon from sklearn\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(1) # reproducible\n",
    "\n",
    "# 1. Generate data\n",
    "x, y = make_moons(n_samples=1000, noise=0.2, random_state=1)\n",
    "x = torch.from_numpy(x).type(torch.FloatTensor)\n",
    "y = torch.from_numpy(y).type(torch.LongTensor)\n",
    "\n",
    "# Plot data\n",
    "plt.scatter(x.numpy()[:, 0], x.numpy()[:, 1], c=y.numpy(), s=40, cmap=plt.cm.Spectral)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741f6084",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1b20b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make moon from sklearn\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(1) # reproducible\n",
    "\n",
    "# 1. Generate data\n",
    "x, y = make_moons(n_samples=1000, noise=0.2, random_state=1)\n",
    "x = torch.from_numpy(x).type(torch.FloatTensor)\n",
    "y = torch.from_numpy(y).type(torch.LongTensor)\n",
    "\n",
    "# Split data into training and testing randomly\n",
    "indices = torch.randperm(x.size(0))\n",
    "x_train = torch.index_select(x, dim=0, index=indices[:800])\n",
    "y_train = torch.index_select(y, dim=0, index=indices[:800])\n",
    "x_test = torch.index_select(x, dim=0, index=indices[800:])\n",
    "y_test = torch.index_select(y, dim=0, index=indices[800:])\n",
    "\n",
    "# 2. Build the network\n",
    "net = Net(n_feature=2, n_hidden=5, n_output=2) # define the network\n",
    "\n",
    "# 3. Train the network\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.2) # define optimizer\n",
    "loss_func = torch.nn.CrossEntropyLoss() # define loss function\n",
    "\n",
    "training_losses = []\n",
    "testing_losses = []\n",
    "\n",
    "for t in range(200):\n",
    "    net.train()\n",
    "    \n",
    "    prediction = net(x_train) # input x and predict based on x\n",
    "    loss = loss_func(prediction, y_train) # must be (1. nn output, 2. target)\n",
    "    optimizer.zero_grad() # clear gradients for next train\n",
    "    loss.backward() # backpropagation, compute gradients\n",
    "    optimizer.step() # apply gradients\n",
    "\n",
    "    if t % 20 == 0:\n",
    "        net.eval()\n",
    "        with torch.inference_mode():\n",
    "            test_prediction = net(x_test)\n",
    "            test_loss = loss_func(test_prediction, y_test)\n",
    "\n",
    "            # print('Training loss=%.4f' % loss.data.numpy())\n",
    "            # print('Testing loss=%.4f' % test_loss.data.numpy())\n",
    "            # print()\n",
    "\n",
    "            training_losses.append(loss.data.numpy())\n",
    "            testing_losses.append(test_loss.data.numpy())\n",
    "\n",
    "plt.plot(training_losses, label='Training loss')\n",
    "plt.plot(testing_losses, label='Testing loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(x.numpy()[:, 0], x.numpy()[:, 1], c=y.numpy(), s=40, cmap=plt.cm.Spectral)\n",
    "# Draw model decision boundary\n",
    "x_min, x_max = x[:, 0].min() - 0.5, x[:, 0].max() + 0.5\n",
    "y_min, y_max = x[:, 1].min() - 0.5, x[:, 1].max() + 0.5\n",
    "xx, yy = torch.meshgrid(torch.arange(x_min, x_max, 0.02), torch.arange(y_min, y_max, 0.02))\n",
    "\n",
    "with torch.inference_mode():\n",
    "    Z = net(torch.cat((xx.reshape(-1, 1), yy.reshape(-1, 1)), dim=1))\n",
    "    Z = torch.argmax(Z, dim=1)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24636b1d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c27c5137",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1cbe35c",
   "metadata": {},
   "source": [
    "It seems the model overfits and the training loss is still high. Let’s try to increase the complexity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3804609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make moon from sklearn\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(1) # reproducible\n",
    "\n",
    "# 1. Generate data\n",
    "x, y = make_moons(n_samples=1000, noise=0.2, random_state=1)\n",
    "x = torch.from_numpy(x).type(torch.FloatTensor)\n",
    "y = torch.from_numpy(y).type(torch.LongTensor)\n",
    "\n",
    "# Split data into training and testing randomly\n",
    "indices = torch.randperm(x.size(0))\n",
    "x_train = torch.index_select(x, dim=0, index=indices[:800])\n",
    "y_train = torch.index_select(y, dim=0, index=indices[:800])\n",
    "x_test = torch.index_select(x, dim=0, index=indices[800:])\n",
    "y_test = torch.index_select(y, dim=0, index=indices[800:])\n",
    "\n",
    "# 2. Build the network\n",
    "net = Net(n_feature=2, n_hidden=300, n_output=2) # define the network\n",
    "\n",
    "# 3. Train the network\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.2) # define optimizer\n",
    "loss_func = torch.nn.CrossEntropyLoss() # define loss function\n",
    "\n",
    "training_losses = []\n",
    "testing_losses = []\n",
    "\n",
    "for t in range(200):\n",
    "    net.train()\n",
    "    \n",
    "    prediction = net(x_train) # input x and predict based on x\n",
    "    loss = loss_func(prediction, y_train) # must be (1. nn output, 2. target)\n",
    "    optimizer.zero_grad() # clear gradients for next train\n",
    "    loss.backward() # backpropagation, compute gradients\n",
    "    optimizer.step() # apply gradients\n",
    "\n",
    "    if t % 20 == 0:\n",
    "        net.eval()\n",
    "        with torch.inference_mode():\n",
    "            test_prediction = net(x_test)\n",
    "            test_loss = loss_func(test_prediction, y_test)\n",
    "\n",
    "            # print('Training loss=%.4f' % loss.data.numpy())\n",
    "            # print('Testing loss=%.4f' % test_loss.data.numpy())\n",
    "            # print()\n",
    "\n",
    "            training_losses.append(loss.data.numpy())\n",
    "            testing_losses.append(test_loss.data.numpy())\n",
    "\n",
    "plt.plot(training_losses, label='Training loss')\n",
    "plt.plot(testing_losses, label='Testing loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(x.numpy()[:, 0], x.numpy()[:, 1], c=y.numpy(), s=40, cmap=plt.cm.Spectral)\n",
    "# Draw model decision boundary\n",
    "x_min, x_max = x[:, 0].min() - 0.5, x[:, 0].max() + 0.5\n",
    "y_min, y_max = x[:, 1].min() - 0.5, x[:, 1].max() + 0.5\n",
    "xx, yy = torch.meshgrid(torch.arange(x_min, x_max, 0.02), torch.arange(y_min, y_max, 0.02))\n",
    "\n",
    "with torch.inference_mode():\n",
    "    Z = net(torch.cat((xx.reshape(-1, 1), yy.reshape(-1, 1)), dim=1))\n",
    "    Z = torch.argmax(Z, dim=1)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1e55b6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "412afaa3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bf4f23b",
   "metadata": {},
   "source": [
    "# Pre-trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5c9c2b",
   "metadata": {},
   "source": [
    "Pytorch has a lot of pre-trained models. We can use them for transfer learning. We will learn more about transfer learning in the next session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0010ce",
   "metadata": {},
   "source": [
    "But let’s see one of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed2ab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "resnet = models.resnet101(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8898e12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "/Users/ruangguru/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
    "  warnings.warn(\n",
    "/Users/ruangguru/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
    "  warnings.warn(msg)\n",
    "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /Users/ruangguru/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
    "100%|██████████| 171M/171M [06:37<00:00, 450kB/s]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef8adaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9809b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet(\n",
    "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (relu): ReLU(inplace=True)\n",
    "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
    "  (layer1): Sequential(\n",
    "    (0): Bottleneck(\n",
    "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "      (downsample): Sequential(\n",
    "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      )\n",
    "    )\n",
    "    (1): Bottleneck(\n",
    "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "    )\n",
    "    (2): Bottleneck(\n",
    "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "    )\n",
    "  )\n",
    "  (layer2): Sequential(\n",
    "    (0): Bottleneck(\n",
    "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "      (downsample): Sequential(\n",
    "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      )\n",
    "    )\n",
    "    (1): Bottleneck(\n",
    "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "    )\n",
    "    (2): Bottleneck(\n",
    "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "    )\n",
    "    (3): Bottleneck(\n",
    "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "    )\n",
    "  )\n",
    "  (layer3): Sequential(\n",
    "    (0): Bottleneck(\n",
    "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "      (downsample): Sequential(\n",
    "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      )\n",
    "    )\n",
    "    (1): Bottleneck(\n",
    "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "    )\n",
    "    (2): Bottleneck(\n",
    "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "    )\n",
    "    (3): Bottleneck(\n",
    "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "    )\n",
    "    (4): Bottleneck(\n",
    "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "    )\n",
    "    (5): Bottleneck(\n",
    "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "    )\n",
    "    (6): Bottleneck(\n",
    "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "    )\n",
    "    (7): Bottleneck(\n",
    "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "    )\n",
    "    (8): Bottleneck(\n",
    "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "    )\n",
    "    (9): Bottleneck(\n",
    "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "    )\n",
    "    (10): Bottleneck(\n",
    "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "    )\n",
    "    (11): Bottleneck(\n",
    "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "    )\n",
    "    (12): Bottleneck(\n",
    "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "    )\n",
    "    (13): Bottleneck(\n",
    "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "    )\n",
    "    (14): Bottleneck(\n",
    "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "    )\n",
    "    (15): Bottleneck(\n",
    "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "    )\n",
    "    (16): Bottleneck(\n",
    "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "    )\n",
    "    (17): Bottleneck(\n",
    "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "    )\n",
    "    (18): Bottleneck(\n",
    "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "    )\n",
    "    (19): Bottleneck(\n",
    "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "    )\n",
    "    (20): Bottleneck(\n",
    "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "    )\n",
    "    (21): Bottleneck(\n",
    "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "    )\n",
    "    (22): Bottleneck(\n",
    "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "    )\n",
    "  )\n",
    "  (layer4): Sequential(\n",
    "    (0): Bottleneck(\n",
    "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "      (downsample): Sequential(\n",
    "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      )\n",
    "    )\n",
    "    (1): Bottleneck(\n",
    "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "    )\n",
    "    (2): Bottleneck(\n",
    "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "    )\n",
    "  )\n",
    "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220c7eaa",
   "metadata": {},
   "source": [
    "# Saving & Loading Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bf1ca6",
   "metadata": {},
   "source": [
    "Model that we have trained can be saved and loaded later. So that we don’t need to train it again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0832f475",
   "metadata": {},
   "source": [
    "Let’s try to save and load the model that we have trained before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf048149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save net model\n",
    "torch.save(net, 'my_net.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346dede5",
   "metadata": {},
   "source": [
    "The saved file can be distributed to other people, or even deployed to production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b115509",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_net = torch.load('my_net.pkl')\n",
    "print(loaded_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb61c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "Net(\n",
    "  (layer_1): Linear(in_features=1, out_features=10, bias=True)\n",
    "  (layer_2): Linear(in_features=10, out_features=1, bias=True)\n",
    ")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
