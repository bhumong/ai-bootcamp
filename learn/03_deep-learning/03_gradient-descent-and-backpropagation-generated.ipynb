{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "059be304",
   "metadata": {},
   "source": [
    "source: [https://ai-bootcamp.ruangguru.com/learn/03_deep-learning/03_gradient-descent-and-backpropagation.html](https://ai-bootcamp.ruangguru.com/learn/03_deep-learning/03_gradient-descent-and-backpropagation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e699d7c",
   "metadata": {},
   "source": [
    "# Gradient Descend and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5d5541",
   "metadata": {},
   "source": [
    "In the previous notebook, we have learned how to compute tangent lines at certain points. But what is it for?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b5a83c",
   "metadata": {},
   "source": [
    "In this notebook, we will learn how to use tangent lines to find the minimum of a function. This is called gradient descend. We will also learn how to use gradient descend to train a neural network. This is called backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982cf930",
   "metadata": {},
   "source": [
    "Let’s say we have a quadratic equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ade2eae",
   "metadata": {},
   "source": [
    "\n",
    "\\[f(x) = x^2 - 6x + 5\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa5f6e8",
   "metadata": {},
   "source": [
    "What is the minimum of this function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111684c1",
   "metadata": {},
   "source": [
    "Let’s plot it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72b00f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot x^2 - 6x + 5\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the function\n",
    "x = np.linspace(-10, 16, 100)\n",
    "y = x**2 - 6*x + 5\n",
    "plt.plot(x, y)\n",
    "# add axe label\n",
    "plt.xlabel('bias')\n",
    "plt.ylabel('cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d3515b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58d6def5",
   "metadata": {},
   "source": [
    "The minimum is at around $x = 3$, isn’t it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8281de0e",
   "metadata": {},
   "source": [
    "Let’s try to find the exact minimum value using gradient descend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aadef56",
   "metadata": {},
   "source": [
    "If you observed the graph carefully, you will notice that the tangent line at the minimum point is horizontal. This means that the slope of the tangent line is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6092f5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot x^2 - 6x + 5\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the function\n",
    "x = np.linspace(-10, 16, 100)\n",
    "y = x**2 - 6*x + 5\n",
    "plt.plot(x, y)\n",
    "\n",
    "# Draw y = 6\n",
    "y2 = [-4 for i in x]\n",
    "plt.plot(x, y2, 'r--')\n",
    "\n",
    "# Draw dot at x = 3\n",
    "plt.plot(3, -4, 'ro')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94390014",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdb84e9d",
   "metadata": {},
   "source": [
    "The idea is: - Start with a random value of $x$ - Compute the tangent line at that point - Move $x$ to the left or right, depending on the slope of the tangent line - Repeat until the slope is close to 0 - Done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f66ed24",
   "metadata": {},
   "source": [
    "Let’s start with $x = 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4d5728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot x^2 - 6x + 5\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(x):\n",
    "    return x**2 - 6*x + 5\n",
    "\n",
    "def f_derivative(x):\n",
    "    return 2*x - 6\n",
    "\n",
    "# Plot the function\n",
    "x = np.linspace(0, 16, 100)\n",
    "y = x**2 - 6*x + 5\n",
    "plt.plot(x, y)\n",
    "\n",
    "# Draw tangent line at x = 10\n",
    "x_0 = 10\n",
    "y_0 = f(x_0)\n",
    "tan = f_derivative(x_0)*(x - x_0) + y_0\n",
    "plt.plot(x, tan, 'r--')\n",
    "\n",
    "# Plot dot at x_0 and y_0\n",
    "plt.scatter(x_0, y_0, c='r')\n",
    "# add label (x, y)\n",
    "plt.text(x_0, y_0+1, '({}, {})'.format(x_0, y_0))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0782c2db",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5c4ce57",
   "metadata": {},
   "source": [
    "The derivative of $f(x)$ is $f'(x) = 2x - 6$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddd5a19",
   "metadata": {},
   "source": [
    "At $x = 10$, the slope of the tangent line is $f'(10) = 2(10) - 6 = 14$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a731b388",
   "metadata": {},
   "source": [
    "Now imagine the blue line as a hill, and we are standing at $x = 10$. The slope of the tangent line tells us the direction of the steepest uphill. So we should move to the left."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524afb53",
   "metadata": {},
   "source": [
    "\n",
    "\\[\n",
    "x = x - \\alpha f'(x) \\\\\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b599b2e1",
   "metadata": {},
   "source": [
    "$\\alpha$ is a small number, called the learning rate. It tells us how big the step should be. Let’s say $\\alpha = 0.1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6274281f",
   "metadata": {},
   "source": [
    "And the operation is $-$ because we want to negate the slope (going down)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4578be8",
   "metadata": {},
   "source": [
    "\n",
    "\\[\n",
    "x = x - 0.1 f'(x) \\\\\n",
    "x = 10 - 0.1(14) \\\\\n",
    "x = 8.6\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98213e39",
   "metadata": {},
   "source": [
    "Now, let’s plot again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec3793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot x^2 - 6x + 5\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(x):\n",
    "    return x**2 - 6*x + 5\n",
    "\n",
    "def f_derivative(x):\n",
    "    return 2*x - 6\n",
    "\n",
    "# Plot the function\n",
    "x = np.linspace(0, 16, 100)\n",
    "y = x**2 - 6*x + 5\n",
    "plt.plot(x, y)\n",
    "\n",
    "# Draw tangent line at x = 10\n",
    "x_0 = 8.6\n",
    "y_0 = f(x_0)\n",
    "tan = f_derivative(x_0)*(x - x_0) + y_0\n",
    "plt.plot(x, tan, 'r--')\n",
    "\n",
    "# Plot dot at x_0 and y_0\n",
    "plt.scatter(x_0, y_0, c='r')\n",
    "# add label (x, y)\n",
    "plt.text(x_0, y_0+1, '({}, {})'.format(x_0, y_0))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4527929",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b5cfd47",
   "metadata": {},
   "source": [
    "It’s getting closer to the minimum, isn’t it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2cbab2",
   "metadata": {},
   "source": [
    "Let’s do it again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98392ae",
   "metadata": {},
   "source": [
    "\n",
    "\\[\n",
    "f'(x) = 2x - 6 \\\\\n",
    "f'(8.6) = 2(8.6) - 6 = 11.2 \\\\\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ead91f",
   "metadata": {},
   "source": [
    "\n",
    "\\[\n",
    "x = x - 0.1 f'(x) \\\\\n",
    "x = 8.6 - 0.1(11.2) \\\\\n",
    "x = 7.48\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f088c83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot x^2 - 6x + 5\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(x):\n",
    "    return x**2 - 6*x + 5\n",
    "\n",
    "def f_derivative(x):\n",
    "    return 2*x - 6\n",
    "\n",
    "# Plot the function\n",
    "x = np.linspace(0, 16, 100)\n",
    "y = x**2 - 6*x + 5\n",
    "plt.plot(x, y)\n",
    "\n",
    "# Draw tangent line at x = 10\n",
    "x_0 = 7.48\n",
    "y_0 = f(x_0)\n",
    "tan = f_derivative(x_0)*(x - x_0) + y_0\n",
    "plt.plot(x, tan, 'r--')\n",
    "\n",
    "# Plot dot at x_0 and y_0\n",
    "plt.scatter(x_0, y_0, c='r')\n",
    "# add label (x, y)\n",
    "plt.text(x_0, y_0+1, '({}, {})'.format(x_0, y_0))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decba52b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03a555d6",
   "metadata": {},
   "source": [
    "If we keep doing this, we will eventually reach close to the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d602b605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot x^2 - 6x + 5\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(x):\n",
    "    return x**2 - 6*x + 5\n",
    "\n",
    "def f_derivative(x):\n",
    "    return 2*x - 6\n",
    "\n",
    "# Plot the function\n",
    "x = np.linspace(0, 16, 100)\n",
    "y = x**2 - 6*x + 5\n",
    "plt.plot(x, y)\n",
    "\n",
    "x_0 = 10\n",
    "y_0 = f(x_0)\n",
    "alpha = 0.1\n",
    "for i in range(20):\n",
    "    plt.scatter(x_0, y_0, c='r')\n",
    "\n",
    "    x_next = x_0 - alpha*f_derivative(x_0)\n",
    "    y_next = f(x_next)\n",
    "\n",
    "    # draw line with arrow from (x_0, y_0) to (x_next, y_next)\n",
    "    plt.annotate('', xy=(x_next, y_next), xytext=(x_0, y_0), arrowprops={'arrowstyle': '->', 'color': 'r', 'lw': 1},\n",
    "                 va='center', ha='center')\n",
    "    x_0 = x_next\n",
    "    y_0 = y_next\n",
    "\n",
    "plt.title('Gradient Descent with alpha = ' + str(alpha))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7581ddfc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7239d4f7",
   "metadata": {},
   "source": [
    "What if we change the learning rate to 0.01?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b3a2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot x^2 - 6x + 5\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(x):\n",
    "    return x**2 - 6*x + 5\n",
    "\n",
    "def f_derivative(x):\n",
    "    return 2*x - 6\n",
    "\n",
    "# Plot the function\n",
    "x = np.linspace(0, 16, 100)\n",
    "y = x**2 - 6*x + 5\n",
    "plt.plot(x, y)\n",
    "\n",
    "x_0 = 10\n",
    "y_0 = f(x_0)\n",
    "alpha = 0.01\n",
    "for i in range(200):\n",
    "    plt.scatter(x_0, y_0, c='r')\n",
    "\n",
    "    x_next = x_0 - alpha*f_derivative(x_0)\n",
    "    y_next = f(x_next)\n",
    "\n",
    "    # draw line with arrow from (x_0, y_0) to (x_next, y_next)\n",
    "    plt.annotate('', xy=(x_next, y_next), xytext=(x_0, y_0), arrowprops={'arrowstyle': '->', 'color': 'r', 'lw': 1},\n",
    "                 va='center', ha='center')\n",
    "    x_0 = x_next\n",
    "    y_0 = y_next\n",
    "\n",
    "plt.title('Gradient Descent with alpha = ' + str(alpha))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7f622a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97794bbe",
   "metadata": {},
   "source": [
    "It learns slower! It’s like climbing down a hill with smaller steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb3335b",
   "metadata": {},
   "source": [
    "So why not just set the learning rate to high number? Isn’t it supposed to be faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a1af84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot x^2 - 6x + 5\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(x):\n",
    "    return x**2 - 6*x + 5\n",
    "\n",
    "def f_derivative(x):\n",
    "    return 2*x - 6\n",
    "\n",
    "# Plot the function\n",
    "x = np.linspace(0, 16, 100)\n",
    "y = x**2 - 6*x + 5\n",
    "plt.plot(x, y)\n",
    "\n",
    "x_0 = 10\n",
    "y_0 = f(x_0)\n",
    "alpha = 0.8\n",
    "for i in range(9):\n",
    "    plt.scatter(x_0, y_0, c='r')\n",
    "\n",
    "    x_next = x_0 - alpha*f_derivative(x_0)\n",
    "    y_next = f(x_next)\n",
    "\n",
    "    # draw line with arrow from (x_0, y_0) to (x_next, y_next)\n",
    "    plt.annotate('', xy=(x_next, y_next), xytext=(x_0, y_0), arrowprops={'arrowstyle': '->', 'color': 'r', 'lw': 1},\n",
    "                 va='center', ha='center')\n",
    "    x_0 = x_next\n",
    "    y_0 = y_next\n",
    "\n",
    "plt.title('Gradient Descent with alpha = ' + str(alpha))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39df12e7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8034fc19",
   "metadata": {},
   "source": [
    "See, higher alpha means faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955bcad0",
   "metadata": {},
   "source": [
    "…doesn’t it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa565696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot x^2 - 6x + 5\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(x):\n",
    "    return x**2 - 6*x + 5\n",
    "\n",
    "def f_derivative(x):\n",
    "    return 2*x - 6\n",
    "\n",
    "# Plot the function\n",
    "x = np.linspace(-1000, 2000, 100)\n",
    "y = x**2 - 6*x + 5\n",
    "plt.plot(x, y)\n",
    "\n",
    "x_0 = 10\n",
    "y_0 = f(x_0)\n",
    "alpha = 1.5\n",
    "for i in range(9):\n",
    "    plt.scatter(x_0, y_0, c='r')\n",
    "\n",
    "    x_next = x_0 - alpha*f_derivative(x_0)\n",
    "    y_next = f(x_next)\n",
    "\n",
    "    # draw line with arrow from (x_0, y_0) to (x_next, y_next)\n",
    "    plt.annotate('', xy=(x_next, y_next), xytext=(x_0, y_0), arrowprops={'arrowstyle': '->', 'color': 'r', 'lw': 1},\n",
    "                 va='center', ha='center')\n",
    "    x_0 = x_next\n",
    "    y_0 = y_next\n",
    "\n",
    "plt.title('Gradient Descent with alpha = ' + str(alpha))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5c35c6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bcbf8d6",
   "metadata": {},
   "source": [
    "It’s too high! It’s jumping around the minimum point. It will never reach the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837a03f6",
   "metadata": {},
   "source": [
    "So what is the best learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f13863d",
   "metadata": {},
   "source": [
    "It’s quite experimental. You have to try different values and see which one works best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470fabf4",
   "metadata": {},
   "source": [
    "Some algorithms can automatically adjust the learning rate. It starts with a high learning rate, and then gradually decreases it as it gets closer to the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80ce28e",
   "metadata": {},
   "source": [
    "This is what gradient descend is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88940555",
   "metadata": {},
   "source": [
    "In Neural Network, the main function is the loss function. We want to minimize the loss function. So we use gradient descend to find the minimum of the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39ff600",
   "metadata": {},
   "source": [
    "Let’s say we have a simple neural network with 1 input and 1 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00be8ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastbook import *\n",
    "\n",
    "\n",
    "# Draw neurons with multiple inputs and weights\n",
    "gv('''\n",
    "z[shape=box3d width=1 height=0.7]\n",
    "bias[shape=circle width=0.3]\n",
    "// Subgraph to force alignment on x-axis\n",
    "subgraph {\n",
    "    rank=same;\n",
    "    z;\n",
    "    bias;\n",
    "    alignmentNode [style=invis, width=0]; // invisible node for alignment\n",
    "    bias -> alignmentNode [style=invis]; // invisible edge\n",
    "    z -> alignmentNode [style=invis]; // invisible edge\n",
    "}\n",
    "x_0->z [label=\"w_0\"]\n",
    "bias->z [label=\"b\" pos=\"0,1.2!\"]\n",
    "z->a [label=\"z = w_0 x_0 + b\"]\n",
    "a->output [label=\"a = g(z)\"]\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1573cea3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8784cd02",
   "metadata": {},
   "source": [
    "And the Cost Function is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3846ea",
   "metadata": {},
   "source": [
    "\n",
    "\\[\n",
    "C = (y - \\hat{y})^2\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c54dca",
   "metadata": {},
   "source": [
    "Let’s plot the Cost Function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f411372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Create y = 3x + 1\n",
    "x = np.linspace(-5, 8, 100)\n",
    "relu = lambda x: np.maximum(0, x)\n",
    "\n",
    "\n",
    "# set y = 3x + 1 with noise, random distribution\n",
    "y = 3*x + 1 + np.random.normal(0, 10, 100)\n",
    "\n",
    "# y = 3*x + 1 + np.random.normal(0, 10, 100)\n",
    "\n",
    "\n",
    "# Initialize lists to store values of w_0, b, and cost\n",
    "w_0_list = []\n",
    "b_list = []\n",
    "cost_list = []\n",
    "\n",
    "# Loop over values of w_0 and b\n",
    "for w_0 in range(-10, 10):\n",
    "    for b in range(-10, 10):\n",
    "        y_hat = relu(w_0*x + b)\n",
    "\n",
    "        # Calculate cost function\n",
    "        cost = np.sum((y_hat - y)**2)\n",
    "\n",
    "        # Append values to lists\n",
    "        w_0_list.append(w_0)\n",
    "        b_list.append(b)\n",
    "        cost_list.append(cost)\n",
    "\n",
    "# Reshape cost_list into a 2D array\n",
    "cost_array = np.array(cost_list).reshape((20, 20))\n",
    "\n",
    "# Plot cost function in 3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel('w_0')\n",
    "ax.set_ylabel('b')\n",
    "ax.set_zlabel('cost')\n",
    "ax.plot_surface(np.array(w_0_list).reshape((20, 20)), np.array(b_list).reshape((20, 20)), cost_array, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7e7fee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2faf78fa",
   "metadata": {},
   "source": [
    "The gradient descent algorithm should find the minimum of the Cost Function. It looks different from the previous example (2D), but the idea is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cb0a3e",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e024ff06",
   "metadata": {},
   "source": [
    "In the previous example, how do we calculate the derivative of the Cost Function? We can use the chain rule. But before that, let’s trace the flow of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4708e06",
   "metadata": {},
   "source": [
    "The input is $x$, and the output is $\\hat{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bce59c3",
   "metadata": {},
   "source": [
    "The output is calculated by:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83406273",
   "metadata": {},
   "source": [
    "\n",
    "\\[\n",
    "z = w_0x + b \\\\\n",
    "a = ReLU(z) \\\\\n",
    "\\hat{y} = a \\\\\n",
    "C = (y - \\hat{y})^2\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8a1b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastbook import *\n",
    "\n",
    "\n",
    "# Draw neurons with multiple inputs and weights\n",
    "gv('''\n",
    "z[shape=box3d width=1 height=0.7]\n",
    "bias[shape=circle width=0.3]\n",
    "// Subgraph to force alignment on x-axis\n",
    "subgraph {\n",
    "    rank=same;\n",
    "    z;\n",
    "    bias;\n",
    "    alignmentNode [style=invis, width=0]; // invisible node for alignment\n",
    "    bias -> alignmentNode [style=invis]; // invisible edge\n",
    "    z -> alignmentNode [style=invis]; // invisible edge\n",
    "}\n",
    "x_0->z [label=\"w_0\"]\n",
    "bias->z [label=\"b\" pos=\"0,1.2!\"]\n",
    "z->a [label=\"z = w_0 x_0 + b\"]\n",
    "a->output [label=\"a = g(z)\"]\n",
    "output->Cost [label=\"C = (a - y)^2\"]\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbaf3f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "568a5f2d",
   "metadata": {},
   "source": [
    "What we want to know is how much the Cost Function changes when we change $w_0$ and $b$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca2b4c7",
   "metadata": {},
   "source": [
    "- \\(\\frac{dC}{dw_0}\\)\n",
    "- \\(\\frac{dC}{db}\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5469f2a",
   "metadata": {},
   "source": [
    "How do we calculate it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f406d8",
   "metadata": {},
   "source": [
    "It looks scary, but it’s actually quite simple. It’s just reversing the flow of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b7e52e",
   "metadata": {},
   "source": [
    "Let’s start with $\\frac{dC}{dw_0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3341b1c4",
   "metadata": {},
   "source": [
    "\n",
    "\\[\n",
    "\\frac{dC}{dw_0} = \\frac{dC}{d\\hat{y}} \\frac{d\\hat{y}}{da} \\frac{da}{dz} \\frac{dz}{dw_0} \\\\\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c654e05f",
   "metadata": {},
   "source": [
    "First, $\\frac{dz}{dw_0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c783c64e",
   "metadata": {},
   "source": [
    "\n",
    "\\[\n",
    "z = w_0x + b \\\\\n",
    "\\frac{dz}{dw_0} = x\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b092bc7",
   "metadata": {},
   "source": [
    "Next, $\\frac{da}{dz}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bf08aa",
   "metadata": {},
   "source": [
    "\n",
    "\\[\n",
    "a = ReLU(z) \\\\\n",
    "\\frac{da}{dz} = \\begin{cases}\n",
    "1, & \\text{if $z > 0$} \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a4acfd",
   "metadata": {},
   "source": [
    "Next, $\\frac{d\\hat{y}}{da}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225fc66d",
   "metadata": {},
   "source": [
    "\n",
    "\\[\n",
    "\\hat{y} = a \\\\\n",
    "\\frac{d\\hat{y}}{da} = 1\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dffe12",
   "metadata": {},
   "source": [
    "Finally, $\\frac{dC}{d\\hat{y}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa17005",
   "metadata": {},
   "source": [
    "\n",
    "\\[\n",
    "C = (y - \\hat{y})^2 \\\\\n",
    "\\frac{dC}{d\\hat{y}} = \\frac{d(y^2 - 2y\\hat{y} + \\hat{y}^2)}{d\\hat{y}} = -2y + 2\\hat{y}  = 2(\\hat{y} - y)\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481b042e",
   "metadata": {},
   "source": [
    "Now, let’s find out how much the Cost Function changes when we change $b$: $\\frac{dC}{db}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a220a631",
   "metadata": {},
   "source": [
    "\n",
    "\\[\n",
    "\\frac{dC}{db} = \\frac{dC}{d\\hat{y}} \\frac{d\\hat{y}}{da} \\frac{da}{dz} \\frac{dz}{db} \\\\\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421a17ae",
   "metadata": {},
   "source": [
    "$\\frac{dC}{d\\hat{y}}$, $\\frac{d\\hat{y}}{da}$, and $\\frac{da}{dz}$ are the same as before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a98ee8",
   "metadata": {},
   "source": [
    "We just need to find $\\frac{dz}{db}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64f594e",
   "metadata": {},
   "source": [
    "\n",
    "\\[\n",
    "z = w_0x + b \\\\\n",
    "\\frac{dz}{db} = 1\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f34d4be",
   "metadata": {},
   "source": [
    "That’s it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4fd7ad",
   "metadata": {},
   "source": [
    "For easier understanding, let’s simulate it in Google Sheet: here"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
