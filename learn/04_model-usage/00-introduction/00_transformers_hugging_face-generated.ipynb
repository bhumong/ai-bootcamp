{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "225c6733",
   "metadata": {},
   "source": [
    "source: [link](https://ai-bootcamp.ruangguru.com/learn/04_model-usage/00-introduction/00_transformers_hugging_face.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3582ae5e",
   "metadata": {},
   "source": [
    "# Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796cecf4",
   "metadata": {},
   "source": [
    "Welcome to Hugging Face: the pioneer in Natural Language Processing (NLP). Built on PyTorch, it offers a rich ecosystem that simplifies working with Transformer models. With Hugging Face, leveraging state-of-the-art NLP capabilities becomes an exciting exploration rather than a complex task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b346d3",
   "metadata": {},
   "source": [
    "# Why Hugging Face?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdc58b5",
   "metadata": {},
   "source": [
    "Explore the Hugging Face Library for all your Natural Language Processing (NLP) needs:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e8a915",
   "metadata": {},
   "source": [
    "# Installation and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6cd9cb",
   "metadata": {},
   "source": [
    "Starting your NLP journey with the Hugging Face Library is easy! If you use pip or conda, simply open your terminal and type in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666b369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For pip users:\n",
    "pip install transformers\n",
    "\n",
    "# For conda aficionados:\n",
    "conda install -c huggingface transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46d9cf9",
   "metadata": {},
   "source": [
    "For a more detailed guide, visit the official documentation. Welcome aboard the NLP adventure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0153b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba91054",
   "metadata": {},
   "source": [
    "The code snippet showcases how to use the Hugging Face Library:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37aef4f9",
   "metadata": {},
   "source": [
    "It imports pipeline from transformers and builds a text classification pipeline with a “distilbert-base-uncased-finetuned-sst-2-english” model. It then classifies the phrase “Hello world”. This succinct code conducts sentiment analysis using a pretrained DistilBERT model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b948586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "pipe(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245f657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[{'label': 'POSITIVE', 'score': 0.9997522234916687}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc5f2d5",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05df738e",
   "metadata": {},
   "source": [
    "This model, from a “High-Level Look”, acts as a ‘black box’ in machine translation applications with transformer model, converting a sentence from one language into its translation in another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2954c25",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27d27b51",
   "metadata": {},
   "source": [
    "![Image](https://storage.googleapis.com/rg-ai-bootcamp/model_usage/transformer-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b433e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def translation(input):\n",
    "    translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-id\")\n",
    "    output = translator(input, max_length=40)[0]['translation_text']\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ee62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"I am a student\"\n",
    "output = translation(input_text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823efaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Saya seorang mahasiswa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b114e7",
   "metadata": {},
   "source": [
    "Let’s see what’s inside transformers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0f6080",
   "metadata": {},
   "source": [
    "Popping open that Transformer, we can observe an encoding component, a decoding component, and the intricate connections between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c931390a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "771ebdff",
   "metadata": {},
   "source": [
    "![Image](https://storage.googleapis.com/rg-ai-bootcamp/model_usage/transformer-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9e019c",
   "metadata": {},
   "source": [
    "In the Transformer model, the encoding section is made up of a stack of encoders, while the decoding section has an equal stack of decoders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58dac84",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d0ba526",
   "metadata": {},
   "source": [
    "![Image](https://storage.googleapis.com/rg-ai-bootcamp/model_usage/transformer-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf31e08",
   "metadata": {},
   "source": [
    "# Code overview of how the Encoder and Decoder Transformer Model work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8ed6b2",
   "metadata": {},
   "source": [
    "First, import the model and tokenizer from the transformers with pretrained model “Helsinki-NLP/opus-mt-en-id”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8f9710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-id\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97fca39",
   "metadata": {},
   "source": [
    "Here, model is a pretrained Seq2Seq Transformer model (Encoder and Decoder) and tokenizer is a way to turn plain text into tokens that the model can understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218c418c",
   "metadata": {},
   "source": [
    "Next, we take the sentence, “I am a student”, and convert it to tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e268cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I am a student\"\n",
    "inputs = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd92ba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor([[   7,  303,   15, 4846,    0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076cf328",
   "metadata": {},
   "source": [
    "inputs is now a tokenized representation of the sentence that can be processed by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5950873a",
   "metadata": {},
   "source": [
    "Following the typical Seq2Seq model workflow, in this step, the encoder of the Seq2Seq model processes the input and generates an internal representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d828e5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(inputs, max_length=40)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c3e5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor([[54795,   200,   227,  8912,     0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fbdcac",
   "metadata": {},
   "source": [
    "outputs is the internal representation created by the encoder part of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930a29df",
   "metadata": {},
   "source": [
    "Finally, we decode this output back into human-readable text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03c5ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = tokenizer.decode(outputs[0])\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45986b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "<pad> Saya seorang mahasiswa</s>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02ecb69",
   "metadata": {},
   "source": [
    "decoded is the translated text output by the model. At this stage, the decoder has taken the internal representation from the encoder and produced the desired text output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfd726d",
   "metadata": {},
   "source": [
    "# Flow of Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2198ee",
   "metadata": {},
   "source": [
    "The animation below illustrates how we apply the Transformer to machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d45a4ee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d673485d",
   "metadata": {},
   "source": [
    "![Image](https://storage.googleapis.com/rg-ai-bootcamp/model_usage/transform20fps.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86d199c",
   "metadata": {},
   "source": [
    "Google’s Visualization describes it all!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cf11d3",
   "metadata": {},
   "source": [
    "## Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f8325d",
   "metadata": {},
   "source": [
    "Transformers are deep learning models that use self-attention to process sequential input data, such as natural language or images. Now, what is Self-Attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeebdf1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4ce10a1",
   "metadata": {},
   "source": [
    "![Image](https://storage.googleapis.com/rg-ai-bootcamp/model_usage/sentence-example-attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605e5c03",
   "metadata": {},
   "source": [
    "One word “attends” to other words in the same sentence differently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fee067",
   "metadata": {},
   "source": [
    "Self-attention, or transformer attention, is a vital component in Transformer models that allows each token in a sentence to interrelate with every other token, regardless of their distance. It enables the model to better understand linguistic context and nuances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c384abc",
   "metadata": {},
   "source": [
    "# Attention Basic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b50b39",
   "metadata": {},
   "source": [
    "The architecture of “Attention” in machine learning is like an author using a summarizer to highlight key points. It’s like the author questioning the summarizer (representing the “attention” mechanism), who offers summarized important points instead of direct answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ee112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694d9672",
   "metadata": {},
   "outputs": [],
   "source": [
    "Collecting graphviz\n",
    "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
    "     ---------------------------------------- 0.0/47.0 kB ? eta -:--:--\n",
    "     ------------------------ ------------- 30.7/47.0 kB 435.7 kB/s eta 0:00:01\n",
    "     -------------------------------------- 47.0/47.0 kB 593.5 kB/s eta 0:00:00\n",
    "Installing collected packages: graphviz\n",
    "Successfully installed graphviz-0.20.1\n",
    "Note: you may need to restart the kernel to use updated packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c782dbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph()\n",
    "dot.attr('node', shape='ellipse')\n",
    "dot.attr(rankdir='LR')\n",
    "\n",
    "# Add nodes (with individual attributes)\n",
    "dot.node('A', '🧑‍🏫 Author', shape='box', color='blue')\n",
    "dot.node('B', '🧑‍💻 Summarizer (attention)', shape='box', color='green')\n",
    "\n",
    "# Add edges (with individual attributes)\n",
    "dot.edge('A', 'B', label='ask question', color='blue')\n",
    "dot.edge('B', 'A', label='gives summary', color='green')\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05398ee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "928c92d3",
   "metadata": {},
   "source": [
    "For example: For the input text, “I am Ridho, I live in Jakarta, I am a software engineer at Ruangguru”. When the author queries, “Who is being introduced in this sentence?”, the summarizer (attention) gives a summary: “I am Ridho, I live in Jakarta, I am a software engineer at Ruangguru”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656fb8dc",
   "metadata": {},
   "source": [
    "Another example with seaborn visualization: for “I just moved here and I want to show you my house, oh by the way, my name is Imam”, the query is “What is my name?”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb3c627",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0397a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sentence = \"I just moved here and I want to show you my house, oh by the way, my name is Imam\".split()\n",
    "scores = np.array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.4, 0.3, 0.5]).reshape(1, -1)\n",
    "\n",
    "df_scores = pd.DataFrame(scores, columns=sentence)\n",
    "\n",
    "plt.figure(figsize=(16, 2))\n",
    "sns.heatmap(df_scores, cmap='YlGnBu', cbar=False)\n",
    "plt.yticks([])\n",
    "plt.ylabel(\"Attention Scoring\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef0da16",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74c92754",
   "metadata": {},
   "source": [
    "Seaborn visualization can be used to show which words in the sentence are most important. The darker the visualization, the more important the word is. If the author needs more information, they can look at the next lighter keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72573719",
   "metadata": {},
   "source": [
    "It is important to remember that attention doesn’t directly resolve the question. It provides clues (through attention scoring) that aid us in finding the answer by pointing out the most crucial words we need to be aware of to answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3249e800",
   "metadata": {},
   "source": [
    "# Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44f9488",
   "metadata": {},
   "source": [
    "Transformers utilize a mechanism known as multi-head attention. But why is this important? Multi-head attention allows the model to focus on different positional contexts simultaneously. This resolves the challenge of understanding the various contextual implications of words in a sentence. Each ‘head’ can pay attention to different parts of the input, thereby providing a richer understanding of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc11e223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph()\n",
    "dot.attr('node', shape='ellipse')\n",
    "dot.attr(rankdir='LR')\n",
    "\n",
    "# Add nodes (with individual attributes)\n",
    "dot.node('A', '🧑‍🏫 Author', shape='box', color='blue')\n",
    "dot.node('B', '🧑‍💻 🧑‍💻 🧑‍💻 🧑‍💻 🧑‍💻 🧑‍💻 🧑‍💻 🧑‍💻 🧑‍💻 🧑‍💻 🧑‍💻 🧑‍💻Summarizer (Multi head attention)', shape='box', color='green')\n",
    "\n",
    "# Add edges (with individual attributes)\n",
    "dot.edge('A', 'B', label='ask question', color='blue')\n",
    "dot.edge('B', 'A', label='gives summary', color='green')\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a60823",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ab81d9c",
   "metadata": {},
   "source": [
    "For example: For the input text, “Taylor feels lonely as his friends move out of the town”. When the author queries, “What does Taylor feel?”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78aafb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sentence = \"Taylor feels lonely as his friends move out of the town\".split()\n",
    "scores = np.array([0.3, 0.25, 0.4, 0.15, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2]).reshape(1, -1)\n",
    "\n",
    "df_scores = pd.DataFrame(scores, columns=sentence)\n",
    "\n",
    "plt.figure(figsize=(16, 2))\n",
    "sns.heatmap(df_scores, cmap='YlGnBu', cbar=False)\n",
    "plt.yticks([])\n",
    "plt.ylabel(\"Attention Scoring\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da3a8ff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb290be9",
   "metadata": {},
   "source": [
    "Each of them summarizes in their own style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5f6838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sentence = \"Taylor feels lonely as his friends move out of the town\".split()\n",
    "num_heads = 12\n",
    "\n",
    "scores = np.array([\n",
    "    [0.13, 0.15, 0.19, 0.15, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
    "    [0.2, 0.2, 0.19, 0.15, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1], \n",
    "    [0.3, 0.25, 0.4, 0.15, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2],\n",
    "    [0.1, 0.2, 0.3, 0.15, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.12],\n",
    "    [0.3, 0.25, 0.2, 0.15, 0.1, 0.1, 0.1, 0.2, 0.1, 0.1, 0.2],\n",
    "    [0.3, 0.25, 0.2, 0.15, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2],\n",
    "    [0.2, 0.28, 0.1, 0.11, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
    "    [0.1, 0.25, 0.2, 0.15, 0.1, 0.1, 0.2, 0.1, 0.1, 0.1, 0.2],\n",
    "    [0.3, 0.25, 0.3, 0.15, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2],\n",
    "    [0.15, 0.23, 0.15, 0.2, 0.15, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
    "    [0.3, 0.15, 0.3, 0.15, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2],\n",
    "    [0.1, 0.3, 0.2, 0.1, 0.15, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "]) \n",
    "\n",
    "df_scores = pd.DataFrame(scores, columns=sentence)\n",
    "\n",
    "plt.figure(figsize=(16, num_heads * 0.5))\n",
    "sns.heatmap(df_scores, cmap='YlGnBu')\n",
    "plt.ylabel(\"12 Summary Committee\")\n",
    "plt.yticks(np.arange(num_heads)+0.5, [f'Head_{i+1}' for i in range(num_heads)], rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1722ea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb25b9aa",
   "metadata": {},
   "source": [
    "Their voices are united"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc90d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sentence = \"Taylor feels lonely as his friends move out of the town\".split()\n",
    "scores = np.array([0.22083333, 0.21525, 0.25416667, 0.14666667, 0.10833333, 0.1, 0.10833333, 0.10833333, 0.1, 0.1, 0.15166667]).reshape(1, -1)\n",
    "\n",
    "df_scores = pd.DataFrame(scores, columns=sentence)\n",
    "\n",
    "plt.figure(figsize=(16, 2))\n",
    "sns.heatmap(df_scores, cmap='YlGnBu', cbar=False)\n",
    "plt.yticks([])\n",
    "plt.ylabel(\"Attention Scoring\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c5a982",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "308f250d",
   "metadata": {},
   "source": [
    "Can we explain what each head is “thinking”? 🤔"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854786e7",
   "metadata": {},
   "source": [
    "The answer is NO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72311c96",
   "metadata": {},
   "source": [
    "The unity of the heads’ voices is essentially the aggregation of the attention scores from each head, emphasizing the most relevant parts of the input that answer the query. However, not all heads agree on the significance of each word. For instance, some heads may consider the word “move” relevant, hence its dark cyan color, potentially because they think it is a crucial factor leading to Taylor’s loneliness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b593c018",
   "metadata": {},
   "source": [
    "We can only speculate each head’s focus. Perhaps one focuses on sentence structure, another on context, and so on. Due to the inscrutability of neural networks, we cannot ascertain the exact thought process of each head (lack of interpretability)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b038e7",
   "metadata": {},
   "source": [
    "Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f68b442",
   "metadata": {},
   "source": [
    "# Multi-Layer Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b637ca1",
   "metadata": {},
   "source": [
    "Transformers also incorporate multi-layer attention. But what does this mean, and why is it necessary? Multi-layer attention allows the model to iteratively refine its understanding of the inputs over multiple layers. Each layer offers the model another opportunity to learn the associations within the data. This layered approach addresses the need for deeper understanding and more complex relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4683c1ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48034632",
   "metadata": {},
   "source": [
    "![Image](https://storage.googleapis.com/rg-ai-bootcamp/model_usage/multi-layer-attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b4fc35",
   "metadata": {},
   "source": [
    "The heads are duplicated across layers, each being asked to re-learn its task. The number of layers varies depending on the model. The final layer incorporates the heads that best understand the task at hand. However, all heads across all layers are preserved. In rare situations, a head from an earlier layer might be more efficient at accomplishing a task than those in later layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de757e81",
   "metadata": {},
   "source": [
    "# Interactive attention with BertViz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e0e998",
   "metadata": {},
   "source": [
    "Now, we’ll use BertViz to visualize self-attention in a pre-trained BERT model with multiple “heads” capturing different word relationships. A 12-layer, 12-head model generates 144 unique attention mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d802547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bertviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e50510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from bertviz import head_view\n",
    "\n",
    "model_version = 'bert-base-uncased'\n",
    "model = BertModel.from_pretrained(model_version, output_attentions=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_version)\n",
    "inputs = tokenizer.encode_plus(\"She is eating a green apple\", \"She bought the apple from the local market.\", return_tensors='pt')\n",
    "\n",
    "attention = model(inputs['input_ids'], token_type_ids=inputs['token_type_ids'])[-1]\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0].tolist()) \n",
    "\n",
    "head_view(attention, tokens, inputs['token_type_ids'][0].tolist().index(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bd18af",
   "metadata": {},
   "source": [
    "# Transformers Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887e084c",
   "metadata": {},
   "source": [
    "Transformers are adaptable and can learn from large data sets for tasks like: - Translation: Breaking down language barriers. - Summarization: Turning long texts into brief summaries. - Text Generation: Creating new text on its own. - and etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997a0f92",
   "metadata": {},
   "source": [
    "We’ll explore these amazing uses in more detail in our next discussions!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
