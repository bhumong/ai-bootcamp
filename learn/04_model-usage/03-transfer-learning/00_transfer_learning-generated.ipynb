{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d084fa7e",
   "metadata": {},
   "source": [
    "source: [link](https://ai-bootcamp.ruangguru.com/learn/04_model-usage/03-transfer-learning/00_transfer_learning.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdc5ea5",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50b1c80",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e92dc7a0",
   "metadata": {},
   "source": [
    "![Image](https://storage.googleapis.com/rg-ai-bootcamp/model_usage/transfer-learning.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf73bdf",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cb42b7",
   "metadata": {},
   "source": [
    "Do you need help to train complex models due to limited high-quality data and resources? ü§î Don‚Äôt panic! The answer is Transfer Learning. This technique leverages pre-trained models, like BERT for NLP or ImageNet for image classification, to significantly reduce training time. Think of it as teaching an old dog new tricks: You can easily adapt an ImageNet model to tasks like dog breed classification. And voila! You‚Äôve just made quick progress even with scarce data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee64172",
   "metadata": {},
   "source": [
    "# How does Neural Network change in Transfer Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28998de",
   "metadata": {},
   "source": [
    "Imagine a chef who is skilled in baking cakes. Suppose this chef needs to cook a new dish, like pasta. Instead of starting from scratch, they leverage their culinary skills, adjusting only where necessary for the pasta-specific nuances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaad5f7",
   "metadata": {},
   "source": [
    "Similarly, two possible approaches exist in machine learning: ‚ÄúTraining from Scratch‚Äù and ‚ÄúTransfer Learning‚Äù. In the former, a model like CNN is trained on a new dataset, like Vehicles, without prior knowledge. In the latter, the model leverages prior knowledge acquired from a different dataset, like Animals, and adjusts this understanding to the new task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fb993a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "983c1b2b",
   "metadata": {},
   "source": [
    "![Image](https://miro.medium.com/v2/resize:fit:720/0*xNjEPIZmPvKeqss6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cae28e",
   "metadata": {},
   "source": [
    "The image above illustrates this concept. As shown, a model trained from scratch (the top one) is set up to learn directly from the vehicle dataset. It starts with no inherent understanding of images and must learn the features that differentiate one vehicle from another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20defc97",
   "metadata": {},
   "source": [
    "In contrast, a transfer learning model (the bottom) begins with a pre-trained network with pre-existing knowledge about different animals. This model is fine-tuned to distinguish different types of vehicles, typically achieving faster and more efficient results than training from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f3f12e",
   "metadata": {},
   "source": [
    "In essence, while both models aim to classify different types of vehicles, they learn differently: the model trained from scratch learns all features independently, like a chef learning a new dish from scratch, whereas the transfer learning model refines existing knowledge for the new task, similar to a chef adapting their existing skills to a new recipe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c506985",
   "metadata": {},
   "source": [
    "# Hands-On with MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77bfff4",
   "metadata": {},
   "source": [
    "Let‚Äôs see transfer learning in action on the famous MNIST dataset, a large collection of handwritten numbers. We will use PyTorch, a powerful open source library for machine learning in Python, as we have done in previous PyTorch materials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a6cc83",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed81c43",
   "metadata": {},
   "source": [
    "First, we have to load the data and check it. But how can you load large data sets in manageable batches? PyTorch DataLoader makes this possible and efficient. This is especially useful when the data is large and cannot be loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79d694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load MNIST data\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load MNIST data\n",
    "mnist_train = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "mnist_test = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "# Inspect data\n",
    "print(mnist_train)\n",
    "print(mnist_test)\n",
    "\n",
    "# Use Data Loader\n",
    "train_loader = DataLoader(mnist_train, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=100, shuffle=False)\n",
    "\n",
    "# Iterate through data\n",
    "for images, labels in train_loader:\n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793121d2",
   "metadata": {},
   "source": [
    "# Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1658b88",
   "metadata": {},
   "source": [
    "Visualizing our data can help us understand it better. But what if you‚Äôre not sure how to display images from your dataset? No worries! With matplotlib, a powerful plotting library in Python, we can easily visualize our images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ccceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define label mapping\n",
    "classes = [str(i) for i in range(10)] # because MNIST has 10 classes from digit 0 to digit 9\n",
    "# Load a batch of images\n",
    "images, labels = next(iter(train_loader))\n",
    "# Convert images to numpy for visualization\n",
    "images = images.numpy()\n",
    "# Convert images from 1 channel to 3 channels for visualization\n",
    "images = np.repeat(images, 3, axis=1)\n",
    "# Plot the images with their labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "\n",
    "# Display 20 images\n",
    "for idx in range(20):\n",
    "    ax = fig.add_subplot(2, 20//2, idx+1, xticks=[], yticks=[])\n",
    "    plt.imshow(np.transpose(images[idx], (1, 2, 0)))\n",
    "    ax.set_title(classes[labels[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e777de4",
   "metadata": {},
   "source": [
    "OK, now let‚Äôs start training with the model we created ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6919dda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define model\n",
    "class CreateModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(784, 100)\n",
    "        self.hidden = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        # Flatten the image tensors using reshape\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        out = self.linear(xb)\n",
    "        out = self.hidden(out)\n",
    "        return out\n",
    "    \n",
    "# Instantiate the model\n",
    "model = CreateModel()\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "learning_rate = 1e-2\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define accuracy function\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "# Train\n",
    "for epoch in range(20):\n",
    "    for images, labels in train_loader:\n",
    "        # Generate predictions\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        # Perform gradient descent\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 20, loss.item()))\n",
    "\n",
    "# Evaluate\n",
    "with torch.no_grad():\n",
    "    accum_acc = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        acc = accuracy(outputs, labels)\n",
    "        accum_acc += acc\n",
    "    \n",
    "    print('Test loss: {:.4f}, Test accuracy: {:.4f}'.format(loss.item(), accum_acc/len(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2353f757",
   "metadata": {},
   "source": [
    "# Making Individual Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d4b39e",
   "metadata": {},
   "source": [
    "Building and training a model is great, but how do we make predictions on individual images? What if you‚Äôre not sure how to use your newly trained model? The predict_image function provides a straightforward way to get your model‚Äôs prediction for a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df38c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def predict_image(img, model):\n",
    "    xb = img.unsqueeze(0)\n",
    "    yb = model(xb)\n",
    "    _, preds  = torch.max(yb, dim=1)\n",
    "    return preds[0].item()\n",
    "\n",
    "img, label = mnist_test[4] # Explore the data with index\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Label:', label, ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cb84dc",
   "metadata": {},
   "source": [
    "# Fine-tuning with FashionMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02710afe",
   "metadata": {},
   "source": [
    "Now, what if we want to classify different types of clothing, not just digits? Transfer learning to the rescue! We can fine-tune our pre-trained model on the new data to achieve great results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d68b13",
   "metadata": {},
   "source": [
    "That‚Äôs where the FashionMNIST dataset comes in. It‚Äôs a dataset of Zalando‚Äôs article images, consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21dd115",
   "metadata": {},
   "source": [
    "Sounds daunting? Don‚Äôt worry! With transfer learning, we can leverage the experience our model gained from MNIST to tackle this new task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d11663f",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6346893e",
   "metadata": {},
   "source": [
    "The first step, we have to load the FashionMNIST data as we do in MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02e5a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FashionMNIST data\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "fashion_train = datasets.FashionMNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "fashion_test = datasets.FashionMNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "# Inspect data\n",
    "print(fashion_train)\n",
    "print(fashion_test)\n",
    "\n",
    "# Use Data Loader\n",
    "fashion_train_loader = DataLoader(fashion_train, batch_size=100, shuffle=True)\n",
    "fashion_test_loader = DataLoader(fashion_test, batch_size=100, shuffle=False)\n",
    "\n",
    "# Iterate through data\n",
    "for images, labels in fashion_train_loader:\n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c608f019",
   "metadata": {},
   "source": [
    "# Visualizing the FashionMNIST Data with Class Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ca696",
   "metadata": {},
   "source": [
    "Visualizing data is helpful, but wouldn‚Äôt it be even better if we could visualize it with the corresponding class labels? What if you‚Äôre not sure how to map the labels of your data to their actual class names? That‚Äôs where the classes list comes into play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b3ae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define label mapping for FashionMNIST\n",
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "images, labels = next(iter(fashion_train_loader))\n",
    "images = images.numpy()\n",
    "images = np.repeat(images, 3, axis=1)\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "\n",
    "# Display 20 images\n",
    "for idx in range(20):\n",
    "    ax = fig.add_subplot(2, 20//2, idx+1, xticks=[], yticks=[])\n",
    "    plt.imshow(np.transpose(images[idx], (1, 2, 0)))\n",
    "    ax.set_title(f\"{labels[idx]}. {classes[labels[idx]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca1ec0a",
   "metadata": {},
   "source": [
    "# Testing the Model‚Äôs Predictions on Unseen FashionMNIST Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bceaf8",
   "metadata": {},
   "source": [
    "After training our model on the MNIST dataset, we might wonder how it would perform on the FashionMNIST dataset without any further training. What if we could use our model to make a prediction on a FashionMNIST image? By calling the predict_image function, we can do exactly that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7723e85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img, label = fashion_test[6] # Explore the data with index\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Label:', label, ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bdf855",
   "metadata": {},
   "source": [
    "Unfortunately, without any further training, the model struggles to correctly classify images from the FashionMNIST dataset. This result may seem disappointing, but it is not entirely surprising. The MNIST and FashionMNIST datasets, despite sharing the same structure, represent completely different kinds of images (handwritten digits versus clothing items), and it‚Äôs a tough ask for a model trained specifically on digits to accurately classify clothing items."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b142c7",
   "metadata": {},
   "source": [
    "This is exactly where the power of transfer learning shines! Let‚Äôs start training with the model architecture that we used for MNIST but this time, we‚Äôll fine-tune it on the FashionMNIST data. With this approach, our model can quickly learn to generalize from digits to clothing items, showcasing the strength of transfer learning in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7a5838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning with FashionMNIST data\n",
    "for epoch in range(10):\n",
    "    for images, labels in fashion_train_loader:\n",
    "        # Generate predictions\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Perform gradient descent\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 20, loss.item()))\n",
    "\n",
    "# Evaluate with FashionMNIST test data\n",
    "with torch.no_grad():\n",
    "    accum_acc = 0\n",
    "    for images, labels in fashion_test_loader:\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        acc = accuracy(outputs, labels)\n",
    "        accum_acc += acc\n",
    "    \n",
    "    print('Test loss: {:.4f}, Test accuracy: {:.4f}'.format(loss.item(), accum_acc/len(fashion_test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5a77ca",
   "metadata": {},
   "source": [
    "# Testing the Model‚Äôs Predictions After Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858b4245",
   "metadata": {},
   "source": [
    "After fine-tuning our model on the FashionMNIST dataset, we need to verify if it improved the model‚Äôs performance. What if we could test our model‚Äôs prediction on a FashionMNIST image again, but this time after fine-tuning? That‚Äôs precisely where the predict_image function comes in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20e2266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "label_map = {\n",
    "    0: \"T-shirt/top\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle boot\",\n",
    "}\n",
    "\n",
    "img, label = fashion_test[1] # Explore the data with index\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print(f\"Label: {label}. is: {label_map[label]}, Predicted: {predict_image(img, model)}. is: {label_map[predict_image(img, model)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de2ab27",
   "metadata": {},
   "source": [
    "# Exploring Further Into Fine-Tuning and Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0412c3c2",
   "metadata": {},
   "source": [
    "After testing our model on FashionMNIST data, we should now further deepen our understanding of fine-tuning. This sophisticated technique is not only limited to image classification tasks but can also be applied across various domains, including:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7629b8",
   "metadata": {},
   "source": [
    "- Text Classification\n",
    "- Summarization\n",
    "- And more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5df34da",
   "metadata": {},
   "source": [
    "- Audio classification\n",
    "- Automatic speech recognition\n",
    "- And more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c18626",
   "metadata": {},
   "source": [
    "- Image Classification\n",
    "- Object detection\n",
    "- And more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aeeb2d",
   "metadata": {},
   "source": [
    "- Image Captioning\n",
    "- Document Question Answering\n",
    "- And more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23f3f2f",
   "metadata": {},
   "source": [
    "One such example is audio classification. By using a pre-trained model like Wav2Vec2 and fine-tuning it on a specific dataset, we can create a powerful audio classification model even with less training data. This process includes several steps such as loading the dataset, preprocessing the data, setting up an evaluation metric, and finally training and evaluating the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457ae7a8",
   "metadata": {},
   "source": [
    "Let‚Äôs see how we can apply fine-tuning to develop an audio classification model and test its performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424067a6",
   "metadata": {},
   "source": [
    "# Audio Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8b176f",
   "metadata": {},
   "source": [
    "Imagine you are a wildlife conservationist, you have a library of sounds, and you want to classify whether they come from one animal species or another. You struggle because there are so many audio files and analyzing them manually takes so much time and effort!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07480198",
   "metadata": {},
   "source": [
    "But don‚Äôt worry! With the power of AI, we can make this process significantly easier and faster using a technique called Audio Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65ddd20",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d462a338",
   "metadata": {},
   "source": [
    "![Image](https://storage.googleapis.com/rg-ai-bootcamp/model_usage/audio-classification.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46fa74a",
   "metadata": {},
   "source": [
    "Tasks: Audio Classification (source: youtube.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea62813e",
   "metadata": {},
   "source": [
    "Audio Classification can be used for various applications, including detecting speaker‚Äôs intent, classifying languages, or in our case, identifying animal species by their sounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d887e7c2",
   "metadata": {},
   "source": [
    "For this purpose, we are going to utilize the pre-trained Wav2Vec2 model and fine-tune it on the MInDS-14 dataset specifically designed for this task. This approach reduces our training time and improves the model‚Äôs performance even with less data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e71d8b",
   "metadata": {},
   "source": [
    "Before we start, we need to make sure to have all the necessary Python libraries installed. Run this command in your Python environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df4dd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers datasets evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c50693",
   "metadata": {},
   "source": [
    "We also encourage you to log in to your Hugging Face account so you can upload and share your model with the community. If you are prompted, enter your token to login:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21ca844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deea32b6",
   "metadata": {},
   "source": [
    "Load MInDS-14 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ad3dba",
   "metadata": {},
   "source": [
    "Now we are ready to load our dataset, MInDS-14, from the Datasets library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0efb421",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "\n",
    "minds = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3184bbfa",
   "metadata": {},
   "source": [
    "We will then split our dataset into a smaller training and testing set. This step allows us to experiment and validate our model before spending more time on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e06c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "minds = minds.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5663fad",
   "metadata": {},
   "source": [
    "Then take a look at the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a403a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "minds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8995c06",
   "metadata": {},
   "source": [
    "While our dataset contains a lot of useful information, we will focus on the audio and intent_class in this guide. Let‚Äôs remove the other columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50eb4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "minds = minds.remove_columns([\"path\", \"transcription\", \"english_transcription\", \"lang_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398855ad",
   "metadata": {},
   "source": [
    "Now let‚Äôs take a look at an example in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361a7a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "minds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fc7bfa",
   "metadata": {},
   "source": [
    "Great! Now we have our dataset loaded and ready. Its fields are: - audio: a 1-dimensional array of the speech signal that we will use for training our model. - intent_class: represents the class id of the speaker‚Äôs intent (or in our case, the species of the animal)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993932bd",
   "metadata": {},
   "source": [
    "To make it easier for the model to get the label name from the label id, we create a dictionary that maps the label name to an integer and vice versa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f639cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = minds[\"train\"].features[\"intent_class\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa630adb",
   "metadata": {},
   "source": [
    "Now we can easily convert a label id to a label name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add2d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label[str(2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebaa8d2",
   "metadata": {},
   "source": [
    "Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993dc7df",
   "metadata": {},
   "source": [
    "Now comes the interesting part: Preprocessing. In this step, we are going to load a Wav2Vec2 feature extractor to process the audio signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3cf520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e61e14",
   "metadata": {},
   "source": [
    "The MInDS-14 dataset has a sampling rate of 8000kHz, as found in its dataset card. The trained Wav2Vec2 model, however, requires the audio input to have a sampling rate of 16000kHz. Therefore, we must resample our dataset to convert the sampling rate from 8000kHz to 16000kHz to meet the model‚Äôs requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2d3381",
   "metadata": {},
   "outputs": [],
   "source": [
    "minds = minds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "minds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dafdd13",
   "metadata": {},
   "source": [
    "We then create a preprocessing function to load and resample the audio file and ensure it matches the sampling rate of the model‚Äôs pre-training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecec6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=16000, truncation=True\n",
    "    )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1007afd8",
   "metadata": {},
   "source": [
    "We use the Datasets map function to apply the preprocessing function across the complete dataset. Speed it up by enabling batched=True to process multiple dataset elements simultaneously. Remove unnecessary columns and rename intent_class to label, as the model expects this name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3757f0d4",
   "metadata": {},
   "source": [
    "Let‚Äôs apply our preprocessing function across the complete dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624036eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_minds = minds.map(preprocess_function, remove_columns=\"audio\", batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffbce7d",
   "metadata": {},
   "source": [
    "After applying our preprocessing function, we rename our column name from intent_class to label because our model expects this name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3b7483",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_minds = encoded_minds.rename_column(\"intent_class\", \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c281327",
   "metadata": {},
   "source": [
    "Setting up Evaluation Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b58865",
   "metadata": {},
   "source": [
    "Now, how can we know if our model is performing well? We need a yardstick to measure it. That‚Äôs when Evaluation Metrics come into play. We will use the accuracy metric from the evaluate library for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78bf61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a0674b",
   "metadata": {},
   "source": [
    "Then, we create a function that will take our model‚Äôs predictions and labels to calculate the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f3f81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3314dda4",
   "metadata": {},
   "source": [
    "Now, our compute_metrics function is ready to be used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3440e24",
   "metadata": {},
   "source": [
    "Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbe908d",
   "metadata": {},
   "source": [
    "We‚Äôve come a long way! It‚Äôs time to Train and Evaluate our model. We‚Äôre going to load the Wav2Vec2 model along with the number of expected labels, and the label mappings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c4c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForAudioClassification\n",
    "\n",
    "num_labels = len(id2label)\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    \"facebook/wav2vec2-base\", num_labels=num_labels, label2id=label2id, id2label=id2label\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9843e295",
   "metadata": {},
   "source": [
    "During training, we will use an optimization strategy where we set a learning rate and batch size. We also set the load_best_model_at_end=True option which means our trainer will load the model with the highest accuracy at the end of the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882227e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model/audio_classification\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,                  # learning rate\n",
    "    per_device_train_batch_size=32,      # training batch size\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,         # model optimization via early stopping\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=True, # When set to True, this allows the trained model to be directly uploaded to the Hugging Face Model Hub.\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_minds[\"train\"],\n",
    "    eval_dataset=encoded_minds[\"test\"],\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf41c726",
   "metadata": {},
   "source": [
    "Once training is completed, we can share our model to the Hugging Face Model Hub so that everyone can use our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2b1451",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f969e705",
   "metadata": {},
   "source": [
    "Here is an example of a model that has been trained: https://huggingface.co/aditira/audio_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0dd557",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6a9ad8",
   "metadata": {},
   "source": [
    "Finally, let‚Äôs test our model with a new audio file. This step is called Inference. We will load an audio file and run our model to classify it. Remember to resample the audio file to match the model‚Äôs sampling rate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3befee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "from transformers import pipeline\n",
    "\n",
    "dataset = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "audio_file = dataset[0][\"audio\"][\"path\"]\n",
    "\n",
    "classifier = pipeline(\"audio-classification\", model=\"model/audio_classification\")\n",
    "classifier(audio_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5e6d82",
   "metadata": {},
   "source": [
    "Congratulations! You just classified an audio file using your fine-tuned model! Now, you can classify your entire library of animal sounds and make your work as a wildlife conservationist a bit easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bc2336",
   "metadata": {},
   "source": [
    "# Image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4987e96a",
   "metadata": {},
   "source": [
    "Imagine you‚Äôve just been hired as a data scientist at a health-focused tech startup. You‚Äôre tasked with developing a system that identifies food items from images uploaded by users. The goal is to enable users to track their nutrition intake simply by taking a picture. But how would you go about this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c885f9",
   "metadata": {},
   "source": [
    "Well, the good news is, with the advent of Image Classification in machine learning, this task is no longer a pipe dream, but a very achievable reality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046ddafa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27c1c2dd",
   "metadata": {},
   "source": [
    "![Image](https://storage.googleapis.com/rg-ai-bootcamp/model_usage/image-classification.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc02061",
   "metadata": {},
   "source": [
    "Tasks: Image Classification (source: youtube.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c308757",
   "metadata": {},
   "source": [
    "Image classification can be used in countless applications, ranging from detecting objects in satellite images to medical imaging. In our case, we aim to classify food items from given images. We will use a pre-trained model called the Vision Transformer, or ViT for short. ViT is a model that applies the transformer architecture, which was initially built for text data, to image data. We will fine-tune this model on the Food-101 dataset to classify images into 101 food categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cf420f",
   "metadata": {},
   "source": [
    "Load Food-101 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6fb1b6",
   "metadata": {},
   "source": [
    "Let‚Äôs start with Loading our Food-101 dataset. We will load only a small subset of it to ensure everything works before committing to training on the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6093ca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "food = load_dataset(\"food101\", split=\"train[:5000]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb6916c",
   "metadata": {},
   "source": [
    "We then split our dataset into a training set and a testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7156e125",
   "metadata": {},
   "outputs": [],
   "source": [
    "food = food.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3425a362",
   "metadata": {},
   "source": [
    "Let‚Äôs take a look at an example from our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f356fc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "food[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c3c2fc",
   "metadata": {},
   "source": [
    "Each example in the dataset has two fields: - image: a PIL image of the food item - label: the label class of the food item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45a84cf",
   "metadata": {},
   "source": [
    "To make it easier for the model to get the label name from the label id, create a dictionary that maps the label name to an integer and vice versa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8ec06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = food[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf37d6f",
   "metadata": {},
   "source": [
    "Now we can easily convert a label id to a label name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a46619",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label[str(79)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb151d7",
   "metadata": {},
   "source": [
    "Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8997da6a",
   "metadata": {},
   "source": [
    "The next step is Preprocessing. Here we load a ViT image processor to process the image into a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c289c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b28f625",
   "metadata": {},
   "source": [
    "To make our model robust against overfitting, we apply some image transformations to the images. Here we are using torchvision‚Äôs transforms module, but you can also use any image library you like. We crop a random part of the image, resize it, and normalize it with the image mean and standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8625265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c793ed7",
   "metadata": {},
   "source": [
    "Then we create a preprocessing function to apply the transforms and return the pixel_values - the inputs to the model - of the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54daf455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7774d76",
   "metadata": {},
   "source": [
    "To apply the preprocessing function over the entire dataset, use Datasets with_transform method. The transforms are applied on the fly when you load an element of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9972501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "food = food.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294f614d",
   "metadata": {},
   "source": [
    "Next, we create a batch of examples using DefaultDataCollator. Unlike other data collators, the DefaultDataCollator does not apply additional preprocessing such as padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b7d82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00d1d8e",
   "metadata": {},
   "source": [
    "Setting up Evaluation Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d4c07c",
   "metadata": {},
   "source": [
    "Including a metric during training is beneficial for evaluating your model‚Äôs performance. We can load an evaluation method with the Evaluate library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e7761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b772a7",
   "metadata": {},
   "source": [
    "Now, we‚Äôll create a function that will take our model‚Äôs predictions and labels to calculate the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94838cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aba3271",
   "metadata": {},
   "source": [
    "Our compute_metrics function is ready now, and we‚Äôll use it when we set up our training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502915fe",
   "metadata": {},
   "source": [
    "Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe094fc6",
   "metadata": {},
   "source": [
    "It‚Äôs time to Train and Evaluate our model. We‚Äôre going to load ViT with AutoModelForImageClassification. We‚Äôll specify the number of labels along with the number of expected labels, and the label mappings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d275032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758e7e4b",
   "metadata": {},
   "source": [
    "At this point, we‚Äôll set our training hyperparameters in TrainingArguments. We‚Äôll make sure to set remove_unused_columns=False to keep the image column, which is crucial for creating pixel_values. We‚Äôll specify output_dir to save your model and enable push_to_hub to upload to Hugging Face. The Trainer will evaluate the accuracy and save a checkpoint after each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e5dbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model/image_classification\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,                 # learning rate\n",
    "    per_device_train_batch_size=16,     # training batch size\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,        # model optimization via early stopping\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=True, # When set to True, this allows the trained model to be directly uploaded to the Hugging Face Model Hub.\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=food[\"train\"],\n",
    "    eval_dataset=food[\"test\"],\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c290fc",
   "metadata": {},
   "source": [
    "Evaluating the Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b15952",
   "metadata": {},
   "source": [
    "After the training is completed, we need to evaluate the model. We‚Äôll use the accuracy metric from the evaluate library to assess the model‚Äôs performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a49f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = trainer.evaluate(eval_dataset=food[\"test\"])\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db91a13c",
   "metadata": {},
   "source": [
    "The evaluation results will give us the accuracy of the test set. If this accuracy is satisfactory, we could decide to publish the model. If not, we might need to revisit the preprocessing, model architecture, or training process (e.g., tuning hyperparameters and increasing the number of epochs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2146e985",
   "metadata": {},
   "source": [
    "Once the training is completed, we can share our model to Hugging Face Model Hub so the model can be accessed by anyone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f1fc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4030e85",
   "metadata": {},
   "source": [
    "Here is an example of a model that has been trained: https://huggingface.co/aditira/image_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48422f6",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f303bc09",
   "metadata": {},
   "source": [
    "With the model trained, we can now make use of it for Inference. Let‚Äôs load an image we‚Äôd like to run inference on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664bfd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"food101\", split=\"validation[:10]\")\n",
    "image = ds[\"image\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffa9be0",
   "metadata": {},
   "source": [
    "The simplest way to try out our fine-tuned model for inference is to use it in a pipeline(). Instantiate a pipeline for image classification with our model, and pass our image to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a611805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"image-classification\", model=\"my_awesome_food_model\")\n",
    "classifier(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184b0cf9",
   "metadata": {},
   "source": [
    "Congratulations! You‚Äôve managed to classify a food item from an image, bringing our startup‚Äôs vision one step closer to reality. Now you can continue to refine and apply this model, making nutrition tracking easier and more accessible for users around the world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a09970b",
   "metadata": {},
   "source": [
    "# Conclusion: The Power of Fine-Tuning and the Right Choice of Pre-trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1041c7e",
   "metadata": {},
   "source": [
    "Think of learning as building a house. Starting from scratch, you‚Äôd need to lay down the foundation, put up the walls, install the plumbing and wiring, and then finally add the finishing touches like paint and furniture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a620f82",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b443548f",
   "metadata": {},
   "source": [
    "![Image](https://kutaisitoday.com/wp-content/uploads/2021/07/ilustrasi-membangun-rumah.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277c87c9",
   "metadata": {},
   "source": [
    "Building a House (source: kutaisitoday.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dea895",
   "metadata": {},
   "source": [
    "Similarly, training a model from scratch requires learning all the features and architectures, often requiring a large amount of data and computational resources, which is not always feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db062085",
   "metadata": {},
   "source": [
    "Now, what if you could start with a house that‚Äôs already built and just rearrange the furniture and repaint the walls to suit your taste? This is the idea behind transfer learning or fine-tuning. We start with a model that has already learned useful features from a large-scale dataset (the pre-built house), and then fine-tune it on our specific task (rearranging and repainting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb74316d",
   "metadata": {},
   "source": [
    "In our case, the ‚Äòhouse‚Äô is the Vision Transformer (ViT), pre-trained on the ImageNet dataset. It‚Äôs a robust and versatile model, well-suited for various image classification tasks. But it‚Äôs not specialized in identifying food items."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8861a22a",
   "metadata": {},
   "source": [
    "So, we give it a makeover. We fine-tune the ViT on our Food-101 dataset. The model keeps its acquired knowledge and tailors it to our specific purpose - classifying food images. It‚Äôs like hiring an interior designer (the fine-tuning process) to transform a generic house (the pre-trained ViT) into a gourmet restaurant (the food-classification model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ea8f64",
   "metadata": {},
   "source": [
    "But why ViT? Why not another model? ü§î"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b61a845",
   "metadata": {},
   "source": [
    "The choice of a pre-trained model is crucial. It should ideally have an architecture that‚Äôs well-suited to your task and has been trained on a large, diverse dataset. ViT is a good choice because it brings the power of transformers, which can capture complex dependencies in data, into the realm of vision. It‚Äôs been pre-trained on ImageNet, a large-scale, diverse dataset, allowing it to learn a wide variety of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955a3989",
   "metadata": {},
   "source": [
    "Yet it‚Äôs important to remember that there is no one-size-fits-all model. The choice would depend on your task, data, and computational resources."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
