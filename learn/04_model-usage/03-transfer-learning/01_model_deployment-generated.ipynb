{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03fedd23",
   "metadata": {},
   "source": [
    "source: [link](https://ai-bootcamp.ruangguru.com/learn/04_model-usage/03-transfer-learning/01_model_deployment.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cccb053",
   "metadata": {},
   "source": [
    "> Note: Materi ini merupakan materi supplemental, tidak bersifat wajib. Namun akan mendukung kalian menjadi seorangAI engineeryang handal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2d259",
   "metadata": {},
   "source": [
    "Note: Materi ini merupakan materi supplemental, tidak bersifat wajib. Namun akan mendukung kalian menjadi seorang AI engineer yang handal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d59f620",
   "metadata": {},
   "source": [
    "# Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89de16c9",
   "metadata": {},
   "source": [
    "Model deployment is a crucial step in the machine learning development where the trained model is made accessible for users and integrated into applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df319fa6",
   "metadata": {},
   "source": [
    "It involves saving the model after training and providing a means for users, frontend (FE), and/or backend (BE) systems to access predictions. In this notebook, we’ll cover various aspects of model deployment, including different deployment options and strategies for optimizing models for deployment on CPU deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de496282",
   "metadata": {},
   "source": [
    "# Various type of Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e6a790",
   "metadata": {},
   "source": [
    "Deploying a machine learning model means making it accessible to users and other systems. This involves creating an interface through which users can provide input data and receive predictions. Here are some common deployment options:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e02427",
   "metadata": {},
   "source": [
    "- API (Application Programming Interface): An API serves as a bridge between your model and external applications. Flask and FastAPI are popular frameworks for building APIs in Python.\n",
    "- Mobile Apps: For mobile deployment, you can use frameworks likePyTorch Mobile,TensorFlow Lite (TFLite)CoreML. These frameworks allow you to integrate your model into Android or iOS applications, enabling real-time inference on mobile devices.\n",
    "- PC Apps: When deploying models in desktop applications, the choice of technology depends on the application platform. For Python-based apps, you can use libraries like Tkinter or PyQt. For cross-platform apps, consider using Flutter for a consistent user experience across different operating systems.\n",
    "- Web Apps: You also can deploy your model using Tensorflow JS to a web app without having to rely on a backend server, With Tensorflow JS you can use both CPU and GPU from user for accelerating the model deployment process. Keep in mind that if we deploy our model to a web app, we’ll basically use end-user computing power to process our model, so consider only using this method on simple and small model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9e4633",
   "metadata": {},
   "source": [
    "API (Application Programming Interface): An API serves as a bridge between your model and external applications. Flask and FastAPI are popular frameworks for building APIs in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a612eb",
   "metadata": {},
   "source": [
    "Mobile Apps: For mobile deployment, you can use frameworks like PyTorch Mobile, TensorFlow Lite (TFLite) CoreML. These frameworks allow you to integrate your model into Android or iOS applications, enabling real-time inference on mobile devices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b325ab9b",
   "metadata": {},
   "source": [
    "PC Apps: When deploying models in desktop applications, the choice of technology depends on the application platform. For Python-based apps, you can use libraries like Tkinter or PyQt. For cross-platform apps, consider using Flutter for a consistent user experience across different operating systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf59681",
   "metadata": {},
   "source": [
    "Web Apps: You also can deploy your model using Tensorflow JS to a web app without having to rely on a backend server, With Tensorflow JS you can use both CPU and GPU from user for accelerating the model deployment process. Keep in mind that if we deploy our model to a web app, we’ll basically use end-user computing power to process our model, so consider only using this method on simple and small model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487c0a47",
   "metadata": {},
   "source": [
    "# Model Optimization for Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c35eee4",
   "metadata": {},
   "source": [
    "In this notebook, we’ll focus on optimizing models for deployment on CPU environments by quantize model using ONNX (Open Neural Network Exchange) framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d78bcf",
   "metadata": {},
   "source": [
    "But before we begin, Why using ONNX instead of Tensofrlow or Pytorch directly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6522362",
   "metadata": {},
   "source": [
    "ONNX is faster for inference using ONNX Runtime to do inference, resulted slightly faster inference time than pytorch in some cases in fp32 weights, but more faster in fp16 weights example case (will discuss more in the next section about fp32, fp16 and int8 later)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac46fc3",
   "metadata": {},
   "source": [
    "Framework Interoperability: ONNX allows you to train a model in one language and then export and run it in another. This can be very advantageous for teams that use different frameworks or if you want to build a model in one language and then deploy it in another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e17d26",
   "metadata": {},
   "source": [
    "Portability and Deployment Ease: ONNX can be used for running models on various platforms including cloud, edge devices, or on-premises servers. This way, you can avoid rewriting and retraining models for all these settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aaae32",
   "metadata": {},
   "source": [
    "Broad Support: Several well-known organizations and software vendors have thrown their support behind ONNX, ensuring that it would not be abandoned anytime soon. These include Microsoft, Facebook, Amazon, Intel, AMD, NVIDIA, IBM, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d8fda2",
   "metadata": {},
   "source": [
    "Install depedencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f285e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install transformers\n",
    "!pip install optimum[\"onnxruntime\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51867ae5",
   "metadata": {},
   "source": [
    "# Accelerate inference using model quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fe8656",
   "metadata": {},
   "source": [
    "# Create Performance Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b845259",
   "metadata": {},
   "source": [
    "Before optimizing a model, it’s important to establish a baseline for comparison so later we can compare it and see if our optimization has any effect. In this case, we’ll measure * Time taken to generate predictions on the CPU using the original model. * Model performance, in this case is accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63be4957",
   "metadata": {},
   "source": [
    "Here we are using Fine-tuned model (model from transfer learning) as our baseline model The dataset used for this notebook is indonlu-smsa, which used for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617cdf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "import time\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ayameRushia/roberta-base-indonesian-1.5G-sentiment-analysis-smsa\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ayameRushia/roberta-base-indonesian-1.5G-sentiment-analysis-smsa\")\n",
    "\n",
    "cls = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381ef5c3",
   "metadata": {},
   "source": [
    "Download Dataset for Testing from Huggingface dataset, in here we will use dataset from indonlu-smsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e0303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"indonlp/indonlu\", \"smsa\")\n",
    "\n",
    "# establish testing input and label\n",
    "testing_input = dataset[\"validation\"][\"text\"]\n",
    "testing_label = dataset[\"validation\"][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fa5450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sneak peek to testing data\n",
    "\n",
    "print(testing_input[:5])\n",
    "print(testing_label[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695f71d1",
   "metadata": {},
   "source": [
    "# Measure Accuracy and time taken to inference all testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c2b230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# We use the evaluate function to evaluate our model on the testing data, and the metrisc we want to use is accuracy\n",
    "precision_metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d853368",
   "metadata": {},
   "source": [
    "Measure inference time of the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a23eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "result = cls(dataset[\"validation\"][\"text\"])\n",
    "result = [model.config.label2id[x[\"label\"]] for x in result]\n",
    "t2 = time.time()\n",
    "\n",
    "inference_time_vanilla = t2-t1\n",
    "avg_time_vanilla = inference_time_vanilla/len(result)\n",
    "\n",
    "print(f'Total inference time for vanilla model is {inference_time_vanilla:.3f} s')\n",
    "print(f'average inference time for vanilla model is {avg_time_vanilla:.3f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e078751",
   "metadata": {},
   "outputs": [],
   "source": [
    "300.5123541355133"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6183800d",
   "metadata": {},
   "source": [
    "Measure accuracy of the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3185999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_vanilla = precision_metric.compute(\n",
    "    references=testing_label, predictions=result)\n",
    "\n",
    "print(f'Accuracy of the vanilla model on testing dataset {results_vanilla[\"accuracy\"]*100:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65fd092",
   "metadata": {},
   "source": [
    "# Now lets try optimize our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20404765",
   "metadata": {},
   "source": [
    "# Quantization Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae68a5e7",
   "metadata": {},
   "source": [
    "The optimization process taken in this notebook is called quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4766cce8",
   "metadata": {},
   "source": [
    "Quantization is a technique to reduce the computational and memory costs of running inference by representing the weights and activations with low-precision data types like 8-bit integer (int8) instead of the usual 32-bit floating point (float32)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d77227",
   "metadata": {},
   "source": [
    "Reducing the number of bits means the resulting model requires less memory storage, consumes less energy (in theory), and operations like matrix multiplication can be performed much faster with integer arithmetic. It also allows to run models on embedded devices, which sometimes only support integer data types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50866ffd",
   "metadata": {},
   "source": [
    "While the quantization process substantially reduces the computational and memory load by using low-precision data types like 8-bit integers for representing weights and activations, it’s crucial to be aware of its impact on model accuracy. The inherent reduction in precision due to quantization can potentially degrade model performance, as it relies on a degree of approximation. This may not be acceptable for use-cases necessitating high precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb3fbad",
   "metadata": {},
   "source": [
    "There are two ways of doing quantization, we won’t be explaining the concept in detail, but rather the difference in their output for both methods :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09945a5",
   "metadata": {},
   "source": [
    "Quantization parameters can be computed on a per-tensor basis, faster but slightly less accurate and requires less memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d2fd49",
   "metadata": {},
   "source": [
    "Quantization parameters can be computed on a per-channel basis, better accuracy, it requires more memory and slower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba5d104",
   "metadata": {},
   "source": [
    "# Quantization Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78631b2c",
   "metadata": {},
   "source": [
    "Calibration is the step during quantization where the float32 ranges are computed. For weights it is quite easy since the actual range is known at quantization-time. But it is less clear for activations, and different approaches exist:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed988a54",
   "metadata": {},
   "source": [
    "Post training dynamic quantization: the range for each activation is computed on the fly at runtime, This is the simplest approach but slower when compared to static quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376e8f29",
   "metadata": {},
   "source": [
    "Post training static quantization: the range for each activation is computed in advance at quantization-time, typically by passing representative data through the model and recording the activation values. Thus, make the inference time faster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d437cb56",
   "metadata": {},
   "source": [
    "# In this notebook we will use per-tensor quantization with post training dynamic quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3f092d",
   "metadata": {},
   "source": [
    "Without further ado, let’s quantize our model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7831ccf7",
   "metadata": {},
   "source": [
    "First we export our model to onnx from pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0a58f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification, ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "\n",
    "# Load the model from the hub and export it to the ONNX format\n",
    "model = ORTModelForSequenceClassification.from_pretrained(\n",
    "    \"ayameRushia/roberta-base-indonesian-1.5G-sentiment-analysis-smsa\", export=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fb646a",
   "metadata": {},
   "source": [
    "# After exporting the model to onnx we will use ORTQuantizer to quantize our model and add quantization config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b04028",
   "metadata": {},
   "outputs": [],
   "source": [
    "  - AVX512 (general user CPU, intel and amd)\n",
    "  - ARM (mobile CPU, apple silicon, embedded device such as Jetson Nano from nvidia or raspberry pi)\n",
    "  - AVX2 (older CPU, intel and amd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d081cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer = ORTQuantizer.from_pretrained(model)\n",
    "dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbe2759",
   "metadata": {},
   "source": [
    "Then, quantize the model using defined quantizer and quantization config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ffb115",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"model/model_onnx\"\n",
    "\n",
    "model_quantized_path = quantizer.quantize(\n",
    "    save_dir=save_dir,\n",
    "    quantization_config=dqconfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572080c1",
   "metadata": {},
   "source": [
    "In hypothesis, after quantizing our model from fp32 to int8, the model size will be reduced and the inference time will be faster Then, lets first compute the size of the model before and after quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed87aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "size_model_q = os.path.getsize(\"/content/model/model_onnx/model_quantized.onnx\")/1024/1024  # divide by 1024 because the result is in bytes, divide by 1024 again to get the result in MB\n",
    "print(f\"Size of the quantized model is {size_model_q:.3f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29c3bc4",
   "metadata": {},
   "source": [
    "The result is pretty good, we reduce the model size from 518 MB (vanilla pytorch) to only 120.98 MB (quantized model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51f2f47",
   "metadata": {},
   "source": [
    "Now, lets try to measure the inference time of the quantized model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913fcfa2",
   "metadata": {},
   "source": [
    "Create pipeline using quantized model from optimum (also library from huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeee03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.pipelines import pipeline\n",
    "\n",
    "model = ORTModelForSequenceClassification.from_pretrained(\"model/model_onnx\")\n",
    "onnx_clx = pipeline(\"text-classification\", model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39c7ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_onnx = time.time()\n",
    "result_q = onnx_clx(dataset[\"validation\"][\"text\"])\n",
    "result_q = [model.config.label2id[x[\"label\"]] for x in result_q]\n",
    "t2_onnx = time.time()\n",
    "\n",
    "time_onnx = (t2_onnx-t1_onnx)/len(result_q) * 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcb6d45",
   "metadata": {},
   "source": [
    "After measure the inference time, lets measure its performance using evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f66d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "precision_metric = evaluate.load(\"accuracy\")\n",
    "results_q = precision_metric.compute(references=dataset[\"validation\"][\"label\"], predictions=result_q)\n",
    "print(f'Accuracy of the vanilla model on testing dataset {results_q[\"accuracy\"]*100:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202a6bfa",
   "metadata": {},
   "source": [
    "# Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92a2258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = ['Results (Q)', 'Results']\n",
    "accuracy_values = [results_q['accuracy'], results_vanilla['accuracy']]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(labels, accuracy_values, color=['blue', 'green'])\n",
    "plt.ylim(0.9, 1.0)  # Set the y-axis limits to ensure the entire range is visible\n",
    "\n",
    "# Add labels and a title\n",
    "plt.xlabel('Results')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Comparison')\n",
    "\n",
    "# Add text labels above the bars\n",
    "for i in range(len(labels)):\n",
    "    plt.text(i, accuracy_values[i] + 0.005, f'{accuracy_values[i]:.5f}', ha='center', fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaccabf2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e741e9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the inference times\n",
    "time_onnx = (t2_onnx - t1_onnx) / len(result_q)\n",
    "time_vanilla = (t2 - t1) / len(result)\n",
    "\n",
    "# Create a bar chart to visualize the inference times\n",
    "labels = ['ONNX Model', 'Vanilla Model']\n",
    "inference_times = [time_onnx, time_vanilla]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(labels, inference_times, color=['blue', 'green'])\n",
    "plt.ylim(0, max(inference_times) * 1.2)  # Set the y-axis limits to ensure the entire range is visible\n",
    "\n",
    "# Add labels and a title\n",
    "plt.xlabel('Model Type')\n",
    "plt.ylabel('Inference Time (per sample)')\n",
    "plt.title('Average Inference Time Comparison (per sample)')\n",
    "\n",
    "# Add text labels above the bars\n",
    "for i in range(len(labels)):\n",
    "    plt.text(i, inference_times[i] + max(inference_times) * 0.02, f'{inference_times[i]:.4f} seconds', ha='center', fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a7cdbb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7f7f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_percentage = ((time_vanilla - time_onnx) / time_vanilla) * 100\n",
    "print(f'Inference time optimized by {optimization_percentage:.3f}%')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
