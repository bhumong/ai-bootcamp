{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "158c88db",
   "metadata": {},
   "source": [
    "Note: Materi ini merupakan materi supplemental, tidak bersifat wajib. Namun akan mendukung kalian menjadi seorang AI engineer yang handal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff0206d",
   "metadata": {},
   "source": [
    "# Choosing the Right Processing Unit for Machine Learning: CPU, GPU, or TPU?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4056e828",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73264983",
   "metadata": {},
   "source": [
    "CPU vs TPU vs GPU (source miro.medium.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eea75d9",
   "metadata": {},
   "source": [
    "The world of machine learning is full of challenging decisions. One of these is choosing between a Central Processing Unit (CPU), Graphics Processing Unit (GPU), and Tensor Processing Unit (TPU) for your machine learning tasks. Are you unsure about which one to pick? Don’t sweat! We’re here to help you understand the implications of this decision and guide you towards the right choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c17e362",
   "metadata": {},
   "source": [
    "# CPU (Central Processing Unit): The Jack-of-all-Trades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96626740",
   "metadata": {},
   "source": [
    "Have you ever thought about what makes your computer tick? That’s the job of the CPU!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648e031d",
   "metadata": {},
   "source": [
    "A CPU is the primary component of a computer that does most of the processing inside the computer. Basically, it’s responsible for the “thinking”. CPUs provide computational abilities for a wide variety of tasks, not just machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a68947f",
   "metadata": {},
   "source": [
    "So, when would you choose CPU for your machine learning journey?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487bdf1c",
   "metadata": {},
   "source": [
    "Pros:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a607756",
   "metadata": {},
   "source": [
    "Cons:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae335209",
   "metadata": {},
   "source": [
    "# GPU (Graphics Processing Unit): The Hardcore Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937ec900",
   "metadata": {},
   "source": [
    "Have you ever wondered how your computer displays high-quality images and videos so quickly? For that, you can thank the GPU!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8bafe5",
   "metadata": {},
   "source": [
    "GPUs, initially designed for rendering high-quality images and videos, are now being increasingly used in machine learning due to their ability to perform parallel operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd59347d",
   "metadata": {},
   "source": [
    "So, when should you opt for a GPU in your machine learning journey?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b92078",
   "metadata": {},
   "source": [
    "Pros:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e7d349",
   "metadata": {},
   "source": [
    "Cons:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40a4d64",
   "metadata": {},
   "source": [
    "# TPU (Tensor Processing Unit): The Machine Learning Maestro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b736236",
   "metadata": {},
   "source": [
    "Ever heard of Google’s secret weapon for machine learning? It’s the TPU!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d70d755",
   "metadata": {},
   "source": [
    "TPUs are Google’s custom-developed application-specific integrated circuits (ASICs) that are specifically designed to accelerate machine learning workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2625b34a",
   "metadata": {},
   "source": [
    "So, when making the big leap into serious machine learning, is a TPU the right choice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20932e0",
   "metadata": {},
   "source": [
    "Pros:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9276c4b9",
   "metadata": {},
   "source": [
    "Cons:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a96a3b",
   "metadata": {},
   "source": [
    "For more information, visit the following video link: https://www.youtube.com/watch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae72e77d",
   "metadata": {},
   "source": [
    "# The CPU vs GPU Showdown: A Simple Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2356d17b",
   "metadata": {},
   "source": [
    "Enough theory, let’s see these two in action! We will conduct a simple experiment using PyTorch and Matplotlib to illustrate the difference in execution times between CPU and GPU. But remember, your mileage may vary based on the specific task and available hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb65da0c",
   "metadata": {},
   "source": [
    "Sadly, we will have to exclude the TPU in this test, as running on TPUs in PyTorch is complicated since it’s not natively supported."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afd1a49",
   "metadata": {},
   "source": [
    "Please make sure to run the below code in a system that has both a CPU and GPU, or else the GPU portion won’t work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f35c840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check for CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    print('CUDA is not available. The script will run on a CPU.')\n",
    "device_cpu = torch.device(\"cpu\")\n",
    "device_gpu = torch.device(\"cuda\") \n",
    "\n",
    "# Function to test speed\n",
    "def speed_test(device, size=10000, runs=100):\n",
    "    print(f'\\nRunning on {device}')\n",
    "    times = []\n",
    "    for _ in range(runs):\n",
    "        start = time.time()\n",
    "        _ = torch.ones((size, size), device=device)\n",
    "        elapsed = time.time() - start\n",
    "        times.append(elapsed)\n",
    "    return times\n",
    "\n",
    "# Running the speed test\n",
    "cpu_times = speed_test(device_cpu)\n",
    "gpu_times = speed_test(device_gpu)\n",
    "\n",
    "# Printing the average execution time\n",
    "print(\"\\nAverage execution time on CPU: \", sum(cpu_times)/len(cpu_times), \" seconds\")\n",
    "print(\"Average execution time on GPU: \", sum(gpu_times)/len(gpu_times), \" seconds\")\n",
    "\n",
    "# Visualizing the results\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "plt.hist(cpu_times, bins=30, alpha=0.5, label='CPU')\n",
    "plt.hist(gpu_times, bins=30, alpha=0.5, label='GPU')\n",
    "\n",
    "plt.xlabel('Execution Time (Seconds)', size=14)\n",
    "plt.ylabel('Count', size=14)\n",
    "plt.title('Execution Time Distribution by Hardware (CPU vs GPU)', size=14)\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334e4fa3",
   "metadata": {},
   "source": [
    "This script creates a tensor of ones on the specified device (CPU or GPU) over and over again, recording the time for each run. These times are then visualized in a handy histogram. From this, we can see the difference in execution times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd0c0ac",
   "metadata": {},
   "source": [
    "But always remember, this is a simple, naive example and real-world tasks will often involve much more complex computations and data transfers, which can increase the execution time when run on a GPU or TPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4410f3",
   "metadata": {},
   "source": [
    "# Bringing ML into the Picture: A Natural Language Processing Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed1e5b6",
   "metadata": {},
   "source": [
    "Let’s raise the stakes a bit and include a typical machine learning task: text classification. This time, we’ll be using the Hugging Face Transformers library with the popular BERT model. We’ll run this model on both CPU and GPU and compare execution times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afa959c",
   "metadata": {},
   "source": [
    "Let’s witness the showdown between CPU and GPU in a real-life scenario!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f68baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f6769c",
   "metadata": {},
   "source": [
    "After installing the Transformers library, it’s time to get the model humming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6c9f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check for CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    print('CUDA is not available. The script will run on a CPU.')\n",
    "device_cpu = 'cpu'\n",
    "device_gpu = 'cuda' \n",
    "\n",
    "# Function to test speed and print pipeline results\n",
    "def speed_test(device, runs=100):\n",
    "    print(f'\\nRunning on {device}')\n",
    "    nlp = pipeline('sentiment-analysis', model=\"distilbert-base-uncased-finetuned-sst-2-english\", device=0 if device == device_gpu else -1)  # GPU if available, else CPU\n",
    "    text = \"This is a sample text for sentiment analysis.\"\n",
    "    times = []\n",
    "    for _ in range(runs):\n",
    "        start = time.time()\n",
    "        result = nlp(text)\n",
    "        elapsed = time.time() - start\n",
    "        times.append(elapsed)\n",
    "        if _ == 0:\n",
    "            print(\"Pipeline output for run 0: \", result)\n",
    "    return times\n",
    "\n",
    "# Running the speed test\n",
    "cpu_times = speed_test(device_cpu)\n",
    "gpu_times = speed_test(device_gpu)\n",
    "\n",
    "# Printing the average execution time\n",
    "print(\"\\nAverage execution time on CPU: \", sum(cpu_times)/len(cpu_times), \" seconds\")\n",
    "print(\"Average execution time on GPU: \", sum(gpu_times)/len(gpu_times), \" seconds\")\n",
    "\n",
    "# Visualizing the results\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "plt.hist(cpu_times, bins=30, alpha=0.5, label='CPU')\n",
    "plt.hist(gpu_times, bins=30, alpha=0.5, label='GPU')\n",
    "\n",
    "plt.xlabel('Execution Time (Seconds)', size=14)\n",
    "plt.ylabel('Count', size=14)\n",
    "plt.title('Execution Time Distribution by Hardware (CPU vs GPU)', size=14)\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25020cbd",
   "metadata": {},
   "source": [
    "This script performs sentiment analysis on a piece of text using the powerful BERT model, records the execution time on both the CPU and GPU, and finally plots a histogram to compare the execution time distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5190197f",
   "metadata": {},
   "source": [
    "# Real-World Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e738a3d6",
   "metadata": {},
   "source": [
    "In the real world, the choice between CPUs, GPUs, and TPUs depends largely on the task at hand and the available resources. For instance, in image rendering or deep learning tasks, which require handling a massive volume of parallel computations, GPUs and TPUs tend to be the preferred choice. On the other hand, for general purpose tasks or when the resources are limited, CPUs might be the way to go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda02017",
   "metadata": {},
   "source": [
    "# Future Trends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb458574",
   "metadata": {},
   "source": [
    "The future of machine learning hardware is likely to see new entrants. As the demand for high-performance computing continues to grow, companies may start developing their own application-specific processors, similar to Google’s TPUs. Furthermore, advancements in hardware design, such as implementing quantum computing for machine learning tasks, could also revolutionize the landscape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e14290",
   "metadata": {},
   "source": [
    "# In a Nutshell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd8d709",
   "metadata": {},
   "source": [
    "To summarize, the choice between CPUs, GPUs, and TPUs for machine learning tasks largely depends on the nature of the task and the resources at hand. CPUs are general-purpose processors and perform well for single-thread tasks. GPUs, originally designed for graphics rendering, excel in parallel computations, making them suitable for large-scale machine learning tasks. TPUs, specifically designed for machine learning tasks, boast incredible speed but are not as versatile as CPUs or GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff16f8c",
   "metadata": {},
   "source": [
    "We hope this guide has illuminated the differences between these three types of hardware and will help you make an informed decision for your next machine learning project!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
