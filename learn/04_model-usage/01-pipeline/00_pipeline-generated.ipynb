{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34bc1462",
   "metadata": {},
   "source": [
    "source: [link](https://ai-bootcamp.ruangguru.com/learn/04_model-usage/01-pipeline/00_pipeline.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2594dbe0",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7506e2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dc9cd46",
   "metadata": {},
   "source": [
    "![Image](https://storage.googleapis.com/rg-ai-bootcamp/model_usage/pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e656f47e",
   "metadata": {},
   "source": [
    "Dive into the world of Pipelines, your new best friend for model inference. They function as an expressway, simplifying convoluted code and turning complex features into user-friendly tools. Each pipeline is designed to offer a streamlined API that caters to an array of tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b630e658",
   "metadata": {},
   "source": [
    "They are versatile and cater to a variety of data types, including:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c3237a",
   "metadata": {},
   "source": [
    "Harness the power of pipelines and elevate your AI projects with their effortless handling and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a8a930",
   "metadata": {},
   "source": [
    "# Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee19fb3",
   "metadata": {},
   "source": [
    "Pipelines are your secret weapon for efficient AI development. Here’s why:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e937a6",
   "metadata": {},
   "source": [
    "Are there any veterans in the house who’ve experienced the power of pipelines in language processing libraries? We’d love to hear your tales from the trenches!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b1286c",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fef406",
   "metadata": {},
   "source": [
    "To use the Hugging Face pipelines, import the pipeline class and specify the task you want to perform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbcd5b7",
   "metadata": {},
   "source": [
    "# Harnessing pipeline() for Text Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12edb70",
   "metadata": {},
   "source": [
    "Embark on a text analysis voyage with the pipeline() function. It’s geared to cater to a variety of text-related tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921ae8d1",
   "metadata": {},
   "source": [
    "Ready to explore? Let’s dive in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859de762",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers dataset evaluate\n",
    "!pip install -q torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd75ecd",
   "metadata": {},
   "source": [
    "Make sure you initialize your notebook with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983cd969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f\"Found {device_count} CUDA device(s) available.\")\n",
    "    for i in range(device_count):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"No CUDA devices available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6cd8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "No CUDA devices available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652b60db",
   "metadata": {},
   "source": [
    "Later, you’ll see that we use pipeline like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b81c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\", device=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53f5cfc",
   "metadata": {},
   "source": [
    "device=0 refer to the key number of the GPU that we want to use if we have multiple GPU available. You can check your available GPU(s) by running nvidia-smi command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27323866",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b7a6540",
   "metadata": {},
   "source": [
    "![Image](https://www.servethehome.com/wp-content/uploads/2021/07/8x-NVIDIA-A100-500W-nvidia-smi-output.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee27ad05",
   "metadata": {},
   "source": [
    "Run this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70bcdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def sentiment_analysis(text, device):\n",
    "    device = 0 if device else \"cpu\"\n",
    "    run = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\", device=0)\n",
    "    result = run(text)\n",
    "    return result\n",
    "\n",
    "def zero_shot_classification(text, kelas_zero_shot, device):\n",
    "    device = 0 if device else \"cpu\"\n",
    "    run = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0)\n",
    "    result = run(text, candidate_labels=kelas_zero_shot.split(\",\"))\n",
    "    return result\n",
    "\n",
    "def text_generation(text, device):\n",
    "    device = 0 if device else \"cpu\"\n",
    "    run = pipeline(\"text-generation\", model=\"gpt2\", max_new_tokens=100, device=0)\n",
    "    result = run(text)\n",
    "    return result\n",
    "\n",
    "def fill_mask(text, device):\n",
    "    device = 0 if device else \"cpu\"\n",
    "    run = pipeline(\"fill-mask\", model=\"distilroberta-base\", device=0)\n",
    "    result = run(text)\n",
    "    return result\n",
    "\n",
    "def ner(text, device):\n",
    "    device = 0 if device else \"cpu\"\n",
    "    run = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", device=0)\n",
    "    result = run(text)\n",
    "    return result\n",
    "\n",
    "def translation_en_to_fr(text, device):\n",
    "    device = 0 if device else \"cpu\"\n",
    "    run = pipeline(\"translation_en_to_fr\", model=\"t5-base\", device=0)\n",
    "    result = run(text)\n",
    "    return result\n",
    "    \n",
    "\n",
    "def func_pipeline(numb_task: int):\n",
    "    if numb_task == \"1\":\n",
    "        print(\"Example input: I've been waiting for a HuggingFace course my whole life.\")\n",
    "        input_text = input(\"Input: \")\n",
    "        device = input(\"Use GPU? (True or False)\")\n",
    "        return sentiment_analysis(input_text, device)\n",
    "    elif numb_task == \"2\":\n",
    "        print(\"Example input: This is a course about the Transformers library\")\n",
    "        input_text = input(\"Input: \")\n",
    "        kelas_zero_shot = input(\"Input Zero Shot Class: \")\n",
    "        device = input(\"Use GPU? (True or False)\")\n",
    "        return zero_shot_classification(input_text, kelas_zero_shot, device)\n",
    "    elif numb_task == \"3\":\n",
    "        print(\"Example input: In this course, we will teach you how to\")\n",
    "        input_text = input(\"Input: \")\n",
    "        device = input(\"Use GPU? (True or False)\")\n",
    "        return text_generation(input_text, device)\n",
    "    elif numb_task == \"4\":\n",
    "        print(\"Example input: This course will teach you all about <mask> models\")\n",
    "        input_text = input(\"Input: \")\n",
    "        device = input(\"Use GPU? (True or False)\")\n",
    "        return fill_mask(input_text, device)\n",
    "    elif numb_task == \"5\":\n",
    "        print(\"Example input: My name is Sylvain and I work at Hugging Face in Brooklyn\")\n",
    "        input_text = input(\"Input: \")\n",
    "        device = input(\"Use GPU? (True or False)\")\n",
    "        return ner(input_text, device)\n",
    "    elif numb_task == \"6\":\n",
    "        print(\"Example input: How are you?\")\n",
    "        input_text = input(\"Input: \")\n",
    "        device = input(\"Use GPU? (True or False)\")\n",
    "        return translation_en_to_fr(input_text, device)\n",
    "    else:\n",
    "        return \"Task not found!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91561e66",
   "metadata": {},
   "source": [
    "And try this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582e831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_task = input(\"Select number task: \")\n",
    "# Task list:\n",
    "# - 1. sentiment-analysis\n",
    "# - 2. zero-shot-classification\n",
    "# - 3. text-generation\n",
    "# - 4. fill-mask\n",
    "# - 5. ner\n",
    "# - 6. translation_en_to_fr\n",
    "print(func_pipeline(input_task))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a547ffcb",
   "metadata": {},
   "source": [
    "# Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2387b5d4",
   "metadata": {},
   "source": [
    "You’re trying to understand if a sentence carries a positive or negative sentiment, but the coding process seems daunting? 🤔 Fear not! By utilizing Hugging Face’s pipeline() function, you can easily tackle this. The following code simplifies the process:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e12605",
   "metadata": {},
   "source": [
    "This piece of code creates a tool that can determine the sentiment of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d86802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfa56ea",
   "metadata": {},
   "source": [
    "Now, let’s use this tool to analyze a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b236b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2149af06",
   "metadata": {},
   "outputs": [],
   "source": [
    "[{'label': 'POSITIVE', 'score': 0.9598049521446228}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547f86ec",
   "metadata": {},
   "source": [
    "What about multiple sentences? No problem: But this time, lets count the time which model took to do inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c2effa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dbd7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bca4e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n",
    " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86068c5e",
   "metadata": {},
   "source": [
    "All the complex computations happen behind the scenes. The pipeline function and the pre-trained model give you a sentiment label (positive/negative), and a confidence score for each text. Simple, isn’t it? 😎"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acc8662",
   "metadata": {},
   "source": [
    "Okay we got the result and also the time taken by model to do inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a4aafb",
   "metadata": {},
   "source": [
    "Can we improve the inference time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec32a826",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Yes we can, if you have GPU, the easiest one will be utilize the GPU to improve the performance\n",
    "## Now lets add GPU to our pipeline\n",
    "classifier_gpu = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2050a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier_gpu(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501e0786",
   "metadata": {},
   "source": [
    "Wait !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c8ee28",
   "metadata": {},
   "source": [
    "How about its performance in language which not in the model’s pretraining data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705dc73b",
   "metadata": {},
   "source": [
    "(BERT/distill-BERT pretraining data only consists of English Language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40cc4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Example dataset from indonlp/indonlu - smsa\n",
    "sentence_id = [\n",
    "\"menu warkop , tempat gaul . paling enak ke sini pagi hari masih sepi . kalau siang - malam ramai bingitzzxz . cocok untuk anak baru besar atau remaja . buat bincang bisnis tidak cocok tetlalu ramai . kopi nya gayo arabica mantap , sayang tidak ada manual brew . semuanya pakai expresso mesin . buat kawula muda cocoklah\"\n",
    " ]\n",
    "\n",
    "classifier_gpu(sentence_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623057b0",
   "metadata": {},
   "source": [
    "What do you think 🤔 It sure the label should be positive right? Lets try with finetuned model which used bahasa indonesia dataset as the pretraining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e2457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier_id = pipeline(\"sentiment-analysis\", model=\"ayameRushia/roberta-base-indonesian-1.5G-sentiment-analysis-smsa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b942f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_id(sentence_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c32348",
   "metadata": {},
   "source": [
    "# Zero-shot classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1867392f",
   "metadata": {},
   "source": [
    "You have a statement and you’re curious about which category it belongs to: education, politics, or business. However, you find the coding part a little tricky, right? 🤔 No worries! Using Hugging Face’s pipeline() function, you can easily classify your statement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd193cc",
   "metadata": {},
   "source": [
    "Let’s break it down with the following code, We start by creating a tool for classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38934d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3cac16",
   "metadata": {},
   "source": [
    "Next, let’s classify a sentence into one of the three categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7218aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fea2516",
   "metadata": {},
   "source": [
    "What’s happening behind the scenes? The pipeline function and the specified model combine their powers to classify the provided statement into either “education”, “politics”, or “business”, even though it hasn’t seen these labels before during its training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d4bcac",
   "metadata": {},
   "source": [
    "This process is called ‘zero-shot classification’, and it’s cool because the model is making an educated guess based on its understanding of language and the relationships between words. Isn’t that amazing? 😎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af5cd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Alternative, you can use model which used Bahasa Indonesia as pretraining dataset, here we have \"ilos-vigil/bigbird-small-indonesian-nli\"\n",
    "classifier_zero_id = pipeline(\"zero-shot-classification\", model=\"ilos-vigil/bigbird-small-indonesian-nli\", device=0)\n",
    "\n",
    "classifier_zero_id(\n",
    "    \"Saya sangat setuju dengan pembangunan gedung perkantoran\",\n",
    "    candidate_labels=[\"office space\", \"housing\", \"mall\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23580530",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f989922",
   "metadata": {},
   "source": [
    "You’re interested in generating new text based on a given input, but the coding details seem overwhelming, right? 🤔 Don’t fret! With Hugging Face’s pipeline() function, you can generate text with ease. Check out how we simplify it below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f541de9",
   "metadata": {},
   "source": [
    "First, we create a tool that can generate text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8878848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534911cd",
   "metadata": {},
   "source": [
    "Next, let’s use this tool to generate some text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a090b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(\"In this course, we will teach you how to\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251c4d52",
   "metadata": {},
   "source": [
    "Here’s what happens: the generator, built with a pre-trained model, uses the string “In this course, we will teach you how to” as a starting point and generates new text from it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be316673",
   "metadata": {},
   "source": [
    "Now, let’s add a few more features to our text generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9290ff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301e4f42",
   "metadata": {},
   "source": [
    "In this case, the max_length=30 tells the generator that the text should not exceed 30 tokens. num_return_sequences=2 asks the generator to provide two possible continuations of the input text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6561e25",
   "metadata": {},
   "source": [
    "So with the help of these lines of code, you get to generate one or more possible continuations of the given string. Isn’t that neat? 😎"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24efa848",
   "metadata": {},
   "source": [
    "For each task, you can search the model hub for various models you can use in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d7596e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31812852",
   "metadata": {},
   "source": [
    "![Image](https://storage.googleapis.com/rg-ai-bootcamp/model_usage/huggingface-models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3322e958",
   "metadata": {},
   "source": [
    "# Text completion (mask filling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ae13d9",
   "metadata": {},
   "source": [
    "You’re trying to complete a sentence that has a missing word but the coding part seems a bit difficult, doesn’t it? 🤔 No stress! With pipeline() function from Hugging Face, you can fill in the blanks easily. Here’s the simplified version with the code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4168e2b",
   "metadata": {},
   "source": [
    "Let’s start by creating a tool that will fill in the masked word(s) in a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54707d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a6672a",
   "metadata": {},
   "source": [
    "Next, we use this tool to find the missing word in a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52a9c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasker(\"This course will teach you all about <mask> models\", top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e8351f",
   "metadata": {},
   "source": [
    "Here’s the magic behind it: The unmasker, equipped with a pre-trained model, looks at the sentence “This course will teach you all about <mask> models”. It understands the <mask> as a placeholder for a word that it needs to predict based on the rest of the sentence. The top_k=2 parameter tells it to come up with the top two most likely words that could fill the blank. Cool, isn’t it? 😎"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e2e4bf",
   "metadata": {},
   "source": [
    "# Token classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863565e1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe60fce7",
   "metadata": {},
   "source": [
    "![Image](https://storage.googleapis.com/rg-ai-bootcamp/model_usage/ner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4893c91d",
   "metadata": {},
   "source": [
    "You have a sentence with names, locations, or organizations, and you’d like to identify each one of them. But the coding aspect seems a bit daunting, right? 🤔 No worries! With Hugging Face’s pipeline() function, you can easily identify these entities. Let’s simplify it with this code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934f47d3",
   "metadata": {},
   "source": [
    "First, create a tool that can recognize named entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9fa744",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\"ner\", grouped_entities=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5689d6ee",
   "metadata": {},
   "source": [
    "Next, use this tool to find the named entities in a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3830e3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28357cc",
   "metadata": {},
   "source": [
    "Here’s what happens: The ner tool, built with a pre-trained model, will analyze the sentence “My name is Sylvain and I work at Hugging Face in Brooklyn”. It identifies named entities like “Sylvain”, “Hugging Face”, and “Brooklyn”, and labels them appropriately as ‘person’, ‘organization’, ‘location’ based on their contexts. The grouped_entities=True parameter allows the model to group related words into a single entity. The result? A list of detected entities each associated with a label and a confidence score. Neat, huh? 😎"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63098567",
   "metadata": {},
   "source": [
    "# Question answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061169aa",
   "metadata": {},
   "source": [
    "You have a piece of text and a question related to it, but finding the answer within that text seems a bit daunting, right? 🤔 No worries! With Hugging Face’s pipeline() function, you can easily find the answer. Let’s simplify it with the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9d5494",
   "metadata": {},
   "source": [
    "We start by creating a tool that can answer questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62608d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b6155f",
   "metadata": {},
   "source": [
    "Next, we use it to find the answer to a question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef9ca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answerer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab5e99b",
   "metadata": {},
   "source": [
    "Here’s the neat part: the question_answerer tool, equipped with a pre-trained model, will analyze the context “My name is Sylvain and I work at Hugging Face in Brooklyn” to find the answer to the question “Where do I work?”. The returned result will be the answer to the question, extracted directly from the provided context, along with a confidence score on how sure it is about the answer. Amazing, isn’t it? 😎"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71255e2",
   "metadata": {},
   "source": [
    "# Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094e1d7b",
   "metadata": {},
   "source": [
    "You have a somewhat lengthy piece of text and you want to find a concise summary without going through the hassle of coding, right? 🤔 Say no more! Using Hugging Face’s pipeline() function, you can easily generate a summary. Here’s how we simplify it with the code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa598f8",
   "metadata": {},
   "source": [
    "First, create a tool that can summarize text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5518ac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fbb5b4",
   "metadata": {},
   "source": [
    "Next, let’s use it to generate a summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b182fd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer(\"\"\"\n",
    "    New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "    A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "    Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "    In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c148f31",
   "metadata": {},
   "source": [
    "Here’s the cool part: the summarizer tool, built using a pre-trained model, will analyze the provided text and create a shorter summary that maintains the main idea and essential details of the original text, making it much easier for you to get the gist without having to read the entire passage. Efficient, right? 😎"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d753c6be",
   "metadata": {},
   "source": [
    "# Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fd2dc9",
   "metadata": {},
   "source": [
    "You have an English phrase and you want to translate it into French, but the coding part seems challenging, doesn’t it? 🤔 Worry not! With Hugging Face’s pipeline() function, you can get your translation swiftly. Here is the simplified code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f62a19",
   "metadata": {},
   "source": [
    "Firstly, create a tool that can translate English to French:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0705cc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation_en_to_fr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cab8b6",
   "metadata": {},
   "source": [
    "Next, use this tool to translate an English phrase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554be1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator(\"How are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7240ba54",
   "metadata": {},
   "source": [
    "Behind the scenes, the translator tool, equipped with a pre-trained model, will take the English phrase “How are you?” and translate it into French. The result? Your English text smoothly translated to French. Amazing, isn’t it? 😎"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf39d9f1",
   "metadata": {},
   "source": [
    "# Using pipeline() for audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f9031e",
   "metadata": {},
   "source": [
    "You have an audio file and you’d like to convert the spoken language into written text. However, the coding process feels a bit confusing, right? 🤔 No worries! Using Hugging Face’s pipeline() function, you can transcribe the audio effortlessly. Let’s look at the simplified code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fd4dd8",
   "metadata": {},
   "source": [
    "First, install the needed ffmpeg package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e159f072",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184387c1",
   "metadata": {},
   "source": [
    "Next, create a tool that can transcribe audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656bc61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "transcriber = pipeline(model=\"openai/whisper-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbda5fc",
   "metadata": {},
   "source": [
    "Finally, use this tool to transcribe an audio file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e08a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2aa7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "/home/aditira/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
    "  warnings.warn("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6400af",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'text': ' He hoped there would be stew for dinner, turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick, peppered flour-fatten sauce.'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c874997e",
   "metadata": {},
   "source": [
    "In essence, the transcriber tool, built with the model “openai/whisper-base”, processes the audio file from the given URL and transcribes the spoken language into written text. The result is a written transcription of what’s being said in the audio file. Easy peasy, isn’t it? 😎"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9ef569",
   "metadata": {},
   "source": [
    "# Using pipeline() for visual:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb30ece",
   "metadata": {},
   "source": [
    "You have an image and you want to classify it or detect objects within it, but the coding part seems a bit overbearing, doesn’t it? 🤔 Fear not! Hugging Face’s pipeline() function allows you to classify images and detect objects with ease. Let’s break down the process:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64a9f11",
   "metadata": {},
   "source": [
    "Firstly, let’s see how to classify an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3043f0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from IPython.display import display\n",
    "\n",
    "# Create a classification pipeline\n",
    "classifier = pipeline(model=\"microsoft/beit-base-patch16-224-pt22k-ft22k\")\n",
    "\n",
    "# Read the image using PIL\n",
    "img_url = \"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\"\n",
    "response = requests.get(img_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "# Display the image\n",
    "display(img)\n",
    "\n",
    "# Classify the image\n",
    "classifier(img_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98412f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c308297",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3d26a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "[{'score': 0.4424389898777008, 'label': 'macaw'},\n",
    " {'score': 0.08767358958721161, 'label': 'popinjay'},\n",
    " {'score': 0.07458024471998215, 'label': 'parrot'},\n",
    " {'score': 0.07278897613286972, 'label': 'parodist, lampooner'},\n",
    " {'score': 0.04623128101229668, 'label': 'poll, poll_parrot'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfb93b5",
   "metadata": {},
   "source": [
    "Here, the classifier tool, set up with the model “microsoft/beit-base-patch16-224-pt22k-ft22k”, classifies the image at the given URL. The result is a label indicating the main subject of the image and a confidence score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba061109",
   "metadata": {},
   "source": [
    "Now, let’s classify the image with specific candidate labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98993238",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(model=\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# Read and display the image\n",
    "response = requests.get(img_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "display(img)\n",
    "\n",
    "# Classify the image with specific candidate labels\n",
    "classifier(\n",
    "    img_url,\n",
    "    candidate_labels=[\"animals\", \"humans\", \"landscape\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fee023",
   "metadata": {},
   "outputs": [],
   "source": [
    "2023-08-01 14:01:02.028179: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
    "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
    "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
    "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f2352e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bc42b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "[{'score': 0.9651921987533569, 'label': 'animals'},\n",
    " {'score': 0.029521869495511055, 'label': 'humans'},\n",
    " {'score': 0.0052859727293252945, 'label': 'landscape'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5070b62",
   "metadata": {},
   "source": [
    "In this case, the classifier tool, set up with the model “openai/clip-vit-large-patch14”, is again used to classify the image. However, this time we provide specific labels (“animals”, “humans”, “landscape”) to guide the classification. The result is a label from our candidate list and a confidence score. Incredible, right? 😎"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d69d6b",
   "metadata": {},
   "source": [
    "# Additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c593190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# example in sentiment analysis\n",
    "model_name_sa = \"Bhumika/roberta-base-finetuned-sst2\"\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model_name_sa)\n",
    "classifier(\"good job on solving those bugs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d76b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "/Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
    "  from .autonotebook import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2a6f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[{'label': 'LABEL_1', 'score': 0.999832272529602}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62672dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoConfig, AutoTokenizer, pipeline\n",
    "\n",
    "#  add id to label for model config\n",
    "id2label = {\n",
    "    0: \"negative\",\n",
    "    1: \"positive\"\n",
    "  }\n",
    "\n",
    "#  add label to id for model config\n",
    "label2id= {\n",
    "    \"positive\": 1,\n",
    "    \"negative\": 0\n",
    "  }\n",
    "\n",
    "#  load config of the pretrained model\n",
    "config = AutoConfig.from_pretrained(model_name_sa)\n",
    "\n",
    "# add your id2label and label2id to model's config\n",
    "config.id2label = id2label\n",
    "config.label2id = label2id\n",
    "\n",
    "#  load tokenizer and model with new config\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name_sa, config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_sa)\n",
    "\n",
    "#  create your pipeline with defined model and tokenizer\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "classifier(\"good job on solving those bugs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01ebe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "[{'label': 'positive', 'score': 0.999832272529602}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cbab48",
   "metadata": {},
   "source": [
    "# Exercise Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b6cfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rggrader sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe32979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title #### Student Identity\n",
    "student_id = \"student_id\" # @param {type:\"string\"}\n",
    "name = \"your_name\" # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19da685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title #### 00. Chat Application\n",
    "from rggrader import submit\n",
    "\n",
    "# Imagine a scenario where you have a chatbot application capable of translating a message from Indonesian to English, generating a response, and analyzing the sentiment of the response. To achieve this, you decide to use HuggingFace's pipeline() function.\n",
    "\n",
    "# TODO: Create a function that receives an Indonesian chat message as an argument, translates it into English, generates a response, and performs sentiment analysis on the response. For each of these tasks, use specific models with HuggingFace's pipeline() function.\n",
    "# - For translation, use \"Helsinki-NLP/opus-mt-id-en\".\n",
    "# - For text-generation, use \"gpt2\".\n",
    "# - For sentiment-analysis, use \"distilbert-base-uncased-finetuned-sst-2-english\".\n",
    "# The function should return a dictionary containing the original Indonesian chat, the English translation, the generated response, the sentiment and the associated score.\n",
    "\n",
    "def chat_application(chat:str):\n",
    "  # Put your code here:\n",
    "  translation = \"\"\n",
    "  response = \"\"\n",
    "  sentiment = \"\"\n",
    "    \n",
    "  return {\n",
    "    \"Indonesian Chat\": chat,\n",
    "    \"Translation\": translation,\n",
    "    \"Response\": response,\n",
    "    \"Sentiment\": sentiment['label'],\n",
    "    \"Score\": sentiment['score']\n",
    "  }\n",
    "  # ---- End of your code ----\n",
    "\n",
    "# Process all chats\n",
    "indonesian_chats = [\n",
    "    \"Ceritakan kisah inspiratif tentang mengatasi tantangan.\",\n",
    "    \"Apa dampak negatif dari Perang Dunia II?\",\n",
    "]\n",
    "results = []\n",
    "submit_results = []\n",
    "for chat in indonesian_chats:\n",
    "  result = chat_application(chat) \n",
    "  results.append(result)\n",
    "  \n",
    "  # Only submit Translation and Sentiment\n",
    "  submit_results.append({\"Translation\": result[\"Translation\"], \"Sentiment\": result[\"Sentiment\"]})\n",
    "\n",
    "# Print all results\n",
    "for result in results:\n",
    "  print(\"---\\nIndonesian Chat: {0}\\nTranslation: {1}\\nResponse: {2}\\nSentiment: {3}, Score: {4}\\n---\".format(\n",
    "      result[\"Indonesian Chat\"],\n",
    "      result[\"Translation\"],\n",
    "      result[\"Response\"],\n",
    "      result[\"Sentiment\"],\n",
    "      result[\"Score\"]\n",
    "  ))\n",
    "  \n",
    "\n",
    "# Submit Method\n",
    "assignment_id = \"00_pipeline\"\n",
    "question_id = \"00_chat_application\"\n",
    "submit(student_id, name, assignment_id, str(submit_results), question_id)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
