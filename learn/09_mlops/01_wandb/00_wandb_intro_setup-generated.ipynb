{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b3d7613",
   "metadata": {},
   "source": [
    "source: [link](https://ai-bootcamp.ruangguru.com/learn/09_mlops/01_wandb/00_wandb_intro_setup.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6049ff9",
   "metadata": {},
   "source": [
    "# Introduction to WanDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e34ed4b",
   "metadata": {},
   "source": [
    "# A. What is WanDB?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897dea84",
   "metadata": {},
   "source": [
    "Weights & Biases (WanDB) is a developer-oriented toolset, designed specifically for machine learning. It helps monitor and visualize the model’s training process and its performance in a more intuitive way. WanDB provides a centralized platform where teams can log, share, and collaborate on their machine learning projects, making it easier to compare different runs and models’ performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cfb3d1",
   "metadata": {},
   "source": [
    "# B. Why use WanDB?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5303282b",
   "metadata": {},
   "source": [
    "There are several reasons why WanDB stands out as a preferred tool for machine learning projects:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8067fe",
   "metadata": {},
   "source": [
    "Track and Visualize Models: WanDB provides a simple way to track every detail of your experiment, providing real-time visualization of your models’ training and results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9262bb3",
   "metadata": {},
   "source": [
    "Hyperparameter Optimization: With WanDB’s Sweeps, you can automate hyperparameter tuning and explore the parameter space more efficiently to optimize your model’s performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527f7058",
   "metadata": {},
   "source": [
    "Collaboration: WanDB makes it easy to share your experiment results with colleagues and the community, fostering collaboration and knowledge sharing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b8314e",
   "metadata": {},
   "source": [
    "Reproducibility: By logging all the metadata from your runs, WanDB helps maintain the reproducibility of your experiments, which is crucial in machine learning projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3411f4",
   "metadata": {},
   "source": [
    "# C. Understanding the Importance of Monitoring and Improving Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f61ffe",
   "metadata": {},
   "source": [
    "Monitoring and improving model training is an essential part of the machine learning workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4b8453",
   "metadata": {},
   "source": [
    "- Monitoring: By keeping track of various metrics such as loss and accuracy during the training process, you can understand how well your model is learning and whether it’s improving over time. This can help you detect issues like overfitting or underfitting early on and take corrective actions.\n",
    "- Improving: Once you monitor your model’s performance, the next step is to improve it. This could involve tweaking the model’s architecture, optimizing hyperparameters, or using more/ different data for training. Tools like WanDB make it easier to experiment with these aspects and track the impact of each change, thereby helping you build better models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df76eba",
   "metadata": {},
   "source": [
    "Monitoring: By keeping track of various metrics such as loss and accuracy during the training process, you can understand how well your model is learning and whether it’s improving over time. This can help you detect issues like overfitting or underfitting early on and take corrective actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207357a2",
   "metadata": {},
   "source": [
    "Improving: Once you monitor your model’s performance, the next step is to improve it. This could involve tweaking the model’s architecture, optimizing hyperparameters, or using more/ different data for training. Tools like WanDB make it easier to experiment with these aspects and track the impact of each change, thereby helping you build better models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a41c79",
   "metadata": {},
   "source": [
    "# Setting up WanDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae56f22",
   "metadata": {},
   "source": [
    "# A. Account Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69961d7",
   "metadata": {},
   "source": [
    "To get started with WanDB, you need to create a free account:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481ee121",
   "metadata": {},
   "source": [
    "# B. Installing the Wandb library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8316e75",
   "metadata": {},
   "source": [
    "Once you’ve set up your account, you need to install the Wandb library in your Python environment. It can be installed using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053fe5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34bfbfe",
   "metadata": {},
   "source": [
    "Or with conda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099806d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478d39ee",
   "metadata": {},
   "source": [
    "Ensure you have the latest version of the library for optimal functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78600aa7",
   "metadata": {},
   "source": [
    "# C. Initializing WanDB in Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245d68e1",
   "metadata": {},
   "source": [
    "After installing the Wandb library, you need to import it and initialize it within your project. Start by importing wandb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3658567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2c5647",
   "metadata": {},
   "source": [
    "Then, initialize wandb with wandb.init(). You can pass several optional parameters to wandb.init(), such as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5ee9fa",
   "metadata": {},
   "source": [
    "- project: The name of the project where you’re logging runs. This could be any string, and a new project will be created if it doesn’t already exist.\n",
    "- entity: The username or team name under which the project is to be created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe40308",
   "metadata": {},
   "source": [
    "An example of initializing Wandb for a project named ‘my_project’ under username ‘my_username’ would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05342aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project='my_project', entity='my_username')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e1523c",
   "metadata": {},
   "source": [
    "After running wandb.init(), a new run will be created on the WanDB website, where you can track your model’s progress, visualize results, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cb871b",
   "metadata": {},
   "source": [
    "Let’s create a simple project that utilizes WandB (Weights and Biases) for experiment tracking. This project will be about classifying the CIFAR-10 dataset using a Convolutional Neural Network (CNN) implemented in Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75914c92",
   "metadata": {},
   "source": [
    "# Project: CIFAR-10 Image Classification with Pytorch and WandB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb05e9dd",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188a2902",
   "metadata": {},
   "source": [
    "The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. The goal of this project is to classify the images into these classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ccfb74",
   "metadata": {},
   "source": [
    "We will use a Convolutional Neural Network (CNN) in Pytorch to perform this classification task. The model’s performance will be logged and visualized using WandB, a tool for machine learning experiment tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7539f2b3",
   "metadata": {},
   "source": [
    "# Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4c9ca5",
   "metadata": {},
   "source": [
    "# 1. Setting Up the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3939cea0",
   "metadata": {},
   "source": [
    "Install the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd95d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf03219",
   "metadata": {},
   "outputs": [],
   "source": [
    "Requirement already satisfied: torch in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (2.0.1)\n",
    "Requirement already satisfied: torchvision in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (0.15.2)\n",
    "Requirement already satisfied: wandb in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (0.16.4)\n",
    "Requirement already satisfied: filelock in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from torch) (3.12.3)\n",
    "Requirement already satisfied: typing-extensions in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from torch) (4.7.1)\n",
    "Requirement already satisfied: sympy in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from torch) (1.12)\n",
    "Requirement already satisfied: networkx in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from torch) (3.1)\n",
    "Requirement already satisfied: jinja2 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from torch) (3.1.2)\n",
    "Requirement already satisfied: numpy in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from torchvision) (1.25.2)\n",
    "Requirement already satisfied: requests in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from torchvision) (2.31.0)\n",
    "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from torchvision) (10.0.0)\n",
    "Requirement already satisfied: Click!=8.0.0,>=7.1 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from wandb) (8.1.7)\n",
    "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from wandb) (3.1.36)\n",
    "Requirement already satisfied: psutil>=5.0.0 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from wandb) (5.9.5)\n",
    "Requirement already satisfied: sentry-sdk>=1.0.0 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from wandb) (1.40.6)\n",
    "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from wandb) (0.4.0)\n",
    "Requirement already satisfied: PyYAML in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from wandb) (6.0.1)\n",
    "Requirement already satisfied: setproctitle in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from wandb) (1.3.3)\n",
    "Requirement already satisfied: setuptools in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from wandb) (68.2.2)\n",
    "Requirement already satisfied: appdirs>=1.4.3 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from wandb) (1.4.4)\n",
    "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from wandb) (4.24.3)\n",
    "Requirement already satisfied: six>=1.4.0 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
    "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
    "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from requests->torchvision) (3.2.0)\n",
    "Requirement already satisfied: idna<4,>=2.5 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from requests->torchvision) (3.4)\n",
    "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from requests->torchvision) (2.0.4)\n",
    "Requirement already satisfied: certifi>=2017.4.17 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from requests->torchvision) (2024.2.2)\n",
    "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
    "Requirement already satisfied: mpmath>=0.19 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
    "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/ruangguru/Projects/ai-bootcamp/env/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
    "\n",
    "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
    "[notice] To update, run: pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d009e7",
   "metadata": {},
   "source": [
    "# 2. Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ef74f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import wandb\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e70b73",
   "metadata": {},
   "source": [
    "# 3. Initialize WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aa802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "run = wandb.init(project='cifar10-classification', entity='ricky-kurniawan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1670d406",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fb0bc2",
   "metadata": {},
   "source": [
    "# 4. Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fcdef3",
   "metadata": {},
   "source": [
    "Load the CIFAR-10 dataset. Normalize the data and create dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcd23ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765572a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Files already downloaded and verified\n",
    "Files already downloaded and verified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4327cc",
   "metadata": {},
   "source": [
    "# 5. Define the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0729b0b4",
   "metadata": {},
   "source": [
    "Define a simple CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737e03fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e2162a",
   "metadata": {},
   "source": [
    "# 6. Set Up the Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefdbb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03af4044",
   "metadata": {},
   "source": [
    "# 7. Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cdb71c",
   "metadata": {},
   "source": [
    "Train the model and log the loss and accuracy to WandB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5eb64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(run, model, criterion, optimizer, trainloader):\n",
    "    for epoch in range(10):  \n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_predictions += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "            if i % 200 == 199:    # Every 200 mini-batches\n",
    "                print('[Epoch %d, Mini-batch %5d] Loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "                wandb.log({'Epoch': epoch + 1, 'Loss': running_loss / 2000})\n",
    "                wandb.log({'Epoch': epoch + 1, 'Loss': running_loss / 2000, 'Accuracy': correct_predictions / total_predictions * 100})\n",
    "                running_loss = 0.0\n",
    "                correct_predictions = 0\n",
    "                total_predictions = 0\n",
    "\n",
    "train_model(run, model, criterion, optimizer, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2e1fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Epoch 1, Mini-batch   200] Loss: 0.230\n",
    "[Epoch 1, Mini-batch   400] Loss: 0.230\n",
    "[Epoch 1, Mini-batch   600] Loss: 0.229\n",
    "[Epoch 2, Mini-batch   200] Loss: 0.218\n",
    "[Epoch 2, Mini-batch   400] Loss: 0.209\n",
    "[Epoch 2, Mini-batch   600] Loss: 0.200\n",
    "[Epoch 3, Mini-batch   200] Loss: 0.193\n",
    "[Epoch 3, Mini-batch   400] Loss: 0.189\n",
    "[Epoch 3, Mini-batch   600] Loss: 0.183\n",
    "[Epoch 4, Mini-batch   200] Loss: 0.179\n",
    "[Epoch 4, Mini-batch   400] Loss: 0.175\n",
    "[Epoch 4, Mini-batch   600] Loss: 0.171\n",
    "[Epoch 5, Mini-batch   200] Loss: 0.166\n",
    "[Epoch 5, Mini-batch   400] Loss: 0.163\n",
    "[Epoch 5, Mini-batch   600] Loss: 0.161\n",
    "[Epoch 6, Mini-batch   200] Loss: 0.155\n",
    "[Epoch 6, Mini-batch   400] Loss: 0.153\n",
    "[Epoch 6, Mini-batch   600] Loss: 0.151\n",
    "[Epoch 7, Mini-batch   200] Loss: 0.148\n",
    "[Epoch 7, Mini-batch   400] Loss: 0.145\n",
    "[Epoch 7, Mini-batch   600] Loss: 0.145\n",
    "[Epoch 8, Mini-batch   200] Loss: 0.141\n",
    "[Epoch 8, Mini-batch   400] Loss: 0.139\n",
    "[Epoch 8, Mini-batch   600] Loss: 0.142\n",
    "[Epoch 9, Mini-batch   200] Loss: 0.137\n",
    "[Epoch 9, Mini-batch   400] Loss: 0.136\n",
    "[Epoch 9, Mini-batch   600] Loss: 0.135\n",
    "[Epoch 10, Mini-batch   200] Loss: 0.133\n",
    "[Epoch 10, Mini-batch   400] Loss: 0.132\n",
    "[Epoch 10, Mini-batch   600] Loss: 0.129"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859df6a0",
   "metadata": {},
   "source": [
    "# 8. Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd17a34",
   "metadata": {},
   "source": [
    "Evaluate the model on the test data and log the test accuracy to WandB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6e6173",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test images: %d %%' % (100 * correct / total))\n",
    "\n",
    "wandb.log({'Test Accuracy': 100.0*correct/total})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eda2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy of the network on the test images: 52 %"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e11f1a1",
   "metadata": {},
   "source": [
    "This section of the code calculates the accuracy of the model on the test set and logs this test accuracy to WandB for visualization and tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5bee2d",
   "metadata": {},
   "source": [
    "At this point, you can go to the WandB website, navigate to your project, and see a live visualization of your model’s loss and accuracy throughout the training process. This helps to understand how well the model is learning and provides insights for further improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b986e6",
   "metadata": {},
   "source": [
    "Finally, don’t forget to close your WandB run after you’re done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657219af",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05927a8c",
   "metadata": {},
   "source": [
    "# Run history:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30452478",
   "metadata": {},
   "source": [
    "| Accuracy | ▁▂▂▃▃▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇██████ |\n",
    "| --- | --- |\n",
    "| Epoch | ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████ |\n",
    "| Loss | ████▇▇▇▆▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁ |\n",
    "| Test Accuracy | ▁ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e60b83a",
   "metadata": {},
   "source": [
    "# Run summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744516a9",
   "metadata": {},
   "source": [
    "| Accuracy | 53.6875 |\n",
    "| --- | --- |\n",
    "| Epoch | 10 |\n",
    "| Loss | 0.12919 |\n",
    "| Test Accuracy | 52.62 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cda02c",
   "metadata": {},
   "source": [
    "This ensures all resources are properly freed and all logs are uploaded to the WandB server. This step is crucial to make sure all your model training progress and metrics are properly saved and can be reviewed later in the WandB dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039deedc",
   "metadata": {},
   "source": [
    "# Basic Concepts of WanDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbe66c9",
   "metadata": {},
   "source": [
    "Here’s what the WanDB dashboard looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0766de0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af645cf6",
   "metadata": {},
   "source": [
    "![Image](https://storage.googleapis.com/rg-ai-bootcamp/mlops/wandb-project.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb64e374",
   "metadata": {},
   "source": [
    "# A. Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22746a22",
   "metadata": {},
   "source": [
    "In Wandb, a project is a collection of related machine learning experiments (known as “runs”). It provides a shared space where you and your team can compare results, share insights, and discuss potential improvements. Each project has a dedicated page on the Wandb web application, showcasing visualizations, comparisons, and other useful metrics. We set the project name when we do wandb.init() in Step-3 above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a334bd",
   "metadata": {},
   "source": [
    "# B. Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc05323",
   "metadata": {},
   "source": [
    "A run in Wandb is a single execution of your machine learning script. During a run, you can log various metrics, such as loss and accuracy, system performance data, and even media like images or 3D objects. Each run gets its page in the Wandb web application, where you can view and analyze logged data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda9622a",
   "metadata": {},
   "source": [
    "As you can see from the screenshot of the WanDB dashboard, on the left we have 5 runs in our project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3b759d",
   "metadata": {},
   "source": [
    "# C. Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2fd4ab",
   "metadata": {},
   "source": [
    "Artifacts in Wandb are used to handle version control of datasets, models, and other result files from runs. They help to track the inputs and outputs of your runs, providing a clear and useful lineage of your models and data. For example, an input artifact could be your training dataset, while output artifacts could be your trained model or predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd36ff4",
   "metadata": {},
   "source": [
    "On the left you can click on the Artifacts navigation icon which will take you to:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73a8877",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8db13bba",
   "metadata": {},
   "source": [
    "![Image](https://storage.googleapis.com/rg-ai-bootcamp/mlops/wandb-artifacts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cab1f1b",
   "metadata": {},
   "source": [
    "In this case, the artifact is a Jupyter notebook file (job-git_github.com_ruang-guru_ai-bootcamp.git_09_mlops_01_wandb_00_wandb_intro_setup.ipynb) from the project cifar10-classification owned by the user ricky-kurniawan. The version of the artifact is specified after the colon - in this case, v1 indicates it’s the first version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d68b333",
   "metadata": {},
   "source": [
    "The artifact.download() command is used to download the artifact to the local machine for use in the current run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54908369",
   "metadata": {},
   "source": [
    "In the “Used By” section, the listed items are the runs that have used this artifact. For example, the run proud-salad-5 used this artifact. Information such as the run’s performance metrics, the project, the user, the artifact used, and the time of artifact creation is displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711cb619",
   "metadata": {},
   "source": [
    "In this case, run-yqpg9z5p-history:v0 is an output artifact of the run. This run history artifact contains information about the run, such as the logged metrics. This artifact is created automatically by W&B when you log metrics or other information during a run. This allows you to revisit the specifics of a run, analyze the performance, and potentially identify areas for improvement or further exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e82c61",
   "metadata": {},
   "source": [
    "# D. Sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fef900a",
   "metadata": {},
   "source": [
    "Sweep is a feature in Wandb for hyperparameter optimization. A sweep involves a set of runs, each with different hyperparameters, allowing you to explore a range of possibilities and identify the best parameters for your model. Wandb automates this process, generating a set of permutations of hyperparameters (based on a configuration file you create), running them, and logging the results. This makes it easier to optimize your model’s performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f747ad9c",
   "metadata": {},
   "source": [
    "Let’s try doing a Sweep using our CIFAR-10 Project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525970c2",
   "metadata": {},
   "source": [
    "# 1. Setting Up the Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2605bfd9",
   "metadata": {},
   "source": [
    "First, we need to create a configuration for our sweep. This configuration will specify the range and distribution of hyperparameters for the sweep. Here’s a basic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dff9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random', #grid, random\n",
    "    'metric': {\n",
    "      'name': 'accuracy',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'epochs': {\n",
    "            'values': [5, 10, 15]\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [16, 32, 64]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'min': 1e-5,\n",
    "            'max': 0.1\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e8a8a7",
   "metadata": {},
   "source": [
    "In this configuration, we’re specifying that we want to use a random search method (other options are grid for grid search and bayes for Bayesian optimization), and that our goal is to maximize accuracy. We’re also specifying the range of values for the hyperparameters that we want to optimize: epochs, batch size, and learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff9a2f4",
   "metadata": {},
   "source": [
    "# 2. Initialize the Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfafc27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"cifar10-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e58b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Create sweep with ID: kqyufn28\n",
    "Sweep URL: https://wandb.ai/ricky-kurniawan/cifar10-classification/sweeps/kqyufn28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422c3e64",
   "metadata": {},
   "source": [
    "This command initializes the sweep and returns a sweep ID. This ID uniquely identifies the sweep in WandB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e919ae5b",
   "metadata": {},
   "source": [
    "# 3. Define the Train Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f408f1f5",
   "metadata": {},
   "source": [
    "Next, we need to modify the training function to accept configurations and log them to WandB. Add the following lines at the beginning of the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f7b83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a global counter\n",
    "global_counter = 0\n",
    "max_runs = 5\n",
    "\n",
    "def train():\n",
    "    global global_counter\n",
    "    if global_counter >= max_runs:\n",
    "        return\n",
    "    global_counter += 1\n",
    "    with wandb.init(config=sweep_config):\n",
    "        config = wandb.config\n",
    "        model = Net() \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config.learning_rate, momentum=0.9)\n",
    "        train_model(run, model, criterion, optimizer, trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b33e4bf",
   "metadata": {},
   "source": [
    "In this function, wandb.init() is called with the sweep configuration to start a new run. wandb.config is then used to access the hyperparameters for the current run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0606452c",
   "metadata": {},
   "source": [
    "For the purpose of teaching, we are limiting the run to a maksimum of 5. Naturally you should let the sweep run and try out all possible combinations which may take a long time. You can safely remove all lines containing “global_counter” for real case study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a402c0be",
   "metadata": {},
   "source": [
    "# 4. Run the Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8ff428",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a50d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb: Agent Starting Run: jbxkaalg with config:\n",
    "wandb:  batch_size: 64\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.052303023093622406\n",
    "wandb: Currently logged in as: ricky-kurniawan. Use `wandb login --relogin` to force relogin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5465fae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Epoch 1, Mini-batch   200] Loss: 0.205\n",
    "[Epoch 1, Mini-batch   400] Loss: 0.171\n",
    "[Epoch 1, Mini-batch   600] Loss: 0.161\n",
    "[Epoch 2, Mini-batch   200] Loss: 0.152\n",
    "[Epoch 2, Mini-batch   400] Loss: 0.148\n",
    "[Epoch 2, Mini-batch   600] Loss: 0.145\n",
    "[Epoch 3, Mini-batch   200] Loss: 0.141\n",
    "[Epoch 3, Mini-batch   400] Loss: 0.141\n",
    "[Epoch 3, Mini-batch   600] Loss: 0.140\n",
    "[Epoch 4, Mini-batch   200] Loss: 0.137\n",
    "[Epoch 4, Mini-batch   400] Loss: 0.136\n",
    "[Epoch 4, Mini-batch   600] Loss: 0.137\n",
    "[Epoch 5, Mini-batch   200] Loss: 0.131\n",
    "[Epoch 5, Mini-batch   400] Loss: 0.134\n",
    "[Epoch 5, Mini-batch   600] Loss: 0.132\n",
    "[Epoch 6, Mini-batch   200] Loss: 0.130\n",
    "[Epoch 6, Mini-batch   400] Loss: 0.132\n",
    "[Epoch 6, Mini-batch   600] Loss: 0.136\n",
    "[Epoch 7, Mini-batch   200] Loss: 0.127\n",
    "[Epoch 7, Mini-batch   400] Loss: 0.134\n",
    "[Epoch 7, Mini-batch   600] Loss: 0.135\n",
    "[Epoch 8, Mini-batch   200] Loss: 0.129\n",
    "[Epoch 8, Mini-batch   400] Loss: 0.132\n",
    "[Epoch 8, Mini-batch   600] Loss: 0.129\n",
    "[Epoch 9, Mini-batch   200] Loss: 0.125\n",
    "[Epoch 9, Mini-batch   400] Loss: 0.130\n",
    "[Epoch 9, Mini-batch   600] Loss: 0.132\n",
    "[Epoch 10, Mini-batch   200] Loss: 0.127\n",
    "[Epoch 10, Mini-batch   400] Loss: 0.128\n",
    "[Epoch 10, Mini-batch   600] Loss: 0.135"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eadd7d",
   "metadata": {},
   "source": [
    "# Run history:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad31b0ee",
   "metadata": {},
   "source": [
    "| Accuracy | ▁▄▅▆▆▆▆▇▇▇▇▇▇▇▇█▇▇█▇▇█▇██████▇ |\n",
    "| --- | --- |\n",
    "| Epoch | ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████ |\n",
    "| Loss | ██▅▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▂▂▁▁▂▂▁▁▂▂▁▁▂▁▁▁▁▂▁▁▁▂ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd8d41a",
   "metadata": {},
   "source": [
    "# Run summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3af58e",
   "metadata": {},
   "source": [
    "| Accuracy | 54.84375 |\n",
    "| --- | --- |\n",
    "| Epoch | 10 |\n",
    "| Loss | 0.13498 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf649d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb: Agent Starting Run: hil12twe with config:\n",
    "wandb:  batch_size: 64\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.0778152359691495"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e12bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Epoch 1, Mini-batch   200] Loss: 0.206\n",
    "[Epoch 1, Mini-batch   400] Loss: 0.184\n",
    "[Epoch 1, Mini-batch   600] Loss: 0.173\n",
    "[Epoch 2, Mini-batch   200] Loss: 0.162\n",
    "[Epoch 2, Mini-batch   400] Loss: 0.164\n",
    "[Epoch 2, Mini-batch   600] Loss: 0.164\n",
    "[Epoch 3, Mini-batch   200] Loss: 0.168\n",
    "[Epoch 3, Mini-batch   400] Loss: 0.161\n",
    "[Epoch 3, Mini-batch   600] Loss: 0.163\n",
    "[Epoch 4, Mini-batch   200] Loss: 0.160\n",
    "[Epoch 4, Mini-batch   400] Loss: 0.161\n",
    "[Epoch 4, Mini-batch   600] Loss: 0.161\n",
    "[Epoch 5, Mini-batch   200] Loss: 0.159\n",
    "[Epoch 5, Mini-batch   400] Loss: 0.162\n",
    "[Epoch 5, Mini-batch   600] Loss: 0.166\n",
    "[Epoch 6, Mini-batch   200] Loss: 0.164\n",
    "[Epoch 6, Mini-batch   400] Loss: 0.166\n",
    "[Epoch 6, Mini-batch   600] Loss: 0.169\n",
    "[Epoch 7, Mini-batch   200] Loss: 0.164\n",
    "[Epoch 7, Mini-batch   400] Loss: 0.166\n",
    "[Epoch 7, Mini-batch   600] Loss: 0.167\n",
    "[Epoch 8, Mini-batch   200] Loss: 0.171\n",
    "[Epoch 8, Mini-batch   400] Loss: 0.169\n",
    "[Epoch 8, Mini-batch   600] Loss: 0.169\n",
    "[Epoch 9, Mini-batch   200] Loss: 0.166\n",
    "[Epoch 9, Mini-batch   400] Loss: 0.173\n",
    "[Epoch 9, Mini-batch   600] Loss: 0.174\n",
    "[Epoch 10, Mini-batch   200] Loss: 0.169\n",
    "[Epoch 10, Mini-batch   400] Loss: 0.172\n",
    "[Epoch 10, Mini-batch   600] Loss: 0.173"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99375986",
   "metadata": {},
   "source": [
    "# Run history:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8059ebf8",
   "metadata": {},
   "source": [
    "| Accuracy | ▁▄▆▇▇▇▇▇▇███████▇▇██▇▇▇▇█▇▇▇▇▇ |\n",
    "| --- | --- |\n",
    "| Epoch | ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████ |\n",
    "| Loss | ██▅▃▁▁▂▂▂▂▁▂▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▃▃▂▂▂▂▃▃▂▂▃▃ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2fea78",
   "metadata": {},
   "source": [
    "# Run summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bd316b",
   "metadata": {},
   "source": [
    "| Accuracy | 40.67188 |\n",
    "| --- | --- |\n",
    "| Epoch | 10 |\n",
    "| Loss | 0.17274 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf0f212",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: wtl4q1ha with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.010806903248724136"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2730d501",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Epoch 1, Mini-batch   200] Loss: 0.224\n",
    "[Epoch 1, Mini-batch   400] Loss: 0.190\n",
    "[Epoch 1, Mini-batch   600] Loss: 0.167\n",
    "[Epoch 2, Mini-batch   200] Loss: 0.146\n",
    "[Epoch 2, Mini-batch   400] Loss: 0.141\n",
    "[Epoch 2, Mini-batch   600] Loss: 0.136\n",
    "[Epoch 3, Mini-batch   200] Loss: 0.126\n",
    "[Epoch 3, Mini-batch   400] Loss: 0.124\n",
    "[Epoch 3, Mini-batch   600] Loss: 0.120\n",
    "[Epoch 4, Mini-batch   200] Loss: 0.114\n",
    "[Epoch 4, Mini-batch   400] Loss: 0.111\n",
    "[Epoch 4, Mini-batch   600] Loss: 0.113\n",
    "[Epoch 5, Mini-batch   200] Loss: 0.105\n",
    "[Epoch 5, Mini-batch   400] Loss: 0.104\n",
    "[Epoch 5, Mini-batch   600] Loss: 0.104\n",
    "[Epoch 6, Mini-batch   200] Loss: 0.099\n",
    "[Epoch 6, Mini-batch   400] Loss: 0.099\n",
    "[Epoch 6, Mini-batch   600] Loss: 0.099\n",
    "[Epoch 7, Mini-batch   200] Loss: 0.091\n",
    "[Epoch 7, Mini-batch   400] Loss: 0.092\n",
    "[Epoch 7, Mini-batch   600] Loss: 0.095\n",
    "[Epoch 8, Mini-batch   200] Loss: 0.087\n",
    "[Epoch 8, Mini-batch   400] Loss: 0.088\n",
    "[Epoch 8, Mini-batch   600] Loss: 0.091\n",
    "[Epoch 9, Mini-batch   200] Loss: 0.081\n",
    "[Epoch 9, Mini-batch   400] Loss: 0.085\n",
    "[Epoch 9, Mini-batch   600] Loss: 0.086\n",
    "[Epoch 10, Mini-batch   200] Loss: 0.079\n",
    "[Epoch 10, Mini-batch   400] Loss: 0.082\n",
    "[Epoch 10, Mini-batch   600] Loss: 0.083"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05da122b",
   "metadata": {},
   "source": [
    "# Run history:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2598ee",
   "metadata": {},
   "source": [
    "| Accuracy | ▁▃▄▅▅▅▆▆▆▆▇▆▇▇▇▇▇▇▇█▇██▇██████ |\n",
    "| --- | --- |\n",
    "| Epoch | ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████ |\n",
    "| Loss | ██▆▅▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d756ed",
   "metadata": {},
   "source": [
    "# Run summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9736dcc",
   "metadata": {},
   "source": [
    "| Accuracy | 70.96094 |\n",
    "| --- | --- |\n",
    "| Epoch | 10 |\n",
    "| Loss | 0.083 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1772de",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb: Agent Starting Run: ettfrm5h with config:\n",
    "wandb:  batch_size: 64\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.0795927437531589"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d6a782",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Epoch 1, Mini-batch   200] Loss: 0.201\n",
    "[Epoch 1, Mini-batch   400] Loss: 0.178\n",
    "[Epoch 1, Mini-batch   600] Loss: 0.174\n",
    "[Epoch 2, Mini-batch   200] Loss: 0.166\n",
    "[Epoch 2, Mini-batch   400] Loss: 0.168\n",
    "[Epoch 2, Mini-batch   600] Loss: 0.166\n",
    "[Epoch 3, Mini-batch   200] Loss: 0.166\n",
    "[Epoch 3, Mini-batch   400] Loss: 0.162\n",
    "[Epoch 3, Mini-batch   600] Loss: 0.166\n",
    "[Epoch 4, Mini-batch   200] Loss: 0.161\n",
    "[Epoch 4, Mini-batch   400] Loss: 0.162\n",
    "[Epoch 4, Mini-batch   600] Loss: 0.165\n",
    "[Epoch 5, Mini-batch   200] Loss: 0.163\n",
    "[Epoch 5, Mini-batch   400] Loss: 0.161\n",
    "[Epoch 5, Mini-batch   600] Loss: 0.163\n",
    "[Epoch 6, Mini-batch   200] Loss: 0.164\n",
    "[Epoch 6, Mini-batch   400] Loss: 0.163\n",
    "[Epoch 6, Mini-batch   600] Loss: 0.162\n",
    "[Epoch 7, Mini-batch   200] Loss: 0.163\n",
    "[Epoch 7, Mini-batch   400] Loss: 0.168\n",
    "[Epoch 7, Mini-batch   600] Loss: 0.162\n",
    "[Epoch 8, Mini-batch   200] Loss: 0.164\n",
    "[Epoch 8, Mini-batch   400] Loss: 0.164\n",
    "[Epoch 8, Mini-batch   600] Loss: 0.165\n",
    "[Epoch 9, Mini-batch   200] Loss: 0.168\n",
    "[Epoch 9, Mini-batch   400] Loss: 0.168\n",
    "[Epoch 9, Mini-batch   600] Loss: 0.172\n",
    "[Epoch 10, Mini-batch   200] Loss: 0.172\n",
    "[Epoch 10, Mini-batch   400] Loss: 0.170\n",
    "[Epoch 10, Mini-batch   600] Loss: 0.168"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dbadbe",
   "metadata": {},
   "source": [
    "# Run history:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb9d7dc",
   "metadata": {},
   "source": [
    "| Accuracy | ▁▅▆▆▆▇▇▇▇█▇▇███▇███▇████▇▇▇▇▇█ |\n",
    "| --- | --- |\n",
    "| Epoch | ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████ |\n",
    "| Loss | ██▄▃▂▂▂▂▂▂▁▂▁▁▁▂▁▁▁▁▂▂▁▁▁▁▂▁▂▂▂▂▂▂▂▃▃▃▃▂ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ba2a8c",
   "metadata": {},
   "source": [
    "# Run summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a9b943",
   "metadata": {},
   "source": [
    "| Accuracy | 42.27344 |\n",
    "| --- | --- |\n",
    "| Epoch | 10 |\n",
    "| Loss | 0.168 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f14b87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb: Agent Starting Run: sli7lpw0 with config:\n",
    "wandb:  batch_size: 64\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.02990947205127064"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94671483",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Epoch 1, Mini-batch   200] Loss: 0.207\n",
    "[Epoch 1, Mini-batch   400] Loss: 0.169\n",
    "[Epoch 1, Mini-batch   600] Loss: 0.157\n",
    "[Epoch 2, Mini-batch   200] Loss: 0.139\n",
    "[Epoch 2, Mini-batch   400] Loss: 0.137\n",
    "[Epoch 2, Mini-batch   600] Loss: 0.134\n",
    "[Epoch 3, Mini-batch   200] Loss: 0.125\n",
    "[Epoch 3, Mini-batch   400] Loss: 0.123\n",
    "[Epoch 3, Mini-batch   600] Loss: 0.125\n",
    "[Epoch 4, Mini-batch   200] Loss: 0.116\n",
    "[Epoch 4, Mini-batch   400] Loss: 0.118\n",
    "[Epoch 4, Mini-batch   600] Loss: 0.117\n",
    "[Epoch 5, Mini-batch   200] Loss: 0.110\n",
    "[Epoch 5, Mini-batch   400] Loss: 0.113\n",
    "[Epoch 5, Mini-batch   600] Loss: 0.113\n",
    "[Epoch 6, Mini-batch   200] Loss: 0.105\n",
    "[Epoch 6, Mini-batch   400] Loss: 0.109\n",
    "[Epoch 6, Mini-batch   600] Loss: 0.110\n",
    "[Epoch 7, Mini-batch   200] Loss: 0.101\n",
    "[Epoch 7, Mini-batch   400] Loss: 0.104\n",
    "[Epoch 7, Mini-batch   600] Loss: 0.109\n",
    "[Epoch 8, Mini-batch   200] Loss: 0.099\n",
    "[Epoch 8, Mini-batch   400] Loss: 0.105\n",
    "[Epoch 8, Mini-batch   600] Loss: 0.101\n",
    "[Epoch 9, Mini-batch   200] Loss: 0.096\n",
    "[Epoch 9, Mini-batch   400] Loss: 0.102\n",
    "[Epoch 9, Mini-batch   600] Loss: 0.103\n",
    "[Epoch 10, Mini-batch   200] Loss: 0.093\n",
    "[Epoch 10, Mini-batch   400] Loss: 0.101\n",
    "[Epoch 10, Mini-batch   600] Loss: 0.104"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e5b0bb",
   "metadata": {},
   "source": [
    "# Run history:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b8fd91",
   "metadata": {},
   "source": [
    "| Accuracy | ▁▄▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇██▇█▇██████▇ |\n",
    "| --- | --- |\n",
    "| Epoch | ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████ |\n",
    "| Loss | ██▆▅▄▄▄▄▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▂▂▁▁▂▂ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dadbfd",
   "metadata": {},
   "source": [
    "# Run summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e87a34",
   "metadata": {},
   "source": [
    "| Accuracy | 64.03125 |\n",
    "| --- | --- |\n",
    "| Epoch | 10 |\n",
    "| Loss | 0.10403 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36208222",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: rtnk6ct2 with config:\n",
    "wandb:  batch_size: 64\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.04013296976488706\n",
    "wandb: Agent Starting Run: bqo62g9l with config:\n",
    "wandb:  batch_size: 64\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.019177756444346203\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: 4xg5m9q5 with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.08786538663827181\n",
    "wandb: Agent Starting Run: hccvb2aj with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.09598692806072671\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: 9l2kwop6 with config:\n",
    "wandb:  batch_size: 64\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.02399552057250517\n",
    "wandb: Agent Starting Run: kxjxgcoz with config:\n",
    "wandb:  batch_size: 64\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.001797124637780443\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: 0hg7ms36 with config:\n",
    "wandb:  batch_size: 64\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.039904043596502\n",
    "wandb: Agent Starting Run: edi3pf6o with config:\n",
    "wandb:  batch_size: 64\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.05230665673262373\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: b0rhoirl with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.07634240948572275\n",
    "wandb: Agent Starting Run: b3kwtw9q with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.01182721332560279\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: 8384njvv with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.03237160620647384\n",
    "wandb: Agent Starting Run: 1s1lmbyu with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.006803383495698264\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: hmillp2c with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.08159207853619327\n",
    "wandb: Agent Starting Run: 3qhajxuy with config:\n",
    "wandb:  batch_size: 64\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.034446125391791926\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: 47338itd with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.027886490017400307\n",
    "wandb: Agent Starting Run: n1vn81r5 with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.06457802029413433\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: cgale92f with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.09884957347400788\n",
    "wandb: Agent Starting Run: b4dzya1c with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.023560550660514372\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: xc1w68kc with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.0677273337735739\n",
    "wandb: Agent Starting Run: 3qc17zvp with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.020872029295613256\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: bycum4ax with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.0009502174683948892\n",
    "wandb: Agent Starting Run: ea13uxm4 with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.03875932439917796\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: cv9ymo3e with config:\n",
    "wandb:  batch_size: 64\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.03643050258260523\n",
    "wandb: Agent Starting Run: rdudww8g with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.07521550538459543\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: j0t2l6xn with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.0926547142606578\n",
    "wandb: Agent Starting Run: hjs4wpup with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.03103738494870234\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: a1zb8bt0 with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.08247355964474885\n",
    "wandb: Agent Starting Run: 6w9frll1 with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.04725600655064744\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: zncziwff with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.043450210349017096\n",
    "wandb: Agent Starting Run: we8x72vm with config:\n",
    "wandb:  batch_size: 64\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.06386487328353212\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: waxypahb with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.042724514638329464\n",
    "wandb: Agent Starting Run: joi5zjsj with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.019430254130361313\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: xx88hfkr with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.07547491698020012\n",
    "wandb: Agent Starting Run: kjhokwe7 with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.00019885257735111447\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: snjtvnuw with config:\n",
    "wandb:  batch_size: 64\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.01991671460892379\n",
    "wandb: Agent Starting Run: a8335nge with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.01795560439131162\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: d8qd3tqr with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.04822296635022568\n",
    "wandb: Agent Starting Run: oynds7y0 with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.044998018963751635\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: 9ll122fe with config:\n",
    "wandb:  batch_size: 64\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.07340241778955124\n",
    "wandb: Agent Starting Run: c0vlax52 with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.002283546818615913\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: bt6cj0un with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.022668240334927293\n",
    "wandb: Agent Starting Run: mrdz8w9o with config:\n",
    "wandb:  batch_size: 64\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.05708704882360202\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: umbfbt8n with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.08304445886730834\n",
    "wandb: Agent Starting Run: 3toy7vas with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.08614471640332713\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: c6esvwxi with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.00975734470018658\n",
    "wandb: Agent Starting Run: h3jc09p7 with config:\n",
    "wandb:  batch_size: 64\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.0006792195004773297\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: zm9y7f8f with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.035608281690541546\n",
    "wandb: Agent Starting Run: flc4dlyi with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.0747376446577179\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: d6zjnt74 with config:\n",
    "wandb:  batch_size: 64\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.0147811451253074\n",
    "wandb: Agent Starting Run: curacom8 with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.03078741176536328\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: dz7jp1uz with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.03647067772864392\n",
    "wandb: Agent Starting Run: 88b26sl8 with config:\n",
    "wandb:  batch_size: 64\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.03164228010599915\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: 1vx8j98a with config:\n",
    "wandb:  batch_size: 64\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.060367573514628875\n",
    "wandb: Agent Starting Run: 0uh1isax with config:\n",
    "wandb:  batch_size: 64\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.09465268340140094\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: houw5pqz with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.04899465514091969\n",
    "wandb: Agent Starting Run: w2o09wx9 with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.07761409080397712\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: 6kmdrt5z with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.03191007075831756\n",
    "wandb: Agent Starting Run: eyedp7sg with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.0389982675078401\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: knj1pboz with config:\n",
    "wandb:  batch_size: 64\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.05755347056457952\n",
    "wandb: Agent Starting Run: uxyumcsf with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.08216783864067907\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: f31dej07 with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.08075061578012568\n",
    "wandb: Agent Starting Run: fs20edf9 with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.07661993280501622\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: 78j6qti3 with config:\n",
    "wandb:  batch_size: 64\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.07472375363565593\n",
    "wandb: Agent Starting Run: 0rbbjcnf with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.062440351458098856\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: 0p5dph5h with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.095907049984872\n",
    "wandb: Agent Starting Run: ftrbpjom with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.03250778308923327\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: xtyertyc with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.08278359995673532\n",
    "wandb: Agent Starting Run: gujy3vad with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.07482464086791861\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: 7xsa7jdq with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.07774704322127651\n",
    "wandb: Agent Starting Run: zoodx6p9 with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.008214304338534106\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: wlhlk1y9 with config:\n",
    "wandb:  batch_size: 64\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.06960814841631531\n",
    "wandb: Agent Starting Run: 5hv0lxmk with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.060997982236647166\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: 5ejtdrgs with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.029460761958973136\n",
    "wandb: Agent Starting Run: udyjpx2s with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.03138232430215001\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: qmhjuyl3 with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.026927982306497504\n",
    "wandb: Agent Starting Run: kqe4dcbu with config:\n",
    "wandb:  batch_size: 64\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.0905052326377824\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: 48petedk with config:\n",
    "wandb:  batch_size: 64\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.08917413284685244\n",
    "wandb: Agent Starting Run: 9cve8j4b with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.09537012996979832\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: cbm95fdg with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.07043152434511156\n",
    "wandb: Agent Starting Run: o7m7zvkl with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.04866494456570549\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: gjpy9r0r with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.07551029408789373\n",
    "wandb: Agent Starting Run: 91fp8jt4 with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.09478472201588872\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: 2pu84sxr with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.017711354426403886\n",
    "wandb: Agent Starting Run: e1sgeidh with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.09046440046876088\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: k25dk7xy with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.091976710481526\n",
    "wandb: Agent Starting Run: c1klbar5 with config:\n",
    "wandb:  batch_size: 64\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.03743326691366536\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: qhqtolza with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.08201836927894077\n",
    "wandb: Agent Starting Run: 10tr16u4 with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.08041062549078154\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: inlf4ymj with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.087283456718445\n",
    "wandb: Agent Starting Run: 819rpaa4 with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.038763119130549355\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: odxiv32v with config:\n",
    "wandb:  batch_size: 64\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.01556679894974132\n",
    "wandb: Agent Starting Run: bh9pdy8l with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.04817863423238899\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: y38z5nr6 with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.016870862546940594\n",
    "wandb: Agent Starting Run: y9kzkujv with config:\n",
    "wandb:  batch_size: 64\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.06284544026485823\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: kmwm24tu with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.07492284565203791\n",
    "wandb: Agent Starting Run: i6dl9cis with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.0427534134153622\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: txbvsz69 with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.05326299600938453\n",
    "wandb: Agent Starting Run: ccw4uhmi with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.0480183490301355\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: ivkyuvs8 with config:\n",
    "wandb:  batch_size: 64\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.06865922913712455\n",
    "wandb: Agent Starting Run: ci2r2guu with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.07737784852558242\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: 2pnrxxz0 with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.05050030078259987\n",
    "wandb: Agent Starting Run: rq54hcia with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.0910912726664158\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: 00clda8n with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.07824430646201114\n",
    "wandb: Agent Starting Run: uri7slp8 with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 10\n",
    "wandb:  learning_rate: 0.06346494197531323\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: 3u5pu7dy with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.09495262177733783\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: ajuxu8d8 with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.02070714388688412\n",
    "wandb: Agent Starting Run: xlz9f0ad with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 15\n",
    "wandb:  learning_rate: 0.029069817807907232\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: yhcoekh6 with config:\n",
    "wandb:  batch_size: 16\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.08499498160510198\n",
    "wandb: Agent Starting Run: simg5ivw with config:\n",
    "wandb:  batch_size: 64\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.034975887586655756\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Job received.\n",
    "wandb: Agent Starting Run: 90gbjyps with config:\n",
    "wandb:  batch_size: 32\n",
    "wandb:  epochs: 5\n",
    "wandb:  learning_rate: 0.08929397988150486\n",
    "wandb: Sweep Agent: Waiting for job.\n",
    "wandb: Sweep Agent: Exiting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8327edd5",
   "metadata": {},
   "source": [
    "This command will start the sweep we just defined. WandB will call the train function with the different combinations of hyperparameters defined in the sweep configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1106b045",
   "metadata": {},
   "source": [
    "Here’s what the Sweep Dashboard looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2a9687",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b4bbf39",
   "metadata": {},
   "source": [
    "![Image](https://storage.googleapis.com/rg-ai-bootcamp/mlops/wandb-sweep.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47b2544",
   "metadata": {},
   "source": [
    "We can clearly see the which combination is more effective by looking at the charts, we can then dive in to fine tune our hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f926ce0c",
   "metadata": {},
   "source": [
    "WandB’s sweeps are a powerful tool for optimizing your model’s hyperparameters. By integrating WandB with your model training code, you can automate the process of training many models with different hyperparameters, and then easily compare their performance on the WandB’s dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2022f951",
   "metadata": {},
   "source": [
    "You can view the full project at this link: CIFAR10 WanDB Dashboard"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
