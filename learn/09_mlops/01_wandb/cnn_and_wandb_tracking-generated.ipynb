{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62898d16",
   "metadata": {},
   "source": [
    "source: [link](https://ai-bootcamp.ruangguru.com/learn/09_mlops/01_wandb/cnn_and_wandb_tracking.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2918acfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import wandb\n",
    "import random\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# Define CNN architecture (example - customize as needed)\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)  # Input channels, output channels, kernel size\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # Kernel size, stride (optional)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # Input features, output features\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10) # Output for 10 CIFAR-10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)  # Flatten for fully-connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def validate_model(model, valid_dl, loss_func, log_images=False, batch_idx=0):\n",
    "    \"Compute performance of the model on the validation dataset and log a wandb.Table\"\n",
    "    model.eval()\n",
    "    val_loss = 0.\n",
    "    with torch.inference_mode():\n",
    "        correct = 0\n",
    "        for i, (images, labels) in enumerate(valid_dl):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass â¡\n",
    "            outputs = model(images)\n",
    "            val_loss += loss_func(outputs, labels)*labels.size(0)\n",
    "\n",
    "            # Compute accuracy and accumulate\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Log one batch of images to the dashboard, always same batch_idx.\n",
    "            if i==batch_idx and log_images:\n",
    "                log_image_table(images, predicted, labels, outputs.softmax(dim=1))\n",
    "    return val_loss / len(valid_dl.dataset), correct / len(valid_dl.dataset)\n",
    "\n",
    "def log_image_table(images, predicted, labels, probs):\n",
    "    \"Log a wandb.Table with (img, pred, target, scores)\"\n",
    "    # ğŸ Create a wandb Table to log images, labels and predictions to\n",
    "    table = wandb.Table(columns=[\"image\", \"pred\", \"target\"]+[f\"score_{i}\" for i in range(10)])\n",
    "    for img, pred, targ, prob in zip(images.to(\"cpu\"), predicted.to(\"cpu\"), labels.to(\"cpu\"), probs.to(\"cpu\")):\n",
    "        table.add_data(wandb.Image(img[0].numpy()*255), pred, targ, *prob.numpy())\n",
    "    wandb.log({\"predictions_table\":table}, commit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222c645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Initialize Wandb project\n",
    "wandb.init(\n",
    "        project=\"cifar10-cnn-wandb\",\n",
    "        config={\n",
    "            \"epochs\": 5,\n",
    "            \"batch_size\": 128,\n",
    "            \"lr\": 2e-3,\n",
    "            \"dropout\": random.uniform(0.01, 0.80),\n",
    "            \"threshold_accuracy\": 0.8\n",
    "            },\n",
    "        save_code=True)\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "# Load and transform CIFAR-10 data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=config.batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "n_steps_per_epoch = math.ceil(len(trainloader.dataset) / config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b4e67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Files already downloaded and verified\n",
    "Files already downloaded and verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3173c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model, optimizer, and loss function\n",
    "model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e32b366",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "for epoch in range(config.epochs):\n",
    "    running_loss = 0.0\n",
    "    example_ct = 0\n",
    "    step_ct = 0\n",
    "    for step, (images, labels) in enumerate(trainloader, 0):\n",
    "        inputs, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        example_ct += len(inputs)\n",
    "        metrics = {\"train/train_loss\": loss,\n",
    "                    \"train/epoch\": (step + 1 + (n_steps_per_epoch * epoch)) / n_steps_per_epoch,\n",
    "                    \"train/example_ct\": example_ct}\n",
    "        if step + 1 < n_steps_per_epoch:\n",
    "            # ğŸ Log train metrics to wandb\n",
    "            wandb.log(metrics)\n",
    "        step_ct += 1\n",
    "\n",
    "    val_loss, accuracy = validate_model(model, testloader, criterion, log_images=(epoch==(config.epochs-1)))\n",
    "    val_metrics = {\n",
    "        \"val/val_loss\": val_loss,\n",
    "        \"val/val_accuracy\": accuracy\n",
    "        }\n",
    "    wandb.log({**metrics, **val_metrics})\n",
    "    print(f\"Train Loss: {loss:.3f}, Valid Loss: {val_loss:3f}, Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    if accuracy > config.threshold_accuracy:\n",
    "        wandb.alert(\n",
    "            title='Low Accuracy',\n",
    "            text=f'Accuracy {accuracy} at step {step_ct} is below the acceptable theshold, {config.threshold_accuracy}',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137c1b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train Loss: 1.429, Valid Loss: 1.424600, Accuracy: 0.48\n",
    "Train Loss: 1.476, Valid Loss: 1.267869, Accuracy: 0.54\n",
    "Train Loss: 1.086, Valid Loss: 1.173016, Accuracy: 0.59\n",
    "Train Loss: 1.096, Valid Loss: 1.134961, Accuracy: 0.60\n",
    "Train Loss: 1.137, Valid Loss: 1.126435, Accuracy: 0.60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3222e608",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52dbe5b",
   "metadata": {},
   "source": [
    "# Run history:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6845c649",
   "metadata": {},
   "source": [
    "| train/epoch | â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ |\n",
    "| --- | --- |\n",
    "| train/example_ct | â–â–‚â–ƒâ–„â–…â–†â–†â–‡â–â–‚â–ƒâ–„â–…â–†â–‡â–ˆâ–â–‚â–ƒâ–„â–…â–†â–‡â–ˆâ–â–‚â–ƒâ–„â–…â–†â–‡â–ˆâ–â–‚â–ƒâ–„â–…â–†â–‡â–ˆ |\n",
    "| train/train_loss | â–ˆâ–†â–…â–…â–…â–„â–„â–„â–ƒâ–„â–„â–„â–„â–„â–„â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–„â–‚â–â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–ƒâ–ƒâ–‚â–‚â–‚ |\n",
    "| val/val_accuracy | â–â–…â–‡â–ˆâ–ˆ |\n",
    "| val/val_loss | â–ˆâ–„â–‚â–â– |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0039665d",
   "metadata": {},
   "source": [
    "# Run summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eddb7a",
   "metadata": {},
   "source": [
    "| train/epoch | 5.0 |\n",
    "| --- | --- |\n",
    "| train/example_ct | 50000 |\n",
    "| train/train_loss | 1.13669 |\n",
    "| val/val_accuracy | 0.6026 |\n",
    "| val/val_loss | 1.12644 |"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
