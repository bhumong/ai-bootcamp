{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de64eae1",
   "metadata": {},
   "source": [
    "# Intro to Prompt in Generative AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1399b250",
   "metadata": {},
   "source": [
    "On this course, we will learn how to use Prompt in Generative AI. We will start with the basics and then move on to more advanced topics. We’ll use ChatGPT to use the prompt on, so if you do not have an account, sign up at https://chat.openai.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aae2e65",
   "metadata": {},
   "source": [
    "# What is Prompt in AI Language Models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c9ac9",
   "metadata": {},
   "source": [
    "A prompt is like a clue given to an AI model. This clue helps the AI know what to talk about. It’s like asking a question or starting a conversation. The AI takes this clue and uses it to come up with a response. The clue can be a question, a sentence, or even just one word. It’s very important because it tells the AI what to say next. It’s the first step that guides the AI’s answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca2b64e",
   "metadata": {},
   "source": [
    "Think of an AI language model as a super-smart parrot. This parrot doesn’t know what to say until you give it a hint, which we call a prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e222485",
   "metadata": {},
   "source": [
    "For example, if you say to the parrot,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fed02e1",
   "metadata": {},
   "source": [
    "Tell me a story about a brave knight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0332169a",
   "metadata": {},
   "source": [
    "The parrot (or AI model) then uses this prompt to create a story about a brave knight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cea810",
   "metadata": {},
   "source": [
    "The prompt is important because it sets the direction for what the AI model will generate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6503de",
   "metadata": {},
   "source": [
    "It’s like giving a topic for an essay to a student. The student uses that topic to write an essay. Similarly, the AI model uses the prompt to generate a response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8936da5",
   "metadata": {},
   "source": [
    "So, in simple terms, a prompt is a piece of information or a question that you give to an AI model to guide what it will talk about or generate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd0a81b",
   "metadata": {},
   "source": [
    "# LLM Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2de0577",
   "metadata": {},
   "source": [
    "When crafting prompts, you’ll either interact with the Large Language Model (LLM) directly or through an API. One of the most important parameter you can adjust to tailor the results of your prompts is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355b0b94",
   "metadata": {},
   "source": [
    "Temperature - Simply put, a lower temperature setting makes it less creative, more following a predefined template (deterministic), while a higher temperature will introduce more randomness, resulting in more diverse or inventive outputs. For instance, you might want to use a lower temperature for tasks like fact-based Q&A to promote more accurate and to the point responses. Conversely, for creative tasks like generating poetry, a higher temperature might be advantageous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a80125",
   "metadata": {},
   "source": [
    "# Components of a Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5299906",
   "metadata": {},
   "source": [
    "A prompt is a set of instructions that guide an AI model to perform a specific task. It consists of several components, each playing a crucial role in shaping the model’s response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b47ed8b",
   "metadata": {},
   "source": [
    "Let’s consider a complex prompt for a detailed breakdown:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c293b51c",
   "metadata": {},
   "source": [
    "Prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f48d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Given the following review of a restaurant, summarize it in one sentence and classify it as positive, negative, or neutral: 'The food was absolutely delicious, and the staff were incredibly friendly. However, the noise level was quite high which made it difficult to have a conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbcba07",
   "metadata": {},
   "source": [
    "Here are the key components of this prompt:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6764b2",
   "metadata": {},
   "source": [
    "Instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918b5d9e",
   "metadata": {},
   "source": [
    "The instruction is the core of the prompt. It directs the AI model on the task it needs to perform. In our example, the instruction is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086f94b1",
   "metadata": {},
   "source": [
    "“Given the following review of a restaurant, summarize it in one sentence and classify it as positive, negative, or neutral”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e45dc7",
   "metadata": {},
   "source": [
    "This part of the prompt tells the AI what task it needs to perform, which is to summarize a restaurant review in one sentence and classify the sentiment of the review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b0370e",
   "metadata": {},
   "source": [
    "Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ec8d3a",
   "metadata": {},
   "source": [
    "Context provides additional information that aids the AI model in understanding the task better and generating more accurate responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9659618",
   "metadata": {},
   "source": [
    "In the given prompt, the context is provided in the instruction itself -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350405e5",
   "metadata": {},
   "source": [
    "“Given the following review of a restaurant, summarize it in one sentence and classify it as positive, negative, or neutral”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fec322",
   "metadata": {},
   "source": [
    "The AI model is informed that the input data is a review of a restaurant. This helps the model understand that it should consider factors like food quality, service, and ambiance in its summary and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483b0f27",
   "metadata": {},
   "source": [
    "Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7d02dc",
   "metadata": {},
   "source": [
    "Input data is the information that the AI model processes to accomplish the task set in the prompt. In our example, the input data is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63983891",
   "metadata": {},
   "source": [
    "“The food was absolutely delicious, and the staff were incredibly friendly. However, the noise level was quite high which made it difficult to have a conversation.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4ed050",
   "metadata": {},
   "source": [
    "This is the restaurant review that needs to be summarized and classified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1b3db7",
   "metadata": {},
   "source": [
    "Output Indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e7ba97",
   "metadata": {},
   "source": [
    "The output indicator signals to the AI model that a response is expected. In this case, the output indicator is implicit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9805726",
   "metadata": {},
   "source": [
    "The instruction to “summarize it in one sentence and classify it as positive, negative, or neutral” implies that the AI model is expected to produce a response, which would be the summarized review and its classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83da16c",
   "metadata": {},
   "source": [
    "# Prompt Engineering Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701f11e2",
   "metadata": {},
   "source": [
    "Creating prompts for AI language models is like giving directions. The better the directions, the better the AI can respond. Here are some tips and examples to help make better prompts:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7988022a",
   "metadata": {},
   "source": [
    "# Giving Context for Clear and Accurate Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b22c42d",
   "metadata": {},
   "source": [
    "Context can be thought of as the setting or background of a conversation. It’s like the stage where the conversation takes place and the theme that it revolves around."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28c60ca",
   "metadata": {},
   "source": [
    "Providing context is like giving a map to the AI model. This map helps the model understand the conversation’s location and topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34b39c6",
   "metadata": {},
   "source": [
    "Context is crucial in any conversation. It acts like a spine, giving the necessary support and direction. When context is added to prompts, it steers the AI model to create responses that make sense and are on point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7132fbf2",
   "metadata": {},
   "source": [
    "Here are some examples of prompts with context:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aef6c6a",
   "metadata": {},
   "source": [
    "# Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e39685",
   "metadata": {},
   "source": [
    "Incorrect Prompt: Tell me about it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b50bc3",
   "metadata": {},
   "source": [
    "This prompt is unclear and lacks context. The AI model can’t identify what “it” refers to and might produce a response that’s off-topic or irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d7437c",
   "metadata": {},
   "source": [
    "Improved Prompt: Tell me about the implications of climate change on global agriculture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc83972e",
   "metadata": {},
   "source": [
    "This prompt gives clear context (climate change and global agriculture), directing the AI model to produce a relevant and coherent response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b38b958",
   "metadata": {},
   "source": [
    "# Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317f07d4",
   "metadata": {},
   "source": [
    "Incorrect Prompt: Analyze the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c181b0",
   "metadata": {},
   "source": [
    "This prompt is unclear and lacks context. The AI model can’t identify what data to analyze or what kind of analysis is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769c9a28",
   "metadata": {},
   "source": [
    "Improved Prompt: Analyze the sales data from the last quarter using a regression model to predict future trends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b51c64",
   "metadata": {},
   "source": [
    "This prompt gives clear context (sales data from the last quarter, regression model, prediction of future trends), directing the AI model to produce a relevant and coherent response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b576da",
   "metadata": {},
   "source": [
    "# Making a Clear Task for Clear Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17654f2",
   "metadata": {},
   "source": [
    "The relationship between the clarity of a task defined in a prompt and the quality of the results obtained is direct. When task definitions are clear and precise, superior results are achieved. This is because they offer the AI model a specific direction to follow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4945ec5",
   "metadata": {},
   "source": [
    "Here are some examples of prompts with clear tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ce31f0",
   "metadata": {},
   "source": [
    "# Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fc3d78",
   "metadata": {},
   "source": [
    "Incorrect Prompt: Improve this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efd0979",
   "metadata": {},
   "source": [
    "This prompt is too ambiguous. The AI model can’t identify what needs enhancement or how to tackle the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fe5762",
   "metadata": {},
   "source": [
    "Improved Prompt: Improve this sentence for clarity: 'The man the professor the student has studies Rome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74902a17",
   "metadata": {},
   "source": [
    "The enhanced prompt clearly outlines the task (improving a sentence for clarity) and provides a specific item (the sentence) for the AI model to work on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd72f24",
   "metadata": {},
   "source": [
    "# Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4051e4f",
   "metadata": {},
   "source": [
    "Incorrect Prompt: Translate this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c630df",
   "metadata": {},
   "source": [
    "Without indicating the source and target languages, the AI model can’t determine how to tackle the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e24e2e",
   "metadata": {},
   "source": [
    "Improved Prompt: Translate this English text into French: 'Artificial Intelligence is transforming industries.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2354448a",
   "metadata": {},
   "source": [
    "The enhanced prompt clearly outlines the task (translation), the source language (English), the target language (French), and provides a specific item (the sentence) for the AI model to work on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82275f70",
   "metadata": {},
   "source": [
    "# Creating Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283ae3ef",
   "metadata": {},
   "source": [
    "Characters represent individuals, creatures, or entities participating in a narrative. They interact with each other and propel the storyline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec083bc",
   "metadata": {},
   "source": [
    "The development of personas can aid in producing responses that are more tailored and captivating. Personas can steer the AI model to react in a specific tone or style, rendering the interaction more akin to human conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803caf05",
   "metadata": {},
   "source": [
    "Here are some examples of prompts with characters:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6154f86a",
   "metadata": {},
   "source": [
    "# Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f05b436",
   "metadata": {},
   "source": [
    "Incorrect Prompt: Write a response to a customer complaint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7daa016",
   "metadata": {},
   "source": [
    "This prompt is overly general and doesn’t specify the tone or style of the response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad75c58",
   "metadata": {},
   "source": [
    "Improved Prompt: Write a response to a customer complaint as 'Emily', a customer service representative who is empathetic, patient, and solution-oriented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17f94b6",
   "metadata": {},
   "source": [
    "The refined prompt creates a persona (Emily) with unique characteristics, guiding the AI model to generate a more personalized and engaging response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c60773",
   "metadata": {},
   "source": [
    "# Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1c676e",
   "metadata": {},
   "source": [
    "Incorrect Prompt: Write a blog post about the benefits of exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e88445",
   "metadata": {},
   "source": [
    "This prompt doesn’t identify the persona of the writer, which could lead to a generic or impersonal blog post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5282c62",
   "metadata": {},
   "source": [
    "Improved Prompt: Write a blog post about the benefits of exercise as 'Dr. Smith', a seasoned fitness trainer with a passion for promoting healthy lifestyles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599306e6",
   "metadata": {},
   "source": [
    "The refined prompt creates a persona (Dr. Smith) with unique characteristics, guiding the AI model to generate a more personalized and engaging blog post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ab5af0",
   "metadata": {},
   "source": [
    "# Adding More Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1de2aa",
   "metadata": {},
   "source": [
    "Adding more context is like giving the AI extra information that it wouldn’t know otherwise. This extra information can be in the form of a setting, a theme, or a character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c49f4f",
   "metadata": {},
   "source": [
    "Here are some examples of prompts with more context:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573618e6",
   "metadata": {},
   "source": [
    "# Example 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b50572",
   "metadata": {},
   "source": [
    "Incorrect Prompt: Write a news report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe62655",
   "metadata": {},
   "source": [
    "This prompt is missing specific context, which could result in a generic or irrelevant news report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915a420b",
   "metadata": {},
   "source": [
    "Improved Prompt: Write a news report on the recent hurricane in Florida, considering its impact on local communities and the ongoing recovery efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2399404",
   "metadata": {},
   "source": [
    "The refined prompt introduces relevant context (recent hurricane in Florida, impact on local communities, ongoing recovery efforts), steering the AI model to generate a more precise and pertinent news report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf6b1ca",
   "metadata": {},
   "source": [
    "# Example 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe2f3bb",
   "metadata": {},
   "source": [
    "Incorrect Prompt: Write a product review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b906680c",
   "metadata": {},
   "source": [
    "This prompt is missing specific context, which could result in a generic or irrelevant product review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d0a534",
   "metadata": {},
   "source": [
    "Improved Prompt: Write a product review for the latest iPhone model, considering its new features, performance, and comparison with the previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33babce9",
   "metadata": {},
   "source": [
    "The refined prompt introduces relevant context (latest iPhone model, new features, performance, comparison with the previous model), steering the AI model to generate a more precise and relevant product review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b66920",
   "metadata": {},
   "source": [
    "# Adding Outside Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b76e67",
   "metadata": {},
   "source": [
    "Adding Outside knowledge into the prompt can assist the AI model in producing responses that are more informed and knowledgeable. This can be especially beneficial when handling subjects that demand expert knowledge or the most current information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1676c5",
   "metadata": {},
   "source": [
    "Here are some examples of prompts with outside knowledge:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a240740e",
   "metadata": {},
   "source": [
    "# Example 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6f0ff8",
   "metadata": {},
   "source": [
    "Incorrect Prompt: Write an article about global warming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc850dc9",
   "metadata": {},
   "source": [
    "This prompt doesn’t indicate the need for expert knowledge or the most recent information, which could result in a generic or outdated article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdcdf20",
   "metadata": {},
   "source": [
    "Improved Prompt: Write an article about global warming, incorporating the latest research findings from the IPCC (Intergovernmental Panel on Climate Change) report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dec1a9c",
   "metadata": {},
   "source": [
    "The refined prompt introduces external knowledge (latest research findings from the IPCC report), steering the AI model to generate a more informed and current article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f4d4f7",
   "metadata": {},
   "source": [
    "# Example 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928dcd0d",
   "metadata": {},
   "source": [
    "Incorrect Prompt: Write a report on the current state of the stock market."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e18d893",
   "metadata": {},
   "source": [
    "This prompt doesn’t indicate the need for expert knowledge or the most recent information, which could result in a generic or outdated report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca7eef6",
   "metadata": {},
   "source": [
    "Improved Prompt: Write a report on the current state of the stock market, incorporating the latest data from the New York Stock Exchange and expert analysis from leading financial analysts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1226f1",
   "metadata": {},
   "source": [
    "The refined prompt introduces external knowledge (latest data from the New York Stock Exchange, expert analysis from leading financial analysts), steering the AI model to generate a more informed and current report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f05fe6f",
   "metadata": {},
   "source": [
    "# Trying Again and Again for Better Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3732241e",
   "metadata": {},
   "source": [
    "The procedure of prompt engineering extends beyond the creation of the initial prompt. It encompasses refining the prompts and assessing the output across multiple iterations. This iterative process aids in understanding how the AI model reacts to various prompts and enables the optimization of prompts for superior results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79bbaf4",
   "metadata": {},
   "source": [
    "# Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bab67f",
   "metadata": {},
   "source": [
    "Initial Prompt: Write a poem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0411fb",
   "metadata": {},
   "source": [
    "The AI model could interpret this prompt in numerous ways, leading to a wide variety of responses. While not necessarily incorrect, it could be optimized for better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b98514",
   "metadata": {},
   "source": [
    "Refined Prompt after Iteration: Write a sonnet about the tranquility of a quiet winter night."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7dd65e",
   "metadata": {},
   "source": [
    "After evaluating the output from the initial prompt, it might become clear that a specific type of poem (a sonnet) about a specific theme (the tranquility of a quiet winter night) is desired. This refined prompt guides the AI model more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f2cce7",
   "metadata": {},
   "source": [
    "# Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f44f5c",
   "metadata": {},
   "source": [
    "Initial Prompt: Summarize the article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb1cb4",
   "metadata": {},
   "source": [
    "While this prompt isn’t incorrect, it’s quite broad and could be optimized for better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b274a8d3",
   "metadata": {},
   "source": [
    "Refined Prompt after Iteration: Summarize the key arguments and conclusions from the article on 'The Impact of Artificial Intelligence on Healthcare'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418fc11b",
   "metadata": {},
   "source": [
    "After evaluating the output from the initial prompt, it might become clear that a summary focusing on key arguments and conclusions is desired. Also, specifying the article provides more context for the AI model. This refined prompt guides the AI model more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a35bef7",
   "metadata": {},
   "source": [
    "Adhering to these guidelines for prompt engineering can enhance the performance of AI models and yield superior responses. The inclusion of context, clear task definition, iterative refinement of prompts, persona creation, context injection, and external knowledge addition will result in more precise and pertinent responses. This not only saves time but also boosts the overall performance of the AI models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a76d14",
   "metadata": {},
   "source": [
    "# Exploring Techniques in Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2399ee78",
   "metadata": {},
   "source": [
    "There are several techniques used in Prompt Engineering, each with its unique approach and application. These techniques include: - Zero-shot Prompting - Few-shot Prompting - Chain-of-Thought Prompting - Generate Knowledge Prompting - Directional Stimulus Prompting - etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba25d937",
   "metadata": {},
   "source": [
    "In this discussion, we will explore a few of these techniques and see how they work in the world of Prompt Engineering. For a more comprehensive understanding of these techniques and to explore others not covered here, you can visit Promptly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6ca82d",
   "metadata": {},
   "source": [
    "# Zero-Shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd93307d",
   "metadata": {},
   "source": [
    "Zero-shot prompting is when we ask a computer program to do a task without giving it any examples beforehand. This is possible with today’s Large Language Models (LLMs) that are trained on lots of data and can follow instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb64a4f4",
   "metadata": {},
   "source": [
    "For example, let’s say we ask the program to classify a sentence as neutral, negative, or positive:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbff013",
   "metadata": {},
   "source": [
    "Prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9304b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Determine the mood of the following statement:\n",
    "Text: The concert was absolutely fantastic, and the band was incredible!\n",
    "Mood:\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9798f36b",
   "metadata": {},
   "source": [
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c93082",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Excited\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8589fbb",
   "metadata": {},
   "source": [
    "Here, we didn’t give the program any examples, but it still gave the correct answer. This is what we call zero-shot capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee02830a",
   "metadata": {},
   "source": [
    "# Few-Shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a9a8a8",
   "metadata": {},
   "source": [
    "Few-shot prompting is like giving a few examples to a computer program before asking it to do a similar task. This helps the program understand what we want it to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480a6e87",
   "metadata": {},
   "source": [
    "For example, let’s say we have a new word, “farduddle,” which means to jump up and down really fast. We can show the program how to use this word with an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febff6cb",
   "metadata": {},
   "source": [
    "Prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffd82df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"To do a 'farduddle' means to jump up and down really fast. An example of a sentence that uses the word farduddle is:\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab93f04",
   "metadata": {},
   "source": [
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1c4893",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"When we won the game, we all started to farduddle in celebration.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328bc113",
   "metadata": {},
   "source": [
    "Here, the program learns how to use the new word correctly with just one example. If the task is harder, we might need to give more examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4613814",
   "metadata": {},
   "source": [
    "But few-shot prompting doesn’t always work, especially for more complex tasks. For example, if we ask the program to figure out if the sum of odd numbers in a group is even, it might not give the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef1375c",
   "metadata": {},
   "source": [
    "Prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1d310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: The answer is False.\n",
    "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
    "A: The answer is True.\n",
    "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
    "A: The answer is True.\n",
    "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
    "A: The answer is False.\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
    "A:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7093d5",
   "metadata": {},
   "source": [
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d76074e",
   "metadata": {},
   "outputs": [],
   "source": [
    "The answer is True."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c58b3f",
   "metadata": {},
   "source": [
    "In such cases, we need to use other techniques or fine-tune the program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb5d599",
   "metadata": {},
   "source": [
    "# Chain-of-Thought (CoT) Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d50c477",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43b5d039",
   "metadata": {},
   "source": [
    "Image Source: Promptly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba7e133",
   "metadata": {},
   "source": [
    "Chain-of-Thought (CoT) prompting is a technique where we guide a computer program step-by-step through a task. This is especially useful for complex tasks that require some thinking before giving an answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e742dcd",
   "metadata": {},
   "source": [
    "For example, let’s say we want the program to figure out if the sum of odd numbers in a group is even. We can guide it through the reasoning process like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6eb41e",
   "metadata": {},
   "source": [
    "Prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4befb3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
    "A:\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb4810",
   "metadata": {},
   "source": [
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa1c8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e4ae36",
   "metadata": {},
   "source": [
    "Here, the program correctly figures out the answer by following the reasoning steps we provided. This technique can be very powerful, especially when combined with few-shot prompting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fbea56",
   "metadata": {},
   "source": [
    "# Zero-shot CoT Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0a189f",
   "metadata": {},
   "source": [
    " Image Source: Promptly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfac020",
   "metadata": {},
   "source": [
    "Zero-shot Chain-of-Thought (CoT) prompting is a new technique where we ask a computer program to think step-by-step before giving an answer. This can be done by simply adding “Let’s think step by step” to the original question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd78ab6f",
   "metadata": {},
   "source": [
    "For example, let’s say we ask the program a simple math problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d23ce5d",
   "metadata": {},
   "source": [
    "Prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022a1f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"I have a jar with 20 candies. I ate 5 candies in the morning and gave 3 candies to my friend. Later, I bought 10 more candies and put them in the jar. How many candies are in the jar now?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c405c67",
   "metadata": {},
   "source": [
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb64b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"25 candies\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d7ea65",
   "metadata": {},
   "source": [
    "The answer is wrong! Now, let’s try the same problem but ask the program to think step-by-step:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1abf5ec",
   "metadata": {},
   "source": [
    "Prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068238fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"I have a jar with 20 candies. I ate 5 candies in the morning and gave 3 candies to my friend. Later, I bought 10 more candies and put them in the jar. How many candies are in the jar now? Let's think step by step.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e4a8cb",
   "metadata": {},
   "source": [
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3190d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"First, you started with 20 candies.\n",
    "You ate 5 candies in the morning, so you had 15 candies left.\n",
    "Then you gave 3 candies to your friend, so you had 12 candies left.\n",
    "Finally, you bought 10 more candies, so now you have 22 candies in the jar.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1315ac",
   "metadata": {},
   "source": [
    "This time, the program gives the correct answer! This technique can be very useful when we don’t have many examples to guide the program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4158fdf",
   "metadata": {},
   "source": [
    "# Generated Knowledge Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b44e347",
   "metadata": {},
   "source": [
    " Image Source: Promptly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905ea5b5",
   "metadata": {},
   "source": [
    "Large Language Models (LLMs) are getting better and better. One way they’re improving is by using extra information or knowledge to make more accurate guesses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a976dae5",
   "metadata": {},
   "source": [
    "Imagine if we could use these models to create new knowledge before making a guess? That’s what a recent study tried to do. They wanted to see if generating knowledge first could help with tasks like understanding common sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2ef0fe",
   "metadata": {},
   "source": [
    "Let’s look at an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda57449",
   "metadata": {},
   "source": [
    "Question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f072a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Do penguins fly like other birds? Yes or No?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6ef486",
   "metadata": {},
   "source": [
    "A Large Language Model (LLM) might answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3250885f",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb32d780",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62502cc6",
   "metadata": {},
   "source": [
    "This answer is incorrect because penguins are flightless birds. They cannot fly like other birds. This shows that LLMs can sometimes struggle with tasks that require real-world knowledge. So, how can we improve this with knowledge generation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3826acab",
   "metadata": {},
   "source": [
    "First, we generate some knowledge about penguins:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697e43a2",
   "metadata": {},
   "source": [
    "Knowledge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d09307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Penguins are a group of flightless birds. They live almost exclusively in the Southern Hemisphere, with only one species, the Galapagos penguin, found north of the equator. Despite their inability to fly, penguins are excellent swimmers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355ff1f7",
   "metadata": {},
   "source": [
    "We then use this knowledge to answer the question again:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d04607",
   "metadata": {},
   "source": [
    "Question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c178fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "Do penguins fly like other birds? Yes or No?\n",
    "Knowledge: Penguins are a group of flightless birds. They live almost exclusively in the Southern Hemisphere, with only one species, the Galapagos penguin, found north of the equator. Despite their inability to fly, penguins are excellent swimmers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390295cb",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cbd6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "No, penguins do not fly like other birds. They are a group of flightless birds. Despite their inability to fly, penguins are excellent swimmers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e00d49",
   "metadata": {},
   "source": [
    "This time, the answer is correct! This shows that generating knowledge first can help LLMs make better guesses. However, the effectiveness of this technique can vary, and there’s still a lot to learn about how to use it most effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e7d02a",
   "metadata": {},
   "source": [
    "# Directional Stimulus Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de93cd8f",
   "metadata": {},
   "source": [
    "Large Language Models (LLMs) are continually being enhanced, and one of the techniques that have gained popularity is the use of Directional Stimulus Prompting. This method involves providing the model with a hint or nudge in the right direction to help it make more accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99c9a1e",
   "metadata": {},
   "source": [
    "The concept behind Directional Stimulus Prompting is similar to giving a friend a hint when they’re trying to solve a puzzle. You’re not directly telling them the answer, but you’re providing enough information to guide them towards the correct solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdba2ac",
   "metadata": {},
   "source": [
    " Image Source: Promptly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a9ca8d",
   "metadata": {},
   "source": [
    "Let’s illustrate this with a simple example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daa8832",
   "metadata": {},
   "source": [
    "Certainly, let’s imagine you’re asking the model to summarize a news article. Here’s a brief news article example for our situation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127fbb77",
   "metadata": {},
   "source": [
    "News Article: “Yesterday, the major tech company, TechVerse, released their highly anticipated phone, the UltraPhone X. This latest model features a 6.7-inch display, a powerful Zenith Processor, and an improved camera with advanced Night Vision capabilities. It also boasts a durable battery that promises to last throughout the day. TechVerse expects the UltraPhone X to be a massive hit, building upon the success of previous models. The UltraPhone X will be made available for purchase worldwide next week.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7289e030",
   "metadata": {},
   "source": [
    "Now, suppose you want a brief summary of this article. You could use directional stimulus prompting to guide the AI’s summarization process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7092593",
   "metadata": {},
   "source": [
    "Prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664158f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Can you provide a brief summary of this article, focusing on the key details such as the features of the new phone and its release date?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7a7dcb",
   "metadata": {},
   "source": [
    "This prompt nudges the AI to highlight specific elements in its summary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f201160",
   "metadata": {},
   "source": [
    "So, the model might respond with:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04e358c",
   "metadata": {},
   "source": [
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887be5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "TechVerse has unveiled their new model, the UltraPhone X, featuring a 6.7-inch display, a potent Zenith Processor, advanced Night Vision capabilities, and a durable battery. The product, following the success of its older versions, is expected to be available for worldwide purchase next week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe80e89",
   "metadata": {},
   "source": [
    "The extra prompting hint helped guide the AI to focus on the necessary details in its summarization process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3198ff57",
   "metadata": {},
   "source": [
    "This example demonstrates how Directional Stimulus Prompting can be used to guide the model towards the correct answer without explicitly providing the answer. It’s a useful technique for tasks where the model might need a little extra help to arrive at the correct prediction."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
