{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93371d31",
   "metadata": {},
   "source": [
    "source: [link](https://ai-bootcamp.ruangguru.com/learn/06_computer-vision/00_cnn/07_pretrained-model.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c147e6",
   "metadata": {},
   "source": [
    "# Pretrained CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6715c653",
   "metadata": {},
   "source": [
    "# Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39fb5d4",
   "metadata": {},
   "source": [
    "So far we have been creating our own CNN model. Are there pretrained CNN model ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d45694",
   "metadata": {},
   "source": [
    "Yes, there are, quite a lot, some of the most popular are: - ResNet: Residual Networks (ResNets) are known for their effectiveness in deep learning tasks. Models like ResNet-50, ResNet-101, and ResNet-152 are available, and you can fine-tune them on your specific high-res dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a111e0c",
   "metadata": {},
   "source": [
    "- InceptionV3: The Inception architecture, particularly InceptionV3, is designed to capture intricate patterns in images and is suitable for high-res images.\n",
    "- VGG16andVGG19: The VGG architecture, specifically VGG16 and VGG19, consists of multiple convolutional layers and is effective for image classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850c6834",
   "metadata": {},
   "source": [
    "InceptionV3: The Inception architecture, particularly InceptionV3, is designed to capture intricate patterns in images and is suitable for high-res images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f648c8",
   "metadata": {},
   "source": [
    "VGG16 and VGG19: The VGG architecture, specifically VGG16 and VGG19, consists of multiple convolutional layers and is effective for image classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74798132",
   "metadata": {},
   "source": [
    "Let’s start with one of the easiest: AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbee24e4",
   "metadata": {},
   "source": [
    "# AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fcebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# weights = None means that we don't want to load the weights of the model, only the architecture\n",
    "alexnet = models.alexnet(weights=None)\n",
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f53fcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "AlexNet(\n",
    "  (features): Sequential(\n",
    "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
    "    (1): ReLU(inplace=True)\n",
    "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
    "    (4): ReLU(inplace=True)\n",
    "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (7): ReLU(inplace=True)\n",
    "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (9): ReLU(inplace=True)\n",
    "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (11): ReLU(inplace=True)\n",
    "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  )\n",
    "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
    "  (classifier): Sequential(\n",
    "    (0): Dropout(p=0.5, inplace=False)\n",
    "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
    "    (2): ReLU(inplace=True)\n",
    "    (3): Dropout(p=0.5, inplace=False)\n",
    "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
    "    (5): ReLU(inplace=True)\n",
    "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303f2c1f",
   "metadata": {},
   "source": [
    "Please spend some time to understand the output above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c780e73",
   "metadata": {},
   "source": [
    "It’s just a simple layers of Convolutional, MaxPooling, and Linear layers. The exact same Conv2D and MaxPool2D layers that we have been using so far. You also have learned kernal size, stride, padding, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46522a4a",
   "metadata": {},
   "source": [
    "Despite its simplicity, AlexNet is monumental! It’s the winner of the 2012 ImageNet Large Scale Visual Recognition Competition (ILSVRC) beating the second place with huge gap: The network achieved a top-5 error of 15.3%, more than 10.8% points lower than that of the runner up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9bb832",
   "metadata": {},
   "source": [
    "# VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c47ccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "vgg16 = models.vgg16(weights=None)\n",
    "print(vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eadacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG(\n",
    "  (features): Sequential(\n",
    "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (1): ReLU(inplace=True)\n",
    "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (3): ReLU(inplace=True)\n",
    "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (6): ReLU(inplace=True)\n",
    "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (8): ReLU(inplace=True)\n",
    "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (11): ReLU(inplace=True)\n",
    "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (13): ReLU(inplace=True)\n",
    "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (15): ReLU(inplace=True)\n",
    "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (18): ReLU(inplace=True)\n",
    "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (20): ReLU(inplace=True)\n",
    "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (22): ReLU(inplace=True)\n",
    "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (25): ReLU(inplace=True)\n",
    "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (27): ReLU(inplace=True)\n",
    "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (29): ReLU(inplace=True)\n",
    "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  )\n",
    "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
    "  (classifier): Sequential(\n",
    "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
    "    (1): ReLU(inplace=True)\n",
    "    (2): Dropout(p=0.5, inplace=False)\n",
    "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
    "    (4): ReLU(inplace=True)\n",
    "    (5): Dropout(p=0.5, inplace=False)\n",
    "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c7dd0d",
   "metadata": {},
   "source": [
    "You should be able to understand the output above, only Conv2d, MaxPool2d, and Linear layers!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107aa745",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9713685",
   "metadata": {},
   "source": [
    "ResNet is a very deep CNN model, it has 152 layers! (compare it to AlexNet which only has 8 layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8338a41",
   "metadata": {},
   "source": [
    "It’s the winner of the 2015 ImageNet competition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedafb11",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fc2e3b8",
   "metadata": {},
   "source": [
    "![Image](https://storage.googleapis.com/rg-ai-bootcamp/cnn/resnet.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa3d9c0",
   "metadata": {},
   "source": [
    "# Residual Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844915da",
   "metadata": {},
   "source": [
    "The special thing about ResNet is the skip connection, which is the addition of the input to the output of the stacked layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfac1ae",
   "metadata": {},
   "source": [
    "The output of layer $i$-th is passed to layer $i+2$-th, this is called skip connection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6ad835",
   "metadata": {},
   "source": [
    "\n",
    "\\[\n",
    "a^{[l+2]} = g(z^{[l+2]} + a^{[l]})\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cd9e35",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23144153",
   "metadata": {},
   "source": [
    "![Image](https://upload.wikimedia.org/wikipedia/commons/thumb/b/ba/ResBlock.png/330px-ResBlock.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fdf407",
   "metadata": {},
   "source": [
    "The problem with deep neural network is that it’s hard to train, the gradient vanishing problem, i.e. the gradient becomes smaller and smaller as it goes deeper and deeper resulting in the weights of the earlier layers don’t get updated much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea139c8",
   "metadata": {},
   "source": [
    "The skip connection solves this problem by passing the output of the earlier layers to the deeper layers, so the gradient doesn’t vanish."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f285e6c8",
   "metadata": {},
   "source": [
    "We can implement the skip connection in PyTorch by adding the input to the output of the layer (before ReLU):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a338bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "      \n",
    "        # Convolutional layers \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, \n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Skip connection\n",
    "        self.skip = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.skip(x)  # Skip connection\n",
    "        out = torch.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993edc7c",
   "metadata": {},
   "source": [
    "Above is the implementation of ResNet, you can see the skip connection in the forward method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8581293",
   "metadata": {},
   "source": [
    "Put attention to this part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc66b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "out += self.skip(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0d4542",
   "metadata": {},
   "source": [
    "x is the original input. It’s not the output from the previous layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6709751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = self.skip(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853c86ce",
   "metadata": {},
   "source": [
    "# Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619dcea1",
   "metadata": {},
   "source": [
    "Before we go into Applied CNN next, let’s do a quick fine tuning/transfer learning on CNN. We’ll use Resnet18 and the CIFAR10 dataset. ResNet-18 is a convolutional neural network that is 18 layers deep. And it’s pretrained on more than a million images from the ImageNet database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a0545e",
   "metadata": {},
   "source": [
    "For the last step we will save our trained model and use it without training it every time, let’s see how that’s done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbf4fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Download the dataset\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "# Define the model\n",
    "model = resnet18(weights='DEFAULT')\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 1000)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the batch size and number of epochs\n",
    "batch_size = 32\n",
    "num_epochs = 1\n",
    "\n",
    "# Create the data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % 100 == 0:  # Print every 100 mini-batches\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Batch [{i + 1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Test the model\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Print the accuracy\n",
    "    print('Epoch [{}/{}], Test Accuracy: {:.2f}%'.format(epoch+1, num_epochs, 100*correct/total))\n",
    "\n",
    "# Save the trained model to a file\n",
    "torch.save(model.state_dict(), 'resnet18_cifar10_classifier.pth')\n",
    "print('Model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51b4937",
   "metadata": {},
   "outputs": [],
   "source": [
    "Files already downloaded and verified\n",
    "Files already downloaded and verified\n",
    "Epoch [1/1], Batch [100/1563], Loss: 2.4852\n",
    "Epoch [1/1], Batch [200/1563], Loss: 1.3526\n",
    "Epoch [1/1], Batch [300/1563], Loss: 1.2018\n",
    "Epoch [1/1], Batch [400/1563], Loss: 1.0929\n",
    "Epoch [1/1], Batch [500/1563], Loss: 1.0548\n",
    "Epoch [1/1], Batch [600/1563], Loss: 1.0232\n",
    "Epoch [1/1], Batch [700/1563], Loss: 0.9527\n",
    "Epoch [1/1], Batch [800/1563], Loss: 0.9520\n",
    "Epoch [1/1], Batch [900/1563], Loss: 0.9351\n",
    "Epoch [1/1], Batch [1000/1563], Loss: 0.8726\n",
    "Epoch [1/1], Batch [1100/1563], Loss: 0.8719\n",
    "Epoch [1/1], Batch [1200/1563], Loss: 0.8006\n",
    "Epoch [1/1], Batch [1300/1563], Loss: 0.8739\n",
    "Epoch [1/1], Batch [1400/1563], Loss: 0.8623\n",
    "Epoch [1/1], Batch [1500/1563], Loss: 0.7862\n",
    "Epoch [1/1], Test Accuracy: 72.49%\n",
    "Model saved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282a15b5",
   "metadata": {},
   "source": [
    "Let’s try out our model with an image from the internet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825d318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import requests\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import io\n",
    "\n",
    "# Load the model architecture (e.g., ResNet18)\n",
    "model = models.resnet18(weights=None)\n",
    "\n",
    "# Load the saved model weights\n",
    "model_weights_path = 'resnet18_cifar10_classifier.pth'  # Path to the saved model weights on your local disk\n",
    "model.load_state_dict(torch.load(model_weights_path))\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# URL of the image you want to classify\n",
    "#image_url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/200125-N-LH674-1073_USS_Theodore_Roosevelt_%28CVN-71%29.jpg/1200px-200125-N-LH674-1073_USS_Theodore_Roosevelt_%28CVN-71%29.jpg'  # aircraft carrier\n",
    "image_url = 'https://akm-img-a-in.tosshub.com/businesstoday/images/assets/202307/16-17-1-sixteen_nine.jpg?size=948:533' #airplane\n",
    "\n",
    "# Download the image from the URL\n",
    "response = requests.get(image_url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Open the downloaded image with PIL\n",
    "    img = Image.open(io.BytesIO(response.content))\n",
    "else:\n",
    "    print('Failed to download the image.')\n",
    "\n",
    "# Apply the same transformations used during training (resize, normalize, etc.)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize to match model's input size, CIFAR10 was 32x32, try commenting this and see if the prediction is still correct\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "input_data = transform(img).unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "#We use CIFAR10 labels\n",
    "class_to_label = {0: 'airplane', 1: 'automobile', 2: 'bird', 3: 'cat', 4: 'deer', 5: 'dog', 6: 'frog', 7: 'horse', 8: 'ship', 9: 'truck'}\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_data)\n",
    "\n",
    "# Get the predicted class label\n",
    "_, predicted_class = torch.max(output, 1)\n",
    "\n",
    "# Print the predicted class\n",
    "print('Predicted Class:', predicted_class.item())\n",
    "print('Predicted Label:', class_to_label[predicted_class.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa91e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Predicted Class: 0\n",
    "Predicted Label: airplane"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
