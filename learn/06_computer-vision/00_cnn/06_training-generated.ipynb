{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cced5200",
   "metadata": {},
   "source": [
    "source: [link](https://ai-bootcamp.ruangguru.com/learn/06_computer-vision/00_cnn/06_training.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8d17cb",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b99fcc",
   "metadata": {},
   "source": [
    "Let’s try our hand in training a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7690205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision scipy matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338a9397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a CNN model for MNIST\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(64 * 5 * 5, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define data transformations\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load MNIST data\n",
    "mnist_train = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "mnist_test = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Use Data Loader\n",
    "train_loader = DataLoader(mnist_train, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=100, shuffle=False)\n",
    "\n",
    "# Instantiate the CNN model\n",
    "cnn_model = CNNModel()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.SGD(cnn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define accuracy function\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "# Training loop\n",
    "total_epochs = 5\n",
    "for epoch in range(total_epochs):\n",
    "    for images, labels in train_loader:\n",
    "        outputs = cnn_model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, total_epochs, loss.item()))\n",
    "\n",
    "# Evaluation\n",
    "#cnn_model.eval()\n",
    "with torch.no_grad():\n",
    "    accum_acc = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = cnn_model(images)\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        acc = accuracy(outputs, labels)\n",
    "        accum_acc += acc\n",
    "    \n",
    "    print('Test loss: {:.4f}, Test accuracy: {:.4f}'.format(loss.item(), accum_acc/len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec45315",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch [1/5], Loss: 0.7850\n",
    "Epoch [2/5], Loss: 0.4941\n",
    "Epoch [3/5], Loss: 0.4238\n",
    "Epoch [4/5], Loss: 0.4913\n",
    "Epoch [5/5], Loss: 0.4813\n",
    "Test loss: 0.4732, Test accuracy: 0.8098"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0780654",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9715aa8",
   "metadata": {},
   "source": [
    "Can we train a CNN mdoel on a relatively small dataset ? What happens if the dataset is small ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f545ad9",
   "metadata": {},
   "source": [
    "It is possible to train on a small dataset, and quite accurate too. However there is one major problem, if the input image differs, for example, it’s upside down, the model will fail. This is known as overfitting. Overfitting occurs when a model learns to perform well on the training data but fails to generalize to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f252962",
   "metadata": {},
   "source": [
    "To overcome this issue, we can use data augmentation. What is Data augmentation ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace73e2d",
   "metadata": {},
   "source": [
    "Basically we artificially increase the size and diversity of the training dataset. We can do this by: - Rotation: Data augmentation can involve rotating the digit images by various angles. This helps the model learn to recognize digits even if they are slightly tilted or rotated when written by different people. - Scaling and Shearing: You can apply transformations that stretch or compress the digit images in both the x and y directions. This allows the model to handle variations in digit size and aspect ratio. - Translation: Shifting the digit images within the image frame helps the model learn to recognize digits in different positions on the input image. - Noise: Adding random noise to the images simulates variations in writing style and drawing imperfections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54af140",
   "metadata": {},
   "source": [
    "Let’s assume we want to make sure that make sure that our CNN model based on the MNIST dataset to recognize digits written by various individuals with different writing styles. Here’s what we could do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ff390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "\n",
    "# Example data augmentation transformations\n",
    "data_augmentation = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=(-10, 10), fill=0),  # Fill with black for rotation\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.RandomResizedCrop(size=(28, 28), scale=(0.8, 1.2)),\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "    transforms.RandomErasing(p=0.5),\n",
    "])\n",
    "\n",
    "# Create a custom dataset class to store augmented data\n",
    "class AugmentedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, original_dataset, data_augmentation):\n",
    "        self.original_dataset = original_dataset\n",
    "        self.data_augmentation = data_augmentation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.original_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.original_dataset[idx]\n",
    "        augmented_image = self.data_augmentation(image)\n",
    "        return augmented_image, label\n",
    "\n",
    "# Create an augmented dataset\n",
    "augmented_dataset = AugmentedDataset(train_dataset, data_augmentation)\n",
    "\n",
    "# Choose a digit class (e.g., digit 7)\n",
    "digit_class = 7\n",
    "\n",
    "# Filter the dataset to get images of the chosen class\n",
    "digit_images = [image for image, label in train_dataset if label == digit_class]\n",
    "\n",
    "# Apply data augmentation to the images and convert to PIL Images\n",
    "augmented_images_pil = [transforms.ToPILImage()(data_augmentation(image)) for image in digit_images]\n",
    "\n",
    "# Convert PIL Images to NumPy arrays before visualization\n",
    "original_images_np = [image.squeeze().numpy() for image in digit_images]\n",
    "augmented_images_np = [np.array(image) for image in augmented_images_pil]\n",
    "\n",
    "# Visualize original and augmented images\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i in range(5):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(original_images_np[i], cmap='gray')\n",
    "    plt.title(\"Original\")\n",
    "\n",
    "for i in range(5):\n",
    "    plt.subplot(2, 5, i + 6)\n",
    "    plt.imshow(augmented_images_np[i], cmap='gray')\n",
    "    plt.title(\"Augmented\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28270e4c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db1ad4c8",
   "metadata": {},
   "source": [
    "How do we combine them ? We can use ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2932244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "# Assuming you have a dataset object, e.g., mnist_train\n",
    "num_images = len(train_dataset)\n",
    "print(\"Number of images in the dataset (before):\", num_images)\n",
    "\n",
    "# Combine the original and augmented datasets\n",
    "combined_dataset = ConcatDataset([train_dataset, augmented_dataset])\n",
    "\n",
    "# Create a DataLoader for the combined dataset\n",
    "combined_train_loader = DataLoader(combined_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "# Assuming you have a dataset object, e.g., mnist_train\n",
    "num_images = len(combined_dataset)\n",
    "print(\"Number of images in the dataset (after):\", num_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b32165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Number of images in the dataset (before): 60000\n",
    "Number of images in the dataset (after): 120000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9933ab8f",
   "metadata": {},
   "source": [
    "Next we can train them as usual. Pretty neat, eh ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bc35fd",
   "metadata": {},
   "source": [
    "# Exercise CNN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69a1728",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rggrader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e4d2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title #### Student Identity\n",
    "student_id = \"student_id\" # @param {type:\"string\"}\n",
    "name = \"your_name\" # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a121ed8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title #### 00. CNN Model using SHVN Dataset\n",
    "\n",
    "from rggrader import submit\n",
    "\n",
    "# TODO: Train a model on your own, using the SVHN (Street View House Numbers) dataset :: https://huggingface.co/datasets/svhn\n",
    "\n",
    "# You may add any code here to derive your variables\n",
    "# Please change this\n",
    "accuracy = 0\n",
    "\n",
    "print(f\"The accuracy is {accuracy}\")\n",
    "\n",
    "\n",
    "# Submit Method\n",
    "assignment_id = \"03_cnn\"\n",
    "question_id = \"01_training_svhn\"\n",
    "submit(student_id, name, assignment_id, str(accuracy), question_id, \"\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
