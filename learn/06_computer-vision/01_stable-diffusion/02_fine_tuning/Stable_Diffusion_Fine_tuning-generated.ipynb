{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "248d5ecb",
   "metadata": {},
   "source": [
    "# Stable Difussion Fine-tuning with Dreambooth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d28169",
   "metadata": {},
   "source": [
    "Imagine the scenario where we need to generate images that resemble a specific person’s face, but that face is not included in the model’s training data, we would rely on the model’s ability to generalize from its learned representations. fine-tuning is a great approach for this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf142685",
   "metadata": {},
   "source": [
    "The model, after being trained on a diverse set of faces, should have grasped the underlying patterns and features common to human faces. A representation of a specific person’s face, such as a sketch or a description, would be inputted, and the model would generate a new face that aligns with the input as closely as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e316edb",
   "metadata": {},
   "source": [
    "Fine-tuning is a technique used to train a custom model based on existing models, enabling the generation of custom images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5810de0b",
   "metadata": {},
   "source": [
    "For example, personal photos can be added to the model, allowing it to generate unique images in various scenarios such as mountains, forests, streets, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc30e533",
   "metadata": {},
   "source": [
    "# Methods of Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0956a8be",
   "metadata": {},
   "source": [
    "There are several methods to apply fine tuning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a71048b",
   "metadata": {},
   "source": [
    "This involves training a base model with an additional dataset. For instance, you can train stable diffusion with an additional old car dataset to orient the aesthetics of the cars to that specific type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476df913",
   "metadata": {},
   "source": [
    "Initially developed by Google, this technique allows for injecting custom subjects into the models. Due to its architecture, it is possible to achieve great results using only 3 or 5 custom images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac6bd7c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a95b6be3",
   "metadata": {},
   "source": [
    "![Image](https://storage.googleapis.com/rg-ai-bootcamp/stable-diffusion/dreambooth-high-level.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639358fd",
   "metadata": {},
   "source": [
    "Source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5791b3ae",
   "metadata": {},
   "source": [
    "In this course, the focus is on using the Dreambooth algorithm for fine-tuning the Stable Diffusion model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a541c48c",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f72649",
   "metadata": {},
   "source": [
    "In the implementation, we’ll use an image of a name in his 30s that we’ll call John. This man was generated using Stable Diffusion to avoid copyright infringement. You can simply use a picture of yourself if you want to try things out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1881eafb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64affab8",
   "metadata": {},
   "source": [
    "![Image](https://storage.googleapis.com/rg-ai-bootcamp/stable-diffusion/john.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782135f7",
   "metadata": {},
   "source": [
    "# Installing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857355e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/examples/dreambooth/train_dreambooth.py\n",
    "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n",
    "%pip install -qq git+https://github.com/ShivamShrirao/diffusers\n",
    "%pip install -q -U --pre triton\n",
    "%pip install -q accelerate transformers ftfy bitsandbytes==0.35.0 gradio natsort safetensors xformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bf522d",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5467084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sd = \"runwayml/stable-diffusion-v1-5\"\n",
    "output_dir = \"/content/stable_diffusion_weights/john\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782511ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19aa03c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7404c4a",
   "metadata": {},
   "source": [
    "Dreambooth training requires a unique identifier, the class name, and images of the subject to be inserted. The images form the dataset. The unique identifier needs to be a term associated with no concept or feature recognized by the model. The class is the type of object you want to generate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db94e719",
   "metadata": {},
   "source": [
    "Three components are needed:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71523e2d",
   "metadata": {},
   "source": [
    "This is a unique name that does not exist in the model. In our case, we will use john."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49701a56",
   "metadata": {},
   "source": [
    "This is the type of object that will be generated. In our case, we will generate faces of people."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec2271e",
   "metadata": {},
   "source": [
    "These are the training datasets. In our case, we have uploaded ten images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01eb349",
   "metadata": {},
   "source": [
    "Instance prompt a photo of [unique identifier] [class name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db9b5c2",
   "metadata": {},
   "source": [
    "Class prompt > a photo of [class name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b0c003",
   "metadata": {},
   "source": [
    "The instance prompt will be as follows: > a photo of john person"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2028495a",
   "metadata": {},
   "source": [
    "As the subject is a person, the class prompt will be as follows: > a photo of a person"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f756ea0a",
   "metadata": {},
   "source": [
    "In this demonstration, we’ll be utilizing photos of John as basis to train the Stable Diffusion Model, aiming to generate images similar to John."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7429e677",
   "metadata": {},
   "source": [
    "# Creating the Instance and Class Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79337b50",
   "metadata": {},
   "source": [
    "We need to create a new variable concepts_list. It will be a list in Python. Then we need to specify the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c155da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts_list = [\n",
    "    {\n",
    "        \"instance_prompt\": \"john\",\n",
    "        \"class_prompt\": \"photo of a person\",\n",
    "        \"instance_data_dir\": \"/content/data/john\",\n",
    "        \"class_data_dir\": \"/content/data/person\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3b6628",
   "metadata": {},
   "source": [
    "# Creating Directories and JSON File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fbedb3",
   "metadata": {},
   "source": [
    "We need to create directories and convert this variable into a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5cb9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "for c in concepts_list:\n",
    "  os.makedirs(c[\"instance_data_dir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace3db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"concepts_list.json\", \"w\") as f:\n",
    "  json.dump(concepts_list, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2a2739",
   "metadata": {},
   "source": [
    "# Upload Training Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb3c8c8",
   "metadata": {},
   "source": [
    "# Specifying Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5766acef",
   "metadata": {},
   "source": [
    "We need to specify some parameters before running the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce44fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_imgs = 15\n",
    "num_class_images = num_imgs * 12\n",
    "max_num_steps = num_imgs * 100\n",
    "learning_rate = 1e-6\n",
    "lr_warmup_steps = int(max_num_steps / num_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884cd6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_imgs, num_class_images, max_num_steps, learning_rate, lr_warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80032e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "15 180 1500 1e-06 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59643e2",
   "metadata": {},
   "source": [
    "The learning_rate is a hyperparameter that determines the step size at which an optimization algorithm (like gradient descent) proceeds while learning from the data. It controls how much to change the model in response to the estimated error each time the model weights are updated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a97603",
   "metadata": {},
   "source": [
    "If the learning rate is too small, the model will need many updates to converge to the best values, which can take a long time. On the other hand, if the learning rate is too large, the updates may be too significant and the model may pass over the optimal solution, or even diverge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea02bd5",
   "metadata": {},
   "source": [
    "lr_warmup_steps is a hyperparameter used in the learning rate scheduling strategy, specifically in the warmup phase of training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7049b1",
   "metadata": {},
   "source": [
    "Learning rate warmup is a strategy where the learning rate is initially set to a small value and gradually increased to the maximum or initial learning rate. This is done over a certain number of steps or epochs, which is what lr_warmup_steps refers to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4301b2",
   "metadata": {},
   "source": [
    "The purpose of this strategy is to prevent the model from overfitting early in the training process. By starting with a smaller learning rate, the model makes smaller adjustments and doesn’t converge too quickly to a suboptimal solution. After the warmup steps, the learning rate is increased to allow the model to learn more quickly and converge to the optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9ccb6c",
   "metadata": {},
   "source": [
    "# Execute Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8876df03",
   "metadata": {},
   "source": [
    "Finally, we can train the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a91416",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train_dreambooth.py \\\n",
    "  --pretrained_model_name_or_path=$model_sd \\\n",
    "  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
    "  --instance_data_dir=$output_dir \\\n",
    "  --output_dir=$output_dir \\\n",
    "  --revision=\"fp16\" \\\n",
    "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
    "  --seed=777 \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --train_text_encoder \\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --use_8bit_adam \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --learning_rate=$learning_rate \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=$lr_warmup_steps \\\n",
    "  --num_class_images=$num_class_images \\\n",
    "  --sample_batch_size=4 \\\n",
    "  --max_train_steps=$max_num_steps \\\n",
    "  --save_interval=10000 \\\n",
    "  --save_sample_prompt=\"john\" \\\n",
    "  --concepts_list=\"concepts_list.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd6ab80",
   "metadata": {},
   "source": [
    "This process will take about 20 minutes to finish. If an error occurs during training, ensure that the images or datasets are in the correct folder. Once the training is complete, we can proceed to perform the first tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760eeeb3",
   "metadata": {},
   "source": [
    "# Images and model weights are stored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a623e8",
   "metadata": {},
   "source": [
    "The weights directory is a specific location in the file system where the weights of a trained machine learning model are stored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca60ee0",
   "metadata": {},
   "source": [
    "These weights are the learned parameters that the model uses to make predictions or decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e1605e",
   "metadata": {},
   "source": [
    "They are typically saved so that the model can be reused later, either for further training, for fine-tuning on a different task, or for direct inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77856a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from natsort import natsorted\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "weights_dir = natsorted(glob(output_dir + os.sep + '*'))[-1]\n",
    "print('Weights directory: ', weights_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec562260",
   "metadata": {},
   "outputs": [],
   "source": [
    "Weights directory:  /content/stable_diffusion_weights/john/1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113fee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# function to display images in grid\n",
    "def grid_img(imgs, rows=1, cols=3, scale=1):\n",
    "  assert len(imgs) == rows * cols\n",
    "\n",
    "  w, h = imgs[0].size\n",
    "  w, h = int(w*scale), int(h*scale)\n",
    "\n",
    "  grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "  grid_w, grid_h = grid.size\n",
    "\n",
    "  for i, img in enumerate(imgs):\n",
    "      img = img.resize((w,h), Image.ANTIALIAS)\n",
    "      grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "  return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4ceddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_folder = output_dir\n",
    "folders = sorted([f for f in os.listdir(weights_folder) if f != \"0\"], key = lambda x: int(x))\n",
    "\n",
    "imgs_test = []\n",
    "\n",
    "for imgs, folder in enumerate(folders):\n",
    "  folder_path = os.path.join(weights_folder, folder)\n",
    "  image_folder = os.path.join(folder_path, \"samples\")\n",
    "  images = [f for f in os.listdir(image_folder)]\n",
    "\n",
    "  for i in images:\n",
    "    img_path = os.path.join(image_folder, i)\n",
    "    r = Image.open(img_path)\n",
    "    imgs_test.append(r)\n",
    "\n",
    "# show images that generated after training\n",
    "grid_img(imgs_test, rows=1, cols=4, scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e4b32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
    "  img = img.resize((w,h), Image.ANTIALIAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7906b9c8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdb973b6",
   "metadata": {},
   "source": [
    "# Convert the weights into (checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f72ec52",
   "metadata": {},
   "source": [
    "Checkpoints are used to save and load the progress of training, allowing you to resume training from the exact point it was stopped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326619da",
   "metadata": {},
   "source": [
    "Converting the weights into a checkpoint involves saving the current state of the model, including its learned weights, into a format that can be easily loaded later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6e5274",
   "metadata": {},
   "source": [
    "This process allows for the model’s state to be preserved, so that the training process can be resumed later if needed, or the trained model can be used for generating images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee28bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = weights_dir + \"/model.ckpt\"\n",
    "\n",
    "half_arg = \"--half\"\n",
    "\n",
    "!python convert_diffusers_to_original_stable_diffusion.py --model_path $weights_dir  --checkpoint_path $ckpt_path $half_arg\n",
    "print(f\"Converted to ckpt and saved in {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6a642c",
   "metadata": {},
   "source": [
    "# Inference / Generating images (tests the fine tune model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b84ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import autocast\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d6d3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = weights_dir\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183f3948",
   "metadata": {},
   "outputs": [],
   "source": [
    "/content/stable_diffusion_weights/john/1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31adef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7f95eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "pipe.safety_checker = lambda images, clip_input: (images, False)\n",
    "\n",
    "seed = 555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b199943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"face portrait of john playing guitar in the restaurant, realistic, hd, vivid, sunset\"\n",
    "negative_prompt = \"bad anatomy, ugly, deformed, desfigured, distorted face, poorly drawn hands, poorly drawn face, poorly drawn feet, blurry, low quality, low definition, lowres, out of frame, out of image, cropped, cut off, signature, watermark\"\n",
    "num_samples = 5\n",
    "guidance_scale = 7.5\n",
    "num_inference_steps = 30\n",
    "height = 512\n",
    "width = 512\n",
    "\n",
    "seed = 123\n",
    "print(\"Seed: {}\".format(str(seed)))\n",
    "generator = torch.Generator(device='cuda').manual_seed(seed)\n",
    "\n",
    "with autocast(\"cuda\"), torch.inference_mode():\n",
    "    imgs = pipe(\n",
    "        prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        height=height, width=width,\n",
    "        num_images_per_prompt=num_samples,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=generator\n",
    "    ).images\n",
    "\n",
    "for img in imgs:\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739b01e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Seed: 123"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9880c05d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6673953",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "049cc13b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96905038",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "695560aa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "809cc5b7",
   "metadata": {},
   "source": [
    "# Testing multiple prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9ecabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"photo of john person, closeup, mountain fuji in the background, natural lighting\",\n",
    "          \"photo of john person in the desert, closeup, pyramids in the background, natural lighting, frontal face\",\n",
    "          \"photo of john person in the forest, natural lighting, frontal face\",\n",
    "          \"photo of john person as an astronaut, natural lighting, frontal face, closeup, starry sky in the background\",\n",
    "          \"face portrait of john in the snow, realistic, hd, vivid, sunset\"]\n",
    "\n",
    "negative_prompt = [\"bad anatomy, ugly, deformed, desfigured, distorted face, poorly drawn hands, poorly drawn face, poorly drawn feet, blurry, low quality, low definition, lowres, out of frame, out of image, cropped, cut off, signature, watermark\" ] * len(prompt)\n",
    "num_samples = 1\n",
    "guidance_scale = 8\n",
    "num_inference_steps = 75\n",
    "height = 512\n",
    "width = 512\n",
    "\n",
    "seed = 88\n",
    "print(\"Seed: {}\".format(str(seed)))\n",
    "generator = torch.Generator(device='cuda').manual_seed(seed)\n",
    "\n",
    "with autocast(\"cuda\"), torch.inference_mode():\n",
    "    imgs = pipe(\n",
    "        prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        height=height, width=width,\n",
    "        num_images_per_prompt=num_samples,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=generator\n",
    "    ).images\n",
    "\n",
    "for img in imgs:\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eacc360",
   "metadata": {},
   "outputs": [],
   "source": [
    "Seed: 88"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1a73db",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3d4feb8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9e11204",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a01cec3f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf8c0025",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bbb7f3a",
   "metadata": {},
   "source": [
    "# More prompt examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e671e8",
   "metadata": {},
   "source": [
    "Other combinations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7139532",
   "metadata": {},
   "source": [
    "photo of john person, closeup, mountain fuji in the background, natural lighting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe6ce99",
   "metadata": {},
   "source": [
    "digital painting of john in the snow, realistic, hd, vivid, sunset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f031f2af",
   "metadata": {},
   "source": [
    "watercolor painting of john person, realistic, blue and orange tones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e418fcb7",
   "metadata": {},
   "source": [
    "digital painting of john person, hyperrealistic, fantasy, Surrealist, painted by Alphonse Mucha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13221218",
   "metadata": {},
   "source": [
    "painting of john person in star wars, realistic, 4k ultra hd, blue and red tones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b381696",
   "metadata": {},
   "source": [
    "photo of john person, in an armor, realistic, visible face, colored, detailed face, ultra detailed, natural lighting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22335618",
   "metadata": {},
   "source": [
    "photo of john person, cyberpunk, vivid, realistic, 4k ultra hd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fa14c5",
   "metadata": {},
   "source": [
    "anime painting of john person, chill day, by tim okamura, noah bradley, trending on artstation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f678df3",
   "metadata": {},
   "source": [
    "# Saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbcf53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987706c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir: cannot create directory ‘results’: File exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eb8c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, img in enumerate(imgs):\n",
    "  img.save('results/result_{}.png'.format(i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743df0a2",
   "metadata": {},
   "source": [
    "# Exercise Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcacb7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing Libs\n",
    "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/examples/dreambooth/train_dreambooth.py\n",
    "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n",
    "%pip install -qq git+https://github.com/ShivamShrirao/diffusers\n",
    "%pip install -q -U --pre triton\n",
    "%pip install -q accelerate transformers ftfy bitsandbytes==0.35.0 gradio natsort safetensors xformers\n",
    "%pip install rggrader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fad997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title #### Student Identity\n",
    "student_id = \"your student id\" # @param {type:\"string\"}\n",
    "name = \"your name\" # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddf5cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title #### 00. Fine-tuning with Dreambooth\n",
    "from rggrader import submit_image\n",
    "import torch\n",
    "from torch import autocast\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "from IPython.display import display\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from natsort import natsorted\n",
    "from glob import glob\n",
    "import json\n",
    "import random\n",
    "\n",
    "# TODO:\n",
    "# 1. Load Model: Load 'runwayml/stable-diffusion-v1-5'.\n",
    "# 2. Prepare Images: use your photos to train the provided model.\n",
    "# 3. Fine-Tune: Train the model on your dataset.\n",
    "# 4. Generate Faces: Use the fine-tuned model to create new faces.\n",
    "# 5. Save Results: Store the generated images in the 'results' folder.\n",
    "# 6. Upload Image: Choose one image from 'results' and upload it for review.\n",
    "\n",
    "# Note: Create folder '/content/data/input_image' to upload Training Image\n",
    "\n",
    "# Loading model and create output dir\n",
    "model_sd = \"runwayml/stable-diffusion-v1-5\"\n",
    "output_dir = \"/content/stable_diffusion_weights/student_data\"\n",
    "!mkdir -p $output_dir\n",
    "\n",
    "# Put your code here:\n",
    "imgs = None\n",
    "\n",
    "# ---- End of your code ----\n",
    "\n",
    "# Saving the results\n",
    "!mkdir results\n",
    "for i, img in enumerate(imgs):\n",
    "    img.save('results/result_{}.png'.format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cd7690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit Method\n",
    "assignment_id = \"00_fine_tuning\"\n",
    "question_id = \"00_fine_tuning_with_dreambooth\"\n",
    "submit_image(student_id, question_id, 'your_image.png') # change 'your_image.png' to the name of the image you want to upload (eg. results/result_3.png)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
