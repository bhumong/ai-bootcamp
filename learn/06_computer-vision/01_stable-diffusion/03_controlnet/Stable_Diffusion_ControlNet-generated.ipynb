{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab1c36e8",
   "metadata": {},
   "source": [
    "source: [link](https://ai-bootcamp.ruangguru.com/learn/06_computer-vision/01_stable-diffusion/03_controlnet/Stable_Diffusion_ControlNet.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4523d19b",
   "metadata": {},
   "source": [
    "# ControlNet with Stable Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32e0ddf",
   "metadata": {},
   "source": [
    "This ControlNet is considered another way to guide the results in terms of composition and general of the image, as we have learned before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f118fd90",
   "metadata": {},
   "source": [
    "As we know, it is possible to generate images from text, from images, and even by training custom model. However, Control Net introduces a new way to guide the generation of images, for example, we can control the Depth to Image, where both a text prompt and a depth image are used to condition the model. This allows you to get even more accurate results than the common image-to-image technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701a154e",
   "metadata": {},
   "source": [
    "# About the technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e176aaf2",
   "metadata": {},
   "source": [
    "Paper: https://arxiv.org/pdf/2302.05543.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59af904",
   "metadata": {},
   "source": [
    "ControlNet is a method used to manage the behavior of a neural network. It does this by adjusting the input conditions of the building blocks of the neural network, which are called network blocks. For example, in a restnet pretrained CNN model, residual network is a network block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1d0f20",
   "metadata": {},
   "source": [
    "# ControlNet 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac79e0d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f528938e",
   "metadata": {},
   "source": [
    "![Image](https://raw.githubusercontent.com/lllyasviel/ControlNet/main/github_page/he.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae8c366",
   "metadata": {},
   "source": [
    "The image illustrates how to apply a ControlNet to any neural network block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f025a7be",
   "metadata": {},
   "source": [
    "The x and y represent deep features in neural networks. These are the complex representations that the network learns from the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66a0bb0",
   "metadata": {},
   "source": [
    "The + symbol refers to feature addition, which is a way of combining the information from different features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42835ae8",
   "metadata": {},
   "source": [
    "The c represents an extra condition that is added to the neural network. This could be any additional information that you want the network to consider when making its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b184d45",
   "metadata": {},
   "source": [
    "In implementing ControlNet, there are various techniques that can be used to condition the model. However, for this discussion, the focus will be on two specific methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c747fdc1",
   "metadata": {},
   "source": [
    "This technique involves identifying the boundaries of objects within an image. The Canny Edge Detection method is a popular algorithm that’s used to detect a wide range of edges in images. It’s used to help the model understand the shapes present in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a740b31",
   "metadata": {},
   "source": [
    "This technique is about understanding the pose of a person in an image or video. Open Pose is a library that allows for real-time multi-person keypoint detection. It can identify where people are and how they are posed in an image or video. This information can be used to condition the model to understand and learn from the poses present in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fd38fb",
   "metadata": {},
   "source": [
    "For more detailed information about implementing ControlNet and the various techniques used to condition the model, you can refer to the ControlNet GitHub repository. This resource provides comprehensive documentation, code examples, and further reading to help you understand and implement ControlNet effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437b45e2",
   "metadata": {},
   "source": [
    "# Installing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529b8f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers==0.14\n",
    "!pip install -q accelerate transformers xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873bbd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q opencv-contrib-python\n",
    "!pip install -q controlnet_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83f0fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
    "import torch\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2a966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function show image as grid\n",
    "def grid_img(imgs, rows=1, cols=3, scale=1):\n",
    "  assert len(imgs) == rows * cols\n",
    "\n",
    "  w, h = imgs[0].size\n",
    "  w, h = int(w*scale), int(h*scale)\n",
    "\n",
    "  grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "  grid_w, grid_h = grid.size\n",
    "\n",
    "  for i, img in enumerate(imgs):\n",
    "      img = img.resize((w,h), Image.ANTIALIAS)\n",
    "      grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "  return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d72ee0",
   "metadata": {},
   "source": [
    "# Generating Images Using Edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f7cb07",
   "metadata": {},
   "source": [
    "# ControlNet Model + Canny Edge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d858b40a",
   "metadata": {},
   "source": [
    "This is the algorithm used to extract the edges of images. It will be easier to understand during the implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cf73c4",
   "metadata": {},
   "source": [
    "We are creating the variable control_net_canny_model with the corresponding link to download it from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c61669",
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet_canny_model = 'lllyasviel/sd-controlnet-canny'\n",
    "control_net_canny = ControlNetModel.from_pretrained(controlnet_canny_model, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd07255",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionControlNetPipeline.from_pretrained('runwayml/stable-diffusion-v1-5',\n",
    "                                                         controlnet=control_net_canny,\n",
    "                                                         torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eb7385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UniPCMultistepScheduler\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3b8d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.enable_attention_slicing()\n",
    "pipe.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d87fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ec6eed",
   "metadata": {},
   "source": [
    "# Loading the image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45321705",
   "metadata": {},
   "source": [
    "Now we can load the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77274999",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('/content/bird2.jpg')\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4323b12e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62cf9dc7",
   "metadata": {},
   "source": [
    "# Detecting edges using Canny Edge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89320323",
   "metadata": {},
   "source": [
    "In this technique, we are going to use the Canny edge algorithm to extract only the borders of the image. So instead of sending the whole image to the algorithm, we are going to send only the borders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0af29b3",
   "metadata": {},
   "source": [
    "We are going to create a function that will receive as parameter this image and will return the edges. We don’t need to worry about it because OpenCV has a pre-built function, so we just need to call it to extract the edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdad22f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def canny_edge(img, low_threshold = 100, high_threshold = 200):\n",
    "  img = np.array(img)\n",
    "\n",
    "  img = cv2.Canny(img, low_threshold, high_threshold)\n",
    "\n",
    "  img = img[:, :, None]\n",
    "\n",
    "  img = np.concatenate([img, img, img], axis = 2)\n",
    "\n",
    "  canny_img = Image.fromarray(img)\n",
    "\n",
    "  return canny_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55631e8f",
   "metadata": {},
   "source": [
    "we can visualize the edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34ce569",
   "metadata": {},
   "outputs": [],
   "source": [
    "canny_img = canny_edge(img)\n",
    "canny_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1be5ec8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1087283b",
   "metadata": {},
   "source": [
    "we are able to visualize only the edges that have been extracted. Just a reminder that instead of sending the whole image to the algorithm, we are going to send only the edges. Then the algorithm will be able to generate new birds according to the edges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3457fd25",
   "metadata": {},
   "source": [
    "We create a prompt, a seed for reproducibility, and a generator. Then we call the pipeline, sending the prompt and the edges of the image as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b887a6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"realistic photo of a blue bird with purple details, high quality, natural light\"\n",
    "neg_prompt = \"\"\n",
    "\n",
    "seed = 777\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
    "\n",
    "imgs = pipe(\n",
    "    prompt,\n",
    "    canny_img,\n",
    "    negative_prompt=neg_prompt,\n",
    "    generator=generator,\n",
    "    num_inference_steps=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db72dde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs.images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f27a8ca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8acec7b",
   "metadata": {},
   "source": [
    "We can see a high-quality image that is related to the edges and is also in accordance with the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b6fc1f",
   "metadata": {},
   "source": [
    "We can perform tests using different prompts and negative prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146d7fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"realistic photo of a blue bird with purple details, high quality, natural light\",\n",
    "          \"realistic photo of a bird in new york during autumn, city in the background\",\n",
    "          \"oil painting of a black bird in the desert, realistic, vivid, fantasy, surrealist, best quality, extremely detailed\",\n",
    "          \"digital painting of a blue bird in space, stars and galaxy in the background, trending on artstation\"]\n",
    "\n",
    "neg_prompt = [\"blurred, lowres, bad anatomy, ugly, worst quality, low quality, monochrome, signature\"] * len(prompt)\n",
    "\n",
    "seed = 777\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
    "\n",
    "imgs = pipe(\n",
    "    prompt,\n",
    "    canny_img,\n",
    "    negative_prompt=neg_prompt,\n",
    "    generator=generator,\n",
    "    num_inference_steps=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99e501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_img(imgs.images, 1, len(prompt), scale=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1830f81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
    "  img = img.resize((w,h), Image.ANTIALIAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7dfab1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9784b9bb",
   "metadata": {},
   "source": [
    "Let’s try with another image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da63fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"/content/wolf.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b83c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "canny_img = canny_edge(img, 200, 255)\n",
    "\n",
    "grid_img([img, canny_img], 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aa2a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
    "  img = img.resize((w,h), Image.ANTIALIAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aa0d3f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f32bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"realistic photo of a wolf, high quality, natural light, full moon\",\n",
    "          \"realistic photo of a wolf in the snow, best quality, extremely detailed\",\n",
    "          \"oil painting of wolf the desert, canyons in the background, realistic, vivid, fantasy, surrealist, best quality, extremely detailed\",\n",
    "          \"watercolor painting of a wolf in space, blue and purple tones, stars and earth in the background\"]\n",
    "\n",
    "neg_prompt = [\"blurred, lowres, bad anatomy, ugly, worst quality, low quality, monochrome, signature\"] * len(prompt)\n",
    "\n",
    "seed = 777\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
    "\n",
    "imgs = pipe(\n",
    "    prompt,\n",
    "    canny_img,\n",
    "    negative_prompt=neg_prompt,\n",
    "    generator=generator,\n",
    "    num_inference_steps=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2a7052",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_img(imgs.images, 1, len(prompt), scale=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6213305",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
    "  img = img.resize((w,h), Image.ANTIALIAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5751de91",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24d75338",
   "metadata": {},
   "source": [
    "# Generating Images Using Poses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73abcf18",
   "metadata": {},
   "source": [
    "We will learn how to generate images using poses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2da29c9",
   "metadata": {},
   "source": [
    "If the desired images cannot be found, there are several online 3D software options available for creating posed images:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ad1636",
   "metadata": {},
   "source": [
    "# Loading the model to extract poses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f530ec9b",
   "metadata": {},
   "source": [
    "The first step is to download the model from controlnet_aux, a library we will import."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaa7530",
   "metadata": {},
   "source": [
    "We will also import the OpenposeDetector. We will send an image to this detector and it will return the pose of that image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfbb9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from controlnet_aux import OpenposeDetector\n",
    "pose_model = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d706409",
   "metadata": {},
   "source": [
    "# Extract The Pose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb59308",
   "metadata": {},
   "source": [
    "First, the image is loaded. Then, the pose is extracted using the pose_model function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af3358b",
   "metadata": {},
   "source": [
    "We will see the pose that has been extracted from the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6cd01d",
   "metadata": {},
   "source": [
    "The extracted keypoints represent specific points related to various body parts such as the head, shoulders, arms, hands, legs, feet, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0fc8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pose = Image.open('/content/pose01.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f93fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = pose_model(img_pose)\n",
    "grid_img([img_pose, pose], rows=1, cols=2, scale=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763623f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
    "  img = img.resize((w,h), Image.ANTIALIAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca2503d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4af3fbc",
   "metadata": {},
   "source": [
    "# Loading the ControlNet model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2704e57",
   "metadata": {},
   "source": [
    "The next step is to load the ControlNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcde438",
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet_pose_model = ControlNetModel.from_pretrained('thibaud/controlnet-sd21-openpose-diffusers', torch_dtype=torch.float16)\n",
    "sd_controlpose = StableDiffusionControlNetPipeline.from_pretrained('stabilityai/stable-diffusion-2-1-base',\n",
    "                                                                   controlnet=controlnet_pose_model,\n",
    "                                                                   torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5390159",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_controlpose.enable_model_cpu_offload()\n",
    "sd_controlpose.enable_attention_slicing()\n",
    "sd_controlpose.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1033123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DEISMultistepScheduler\n",
    "\n",
    "sd_controlpose.scheduler = DEISMultistepScheduler.from_config(sd_controlpose.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca85ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 555\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
    "prompt = \"professional photo of a young woman in the street, casual fashion, sharp focus, insanely detailed, photorealistic, sunset, side light\"\n",
    "neg_prompt = \"ugly, tiling, closed eyes, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, extra limbs, disfigured, deformed, body out of frame, bad anatomy, watermark, signature, cut off, low contrast, underexposed, overexposed, bad art, beginner, amateur, distorted face\"\n",
    "\n",
    "imgs = sd_controlpose(\n",
    "    prompt,\n",
    "    pose,\n",
    "    negative_prompt=neg_prompt,\n",
    "    num_images_per_prompt=4,\n",
    "    generator=generator,\n",
    "    num_inference_steps=20,\n",
    ")\n",
    "grid_img(imgs.images, 1, 4, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c21d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
    "  img = img.resize((w,h), Image.ANTIALIAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410d1634",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8d46f74",
   "metadata": {},
   "source": [
    "# Trying Different Images and Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69f3a69",
   "metadata": {},
   "source": [
    "Let’s switch things up and use a different pose image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3ea30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pose = Image.open(\"man-pose.jpg\")\n",
    "\n",
    "pose = pose_model(img_pose)\n",
    "\n",
    "grid_img([img_pose, pose], 1, 2, scale=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07115b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
    "  img = img.resize((w,h), Image.ANTIALIAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae856c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7155bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 999\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
    "prompt = \"professional photo of a young asian man in the office, formal fashion, smile, waring hat, sharp focus, insanely detailed, photorealistic, side light\"\n",
    "neg_prompt = \"ugly, tiling, closed eyes, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, extra limbs, disfigured, deformed, body out of frame, bad anatomy, watermark, signature, cut off, low contrast, underexposed, overexposed, bad art, beginner, amateur, distorted face\"\n",
    "\n",
    "imgs = sd_controlpose(\n",
    "    prompt,\n",
    "    pose,\n",
    "    negative_prompt=neg_prompt,\n",
    "    num_images_per_prompt=4,\n",
    "    generator=generator,\n",
    "    num_inference_steps=20,\n",
    ")\n",
    "grid_img(imgs.images, 1, 4, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd55830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
    "  img = img.resize((w,h), Image.ANTIALIAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a8d774",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f55b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "prompt = [\"oil painting walter white wearing a suit and black hat and sunglasses, face portrait, in the desert, realistic, vivid\",\n",
    "          \"oil painting walter white wearing a jedi brown coat, face portrait, wearing a hood, holding a cup of coffee, in another planet, realistic, vivid\",\n",
    "          \"professional photo of walter white wearing a space suit, face portrait, in mars, realistic, vivid\",\n",
    "          \"professional photo of walter white in the kitchen, face portrait, realistic, vivid\"]\n",
    "\n",
    "neg_prompt = [\"helmet, ugly, tiling, closed eyes, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, extra limbs, disfigured, deformed, body out of frame, bad anatomy, watermark, signature, cut off, low contrast, underexposed, overexposed, bad art, beginner, amateur, distorted face\"] * len(prompt)\n",
    "num_imgs = 1\n",
    "\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
    "imgs = sd_controlpose(\n",
    "    prompt,\n",
    "    pose,\n",
    "    negative_prompt=neg_prompt,\n",
    "    generator=generator,\n",
    "    num_inference_steps=20,\n",
    ")\n",
    "grid_img(imgs.images, 1, len(prompt), 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7323a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
    "  img = img.resize((w,h), Image.ANTIALIAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7311ff76",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79827991",
   "metadata": {},
   "source": [
    "# Improve The Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceabc85",
   "metadata": {},
   "source": [
    "For enhanced results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3d65a1",
   "metadata": {},
   "source": [
    "# Exercise ControlNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161e1eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title #### Student Identity\n",
    "student_id = \"your student id\" # @param {type:\"string\"}\n",
    "name = \"your name\" # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0040f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intalling Libs\n",
    "%pip install diffusers==0.14\n",
    "%pip install -q accelerate transformers xformers\n",
    "%pip install -q controlnet_aux\n",
    "%pip install rggrader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2a8680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title #### 00. Generating Images Using Poses\n",
    "from rggrader import submit_image\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
    "import torch\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from controlnet_aux import OpenposeDetector\n",
    "from diffusers import DEISMultistepScheduler\n",
    "\n",
    "# TODO:\n",
    "# 1. Use the 'lllyasviel/ControlNet' model to extract the pose from the reference image. This model will allow us to understand the pose that is present in the image.\n",
    "# 2. Use the ControlNet models 'thibaud/controlnet-sd21-openpose-diffusers' and 'stabilityai/stable-diffusion-2-1-base' to generate the desired image. These models will take the pose extracted from the previous step and use it to generate a new image.\n",
    "# 3. The image generation will be based on the prompt that you input. Make sure your prompt is clear and describes the image you want to generate accurately.\n",
    "# 4. Once the image is generated, save it in the 'results' folder. This will ensure that you can easily locate and review the image later.\n",
    "# 5. Finally, select one of the generated images to upload. This image will be the final output of your exercise.\n",
    "\n",
    "# NOTE: Remember, the quality of the generated image will greatly depend on the accuracy of the pose extracted from the reference image and the clarity of your prompt.\n",
    "\n",
    "# Loading model and create output dir\n",
    "pose_model = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\n",
    "\n",
    "# Put your code here:\n",
    "imgs = None\n",
    "\n",
    "# ---- End of your code ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed0e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the results\n",
    "!mkdir results\n",
    "for i, img in enumerate(imgs.images):\n",
    "  img.save('results/result_{}.png'.format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccd87a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit Method\n",
    "assignment_id = \"00_controlnet\"\n",
    "question_id = \"00_generating_images_using_poses\"\n",
    "submit_image(student_id, question_id, 'your_image.png') # change 'your_image.png' to the name of the image you want to upload (eg. results/result_3.png)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
