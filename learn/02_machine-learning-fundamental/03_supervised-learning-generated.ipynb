{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61e9a515",
   "metadata": {},
   "source": [
    "source: [https://ai-bootcamp.ruangguru.com/learn/02_machine-learning-fundamental/03_supervised-learning.html](https://ai-bootcamp.ruangguru.com/learn/02_machine-learning-fundamental/03_supervised-learning.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454b0d78",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43622bde",
   "metadata": {},
   "source": [
    "What is Supervised Learning? Maybe that’s one of your big questions when you get into this topic. But, before digging deeper into Supervised Learning, it’s a good idea to first do a quick recap about Machine Learning and what it has to do with Supervised Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bbaa6a",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93adc35b",
   "metadata": {},
   "source": [
    "Machine Learning (ML) is a subset of Artificial Intelligence (AI) that allows software applications to become more accurate in predicting outcomes without being explicitly programmed. In simple terms, it provides machines the ability to automatically learn and improve from experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def15aeb",
   "metadata": {},
   "source": [
    "It’s like teaching a small child how to walk. First, the kid observes how others do it, then they try to do it themselves and keep on trying until they walk perfectly. This process involves a lot of falling and getting up. Similarly, in machine learning, models are given data, with which they make predictions. Based on the outcome of those predictions—right or wrong, the models adjust their behaviours, learning and improving over time to make accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3992b16a",
   "metadata": {},
   "source": [
    "According to Arthur Samuel (American pioneer in AI and gaming), Machine Learning is a field of study that gives computers the ability to learn without being explicitly programmed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f1adcc",
   "metadata": {},
   "source": [
    "# Definition and types of Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddb6296",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "743b1942",
   "metadata": {},
   "source": [
    "![Image](https://storage.googleapis.com/rg-ai-bootcamp/machine-learning/machine-learning-algorithm-min.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521e9126",
   "metadata": {},
   "source": [
    "There are various machine learning algorithms:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8c323e",
   "metadata": {},
   "source": [
    "Supervised Learning - In supervised learning, we provide the machine with labelled input data and a correct output. In other words, we guide the machine towards the right output. It’s kind of like a student learning under the guidance of a teacher. The ‘teacher’, in this case, is the label that tells the model about the data so it can learn from it. Once the model is trained with this labelled data, it can start to make predictions or decisions when new, unlabeled data is fed to it. A common example of supervised learning is classifying emails into “spam” or “not spam”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99f8b66",
   "metadata": {},
   "source": [
    "Unsupervised Learning - Unlike supervised learning, we do not have the comfort of a ‘teacher’ or labelled data here. In unsupervised learning, we only provide input to the machine learning model but no corresponding output. That is, the model is not told the right answer. The idea is for the model to explore the data and find some structure within. A classic example of unsupervised learning is customer segmentation, where you want to group your customers into different segments based on purchasing behaviour, but you don’t have any pre-existing labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48299170",
   "metadata": {},
   "source": [
    "Recommender Systems - Imagine walking into a bookstore and having an expert assistant who knows your reading habits guide you to books that would perfectly suit your taste – wouldn’t it be a delight? This is what recommender systems strive to do in virtual environments. They are intelligent algorithms that create a personalized list of suggestions based on data about each user’s past behaviour or user-item interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a604e4",
   "metadata": {},
   "source": [
    "Reinforcement Learning - This is a bit like learning to ride a bike. You don’t know how to do it at first, but with trial and error, you learn that pedalling keeps you balanced and turning the handlebars allows you to change direction. That’s essentially how reinforcement learning works - the model learns by interacting with its environment and receiving rewards or penalties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08526a9a",
   "metadata": {},
   "source": [
    "# What is Supervised Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd233f6",
   "metadata": {},
   "source": [
    "Now, let’s dig deeper into supervised learning, as it’s the most commonly used type of machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb734dea",
   "metadata": {},
   "source": [
    "In supervised learning, we train the model using ‘labeled’ data. That means our data set includes both the input ($x$) data and its corresponding correct output ($y$). This pair of input-output is also known as a ‘feature’ and a ‘label’. An easy way to think about this is with a recipe: the ‘feature’ is a list of ingredients (our input), and the ‘label’ is the dish that results from those ingredients (our correct output)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a53b6d",
   "metadata": {},
   "source": [
    "The algorithm analyses this input-output pair, maps the function that transforms input to output, and tries to gain understanding of such relationship so that it can apply it to unlabeled, new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28da9e0",
   "metadata": {},
   "source": [
    "There are two main types of problems that Supervised Learning tackles: Regression and Classification. 1. Regression: predict a number from many possible numbers (like predicting the price of a house based on its features like size, location etc.). 2. Classification: predicts only a small number of possible outputs or categories (like determining whether an email is spam or not spam, or whether a tumor is malignant or benign, or the picture is of a cat or a dog etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1562bd",
   "metadata": {},
   "source": [
    "Overall, supervised learning offers an efficient way to use known data to make meaningful predictions about new, future data. By analyzing the relationships within labeled training data, supervised learning models can then apply what they’ve learned to unlabeled, real-world data and make informed decisions or predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bb779d",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fa1daa",
   "metadata": {},
   "source": [
    "Let’s dive into one of the most basic and fundamental algorithms in the realm of Supervised Learning: Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be65e218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_train = np.array([6.50, 7.60, 12.00, 7.20, 9.75, 10.75, 9.00, 11.50, 8.60, 13.25])\n",
    "y_train = np.array([77.2, 99.8, 120.5, 89.5, 113.5, 124.2, 106.0, 133.0, 101.5, 148.2])\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(x_train, y_train, marker='x', c='b')\n",
    "# Set the title\n",
    "plt.title(\"Housing Prices\")\n",
    "# Set the y-axis label\n",
    "plt.ylabel('Price (in 10.000s of dollars)')\n",
    "# Set the x-axis label\n",
    "plt.xlabel('Size (in 100s sqft)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dfeff0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c93bb97",
   "metadata": {},
   "source": [
    "# Introduction to Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31315ba9",
   "metadata": {},
   "source": [
    "Imagine you’re a real estate agent, and you’re helping customers sell their houses at a reasonable market price. To do this, you need a way to predict a house’s market price based on its characteristics, such as the number of rooms, the year it was built, the neighborhood it’s in, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f70a229",
   "metadata": {},
   "source": [
    "This is where Linear Regression comes in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80389342",
   "metadata": {},
   "source": [
    "Linear Regression is like drawing a straight line through a cloud of data points. The line represents the relationship between the independent variables (the house characteristics, or ‘features’) and the dependent variable (the house price, or ‘target variable’)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1265f29d",
   "metadata": {},
   "source": [
    "But you might be wondering, “Why is it called ‘Linear’ Regression?” Well, it’s because this approach assumes that the relationship between the independent and dependent variables is linear. This simply means that if you were to plot this relationship on a graph, you could draw a straight line, or ‘linear’ line, to represent this relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051c7285",
   "metadata": {},
   "source": [
    "In simple terms, we’re trying to draw a line of best fit through our data points that minimizes the distance between the line and all the data points. Let’s take a look below, this is how the linear regression process works:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22973633",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a956fa65",
   "metadata": {},
   "source": [
    "![Image](https://storage.googleapis.com/rg-ai-bootcamp/machine-learning/linear-regression-min.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c294817",
   "metadata": {},
   "source": [
    "Supervised Learning includes both the input features and also the output targets, The output targets are the right answers to the model we’ll learn from. To train the model, you feed the training set, both the input features and the output targets to your learning algorithm. Then your supervised learning algorithm will produce some function ($f$). The job with $f$ is to take a new input $x$ and output and estimate or a prediction ($\\hat{y}$), In machine learning, the convention is that $\\hat{y}$ is the estimate or the prediction for $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43935359",
   "metadata": {},
   "source": [
    "The model for Linear Regression is a linear function of the input features. It’s represented as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404f3cf4",
   "metadata": {},
   "source": [
    "\n",
    "\\[ f(x_{(i)}) = wx_{(i)} + b\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1b313e",
   "metadata": {},
   "source": [
    "| General Notation | Description |\n",
    "| --- | --- |\n",
    "| \\(x\\) | Training Example feature values |\n",
    "| \\(x_{(i)}\\) | \\(i_{th}\\)Training Example |\n",
    "| \\(\\hat{y}\\) | Training Example targets |\n",
    "| m | Number of training examples |\n",
    "| \\(w\\) | parameter: weight |\n",
    "| \\(b\\) | parameter: bias |\n",
    "| \\(f\\) | The model for Linear Regression |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065e6ee",
   "metadata": {},
   "source": [
    "If you pay attention, in a linear function there are $w$ and $b$. The $w$ and $b$ are called the parameters of the model. In machine learning parameters of the model are the variables you can adjust during training in order to improve the model. Sometimes you also hear the parameters $w$ and $b$ referred to as weights and bias. Now let’s take a look at what these parameters $w$ and $b$ do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683101ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(0, 5)\n",
    "y = np.arange(0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea00f18",
   "metadata": {},
   "source": [
    "# Case 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094be460",
   "metadata": {},
   "source": [
    "If $w$ = 0 and $b$ = 1.5, then $f$ looks like a horizontal line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c766724f",
   "metadata": {},
   "source": [
    "In this case the function $f (x)=0 * x + 1.5 $ so $f$ is always constant. It always predicts 1.5 for the estimated $y$ value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370252bc",
   "metadata": {},
   "source": [
    "$\\hat{y}$ is always equal to $b$ and here $b$ is also called y-intercept because it intersects the vertical axis or y-axis on this graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b80ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 0\n",
    "b = 1.5\n",
    "\n",
    "def compute(x, w, b):\n",
    "    return w * x + b\n",
    "\n",
    "tmp_fx = compute(x, w, b,)\n",
    "\n",
    "tmp_x = 2.5\n",
    "y = w * tmp_x + b\n",
    "\n",
    "plt.plot(x, tmp_fx, c='r')\n",
    "plt.scatter(tmp_x, y, marker='o', c='b')\n",
    "plt.xlim(0, 3)\n",
    "plt.ylim(0, 3)\n",
    "plt.title(\"w=0, b=1.5 -> y = 0x + 1.5\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a04ae8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca677216",
   "metadata": {},
   "source": [
    "# Case 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a78cc4",
   "metadata": {},
   "source": [
    "If $w$ = 0.5 and $b$ = 0, then $f(x)=0.5 * x $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27236fa",
   "metadata": {},
   "source": [
    "If $x$ = 0 then the prediction is also 0, and if $x$ = 2 then the prediction is 0.5 * 2 which is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8d7a40",
   "metadata": {},
   "source": [
    "You get a line like this and notice that the slope is 0.5 / 1. The value of $w$ gives you the slope of the line , which is 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e59c835",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 0.5\n",
    "b = 0\n",
    "\n",
    "def compute(x, w, b):\n",
    "    return w * x + b\n",
    "\n",
    "tmp_fx = compute(x, w, b,)\n",
    "\n",
    "tmp_x = 2\n",
    "y = w * tmp_x + b\n",
    "\n",
    "plt.plot(x, tmp_fx, c='r')\n",
    "plt.scatter(tmp_x, y, marker='o', c='b')\n",
    "plt.xlim(0, 3)\n",
    "plt.ylim(0, 3)\n",
    "plt.title(\"w=0.5, b=0 -> y = 0.5x + 0\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b18aca8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4db010a4",
   "metadata": {},
   "source": [
    "# Case 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e03009e",
   "metadata": {},
   "source": [
    "If $w$ = 0.5 and $b$ = 1, then $f(x)=0.5 * x + 1$. and if $x$ = 0, then $f(x)= b$, which is 1. So the line intersects the vertical axis at $b$, the intersection $y$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9e06ca",
   "metadata": {},
   "source": [
    "Also if $x$ = 2, then $f(x)= 2$, so the line looks like this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c282ffea",
   "metadata": {},
   "source": [
    "Again, this slope is 0.5 / 1 so the value of $w$ results in a slope of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0015eb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 0.5\n",
    "b = 1\n",
    "\n",
    "def compute(x, w, b):\n",
    "    return w * x + b\n",
    "\n",
    "tmp_fx = compute(x, w, b,)\n",
    "\n",
    "tmp_x = 2\n",
    "y = w * tmp_x + b\n",
    "\n",
    "plt.plot(x, tmp_fx, c='r')\n",
    "plt.scatter(tmp_x, y, marker='o', c='b')\n",
    "plt.xlim(0, 3)\n",
    "plt.ylim(0, 3)\n",
    "plt.title(\"w=0.5, b=1 -> y = 0.5x + 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2893440f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d649bd1",
   "metadata": {},
   "source": [
    "Let’s consider a simple case study by Predicting a house’s price based on its size. The train set can be represented as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03963066",
   "metadata": {},
   "source": [
    "| House Size (100 sq.ft) (\\(x\\)) | Price (10.000s USD) (\\(y\\)) |\n",
    "| --- | --- |\n",
    "| 6.5 | 77.2 |\n",
    "| 7.6 | 99.8 |\n",
    "| 12.0 | 120.5 |\n",
    "| 7.20 | 89.5 |\n",
    "| 9.75 | 113.5 |\n",
    "| 10.75 | 124.2 |\n",
    "| 9.0 | 106.0 |\n",
    "| 11.5 | 133.0 |\n",
    "| 8.6 | 101.5 |\n",
    "| 13.25 | 148.2 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e18245a",
   "metadata": {},
   "source": [
    "Explanation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7334f4",
   "metadata": {},
   "source": [
    "In this example, we have house sizes as our independent variable (input) and corresponding house price as the dependent variable (output we want to predict). Assuming a linear relationship between size and price, we can use a linear regression model to predict the price based on the size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a761198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_train = np.array([6.50, 7.60, 12.00, 7.20, 9.75, 10.75, 9.00, 11.50, 8.60, 13.25])\n",
    "y_train = np.array([77.2, 99.8, 120.5, 89.5, 113.5, 124.2, 106.0, 133.0, 101.5, 148.2])\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(x_train, y_train, marker='x', c='b')\n",
    "# Set the title\n",
    "plt.title(\"Housing Prices\")\n",
    "# Set the y-axis label\n",
    "plt.ylabel('Price (in 10.000s of dollars)')\n",
    "# Set the x-axis label\n",
    "plt.xlabel('Size (in 100s sqft)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae7927e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d666d2c2",
   "metadata": {},
   "source": [
    "Now, the question is: How to predict the price of 9.5 sqft house? We can use the Linear Regression model to predict this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d054c9",
   "metadata": {},
   "source": [
    "With the amount of data we have, it would be difficult if we did 1 by 1 calculations manually. So, you can compute the function output in a for loop as shown in the compute_output function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9cf5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_output(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the prediction of a linear model\n",
    "    Args:\n",
    "      x    : Data, m examples \n",
    "      w,b  : model parameters  \n",
    "    Returns\n",
    "      f_wb : model prediction\n",
    "    \"\"\"\n",
    "    m = len(x_train)\n",
    "    f_wb = w * x + b\n",
    "\n",
    "    return f_wb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a4dc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please replace the values of w and b with numbers that you think are appropriate\n",
    "w = 9.8\n",
    "b = 15\n",
    "\n",
    "tmp_f_wb = compute_output(x_train, w, b,)\n",
    "\n",
    "# predict the price of 9.5 sqft house\n",
    "tmp_x = 9.5\n",
    "y_hat = w * tmp_x + b\n",
    "\n",
    "# Plot the price of 9.5 sqft house\n",
    "plt.scatter(tmp_x, y_hat, marker='o', c='black',label='9.5 sqft house')\n",
    "plt.text(11.5, 80, f\"Price = {y_hat:.2f}\", fontsize=12)\n",
    "\n",
    "# Plot our model prediction\n",
    "plt.plot(x_train, tmp_f_wb, c='r',label='Our Prediction')\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(x_train, y_train, marker='x', c='b',label='Actual Values')\n",
    "\n",
    "# Set the title\n",
    "plt.title(\"Housing Prices\")\n",
    "# Set the y-axis label\n",
    "plt.ylabel('Price (in 10.000s of dollars)')\n",
    "# Set the x-axis label\n",
    "plt.xlabel('Size (in 100s sqft)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a83d01",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94a0855e",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26e9353",
   "metadata": {},
   "source": [
    "Now, how do we know if the line is a good fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d920aa9f",
   "metadata": {},
   "source": [
    "We can calculate the error between the predicted value and the actual value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ced5d4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab371066",
   "metadata": {},
   "source": [
    "![Image](https://storage.googleapis.com/rg-ai-bootcamp/machine-learning/error-loss-min.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37609d29",
   "metadata": {},
   "source": [
    "For example, if the actual value is $y$, and the predicted value is $\\hat{y}$, then the error is $y - \\hat{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e58be6",
   "metadata": {},
   "source": [
    "We can calculate the error for each data point, and then sum them up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3adb30",
   "metadata": {},
   "source": [
    "The error function is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fdd7c7",
   "metadata": {},
   "source": [
    "\n",
    "\\[\n",
    "E = \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdce0522",
   "metadata": {},
   "source": [
    "where $y$ is the actual value, and $\\hat{y}$ is the predicted value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de578eb",
   "metadata": {},
   "source": [
    "This is called the sum of squared errors. (SSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dff31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "x_data = np.random.rand(100) * 10\n",
    "noise = np.random.normal(0, 2, x_data.shape)\n",
    "y_data = 3*x_data + 8 + noise\n",
    "\n",
    "# Define the update function\n",
    "def update(a, b):\n",
    "    y = a*x + b\n",
    "    plt.plot(x, y, color='red')\n",
    "    plt.scatter(x_data, y_data, s=1)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "\n",
    "    plt.title(f\"Equation y = {a}x + {b}\")\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = np.sum((y_data - y)**2)\n",
    "\n",
    "    # Draw the loss\n",
    "    plt.text(0, 20, f\"Loss = {loss:.2f}\", fontsize=12)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Create scattered dot around y = 3x + 8\n",
    "x = np.random.rand(100) * 10\n",
    "noise = np.random.normal(0, 2, x.shape)\n",
    "y = 3*x + 8 + noise\n",
    "\n",
    "# Define the slider widgets\n",
    "a_slider = widgets.FloatSlider(min=0, max=10, step=0.1, value=0, description='a:')\n",
    "b_slider = widgets.FloatSlider(min=0, max=10, step=0.1, value=0, description='b:')\n",
    "\n",
    "# Display the widgets and plot\n",
    "widgets.interactive(update, a=a_slider, b=b_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1841055c",
   "metadata": {},
   "source": [
    "So, the goal of linear regression is to find the best a and b that minimize the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01637707",
   "metadata": {},
   "source": [
    "In other words, we want to find the best a and b that make the red line as close as possible to the scattered dots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127826fb",
   "metadata": {},
   "source": [
    "# Other Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ccee9b",
   "metadata": {},
   "source": [
    "There are other loss functions, for example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba376370",
   "metadata": {},
   "source": [
    "# Mean Square Error (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf2a99b",
   "metadata": {},
   "source": [
    "\n",
    "\\[\n",
    "E = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823b50d7",
   "metadata": {},
   "source": [
    "# Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c21f29",
   "metadata": {},
   "source": [
    "\n",
    "\\[\n",
    "E = \\sum_{i=1}^{n} |y_i - \\hat{y_i}|\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3e256f",
   "metadata": {},
   "source": [
    "# Root Mean Squared Error (RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2634a42a",
   "metadata": {},
   "source": [
    "\n",
    "\\[\n",
    "E = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b351e6f",
   "metadata": {},
   "source": [
    "RMSE is in the same units as the target variable, which can make it more interpretable in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8977cb",
   "metadata": {},
   "source": [
    "The loss function, also called the cost function, tells us how well the model performs so we can try to make it better. The cost function itself measures the difference (error/loss rate) between the model prediction and the actual value for $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe51339",
   "metadata": {},
   "source": [
    "Let’s break down the formula for SSE to help you understand:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7823c584",
   "metadata": {},
   "source": [
    "The cost function takes the prediction y hat and compares it to the target $y$ by taking $\\hat{y}$ minus $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eec0f3",
   "metadata": {},
   "source": [
    "\n",
    "\\[ (f(x^{(i)}) - y^{(i)})^2 \\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd09eda",
   "metadata": {},
   "source": [
    "Then, the formula is the sum of the absolute value of the diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75213fb1",
   "metadata": {},
   "source": [
    "\n",
    "\\[J(w,b) = \\sum\\limits_{i = 0}^{m-1} (f(x^{(i)}) - y^{(i)})^2\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8a990e",
   "metadata": {},
   "source": [
    "But that equation is not fair because it doesn’t take into account the number of examples we have. So we divide it by the number of examples, $m$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc82b09f",
   "metadata": {},
   "source": [
    "\n",
    "\\[J(w,b) = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f(x^{(i)}) - y^{(i)})^2\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b7dd21",
   "metadata": {},
   "source": [
    "where \n",
    "\\[f(x^{(i)}) = wx^{(i)} + b\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3353ed25",
   "metadata": {},
   "source": [
    "- \\(f(x^{(i)})\\)is our prediction for example\\(i\\)using parameters\\(w,b\\).\n",
    "- \\((f(x^{(i)}) -y^{(i)})^2\\)is the squared difference between the target value and the prediction.\n",
    "- These differences are summed over all the\\(m\\)examples and divided bymto produce the cost,\\(J(w,b)\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3e49dc",
   "metadata": {},
   "source": [
    "Look at the code below, where it will calculate the cost by looping each example. In each loop:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6513811a",
   "metadata": {},
   "source": [
    "- \\(fx\\), a prediction is calculated\n",
    "- the difference between the target and the prediction is calculated and squared.\n",
    "- this is added to the total cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc432e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the cost function for linear regression.\n",
    "\n",
    "    Args:\n",
    "      x : Data, m examples\n",
    "      y : target values\n",
    "      w,b : model parameters\n",
    "\n",
    "    Returns\n",
    "        total_cost (float): The cost of using w,b as the parameters for linear regression\n",
    "               to fit the data points in x and y\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m = len(x_train)\n",
    "\n",
    "    cost_sum = 0\n",
    "    for i in range(m):\n",
    "        fx = w * x[i] + b\n",
    "        cost = (fx - y[i]) ** 2\n",
    "        cost_sum = cost_sum + cost\n",
    "    total_cost = (1 / m) * cost_sum\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aff2dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([6.50, 7.60, 12.00, 7.20, 9.75, 10.75, 9.00, 11.50, 8.60, 13.25])\n",
    "y_train = np.array([77.2, 99.8, 120.5, 89.5, 113.5, 124.2, 106.0, 133.0, 101.5, 148.2])\n",
    "w = 10\n",
    "b = 15\n",
    "\n",
    "errors = compute_cost(x_train, y_train, w, b)\n",
    "errors\n",
    "# print(f\"Number of errors: {errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92401f7e",
   "metadata": {},
   "source": [
    "Based on the data owned and the value of $w$ and $b$, when viewed from the results of the compute_cost function, the total mistakes we got were 31.641."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1036f1e4",
   "metadata": {},
   "source": [
    "Then, is the best amount? Remember that the purpose of linear regression is to minimize total errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78bf421",
   "metadata": {},
   "source": [
    "Then how do you do it? In other words we want to get the best $w$ and $b$ values that make the lines as close as possible to the spread points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1c8208",
   "metadata": {},
   "source": [
    "One way is you can use the Scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e25382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "answer = ()\n",
    "minimum_cost = 100000\n",
    "\n",
    "for w in np.arange(8.0, 12.0, 0.001):\n",
    "    for b in np.arange(13.0, 15.0, 0.001):\n",
    "        cost = compute_cost(x_train, y_train, w, b)\n",
    "        if cost < minimum_cost:\n",
    "            minimum_cost = cost\n",
    "            answer = (w, b)\n",
    "\n",
    "print(f\"Minimum cost: {minimum_cost}\")\n",
    "print(f\"w: {answer[0]} b: {answer[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c402ae",
   "metadata": {},
   "source": [
    "# Scikit-learn for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13530944",
   "metadata": {},
   "source": [
    "The sklearn (or Scikit-learn) library in Python is a powerful, open-source library that provides simple and efficient tools for data analysis and modeling, including machine learning. It has numerous features, ranging from simple regression models to complex machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6077eab0",
   "metadata": {},
   "source": [
    "For our Linear Regression model, sklearn provides the LinearRegression class within the sklearn.linear_model module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea99c9c",
   "metadata": {},
   "source": [
    "Here’s how you can do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07654d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Reshape the input array\n",
    "x_train = np.array([6.50, 7.60, 12.00, 7.20, 9.75, 10.75, 9.00, 11.50, 8.60, 13.25]).reshape((-1, 1))\n",
    "y_train = np.array([77.2, 99.8, 120.5, 89.5, 113.5, 124.2, 106.0, 133.0, 101.5, 148.2])\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "# Use this score function to in model.fit\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Generates and displays coefficients and intercepts\n",
    "coefficient = model.coef_ # w\n",
    "intercept = model.intercept_ # b\n",
    "print(f\"Coefficients: {coefficient}\")\n",
    "print(f\"Intercept: {intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121554f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Coefficients: [9.09553728]\n",
    "Intercept: 23.886409100809274"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79df63ec",
   "metadata": {},
   "source": [
    "From the results of the code above, the $w$ (coefficient) value is 9.09553728 and $b$ (intercept) is 23.886409100809274. That way we can produce the minimum possible error/loss value. Now we look again at the data we have with the new $w$ and $b$ values and what error/loss we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769a1159",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([6.50, 7.60, 12.00, 7.20, 9.75, 10.75, 9.00, 11.50, 8.60, 13.25])\n",
    "y_train = np.array([77.2, 99.8, 120.5, 89.5, 113.5, 124.2, 106.0, 133.0, 101.5, 148.2])\n",
    "w = coefficient[0]\n",
    "b = intercept\n",
    "\n",
    "cost = compute_cost(x_train, y_train, w, b)\n",
    "tmp_f_wb = compute_output(x_train, w, b,)\n",
    "\n",
    "# predict the price of 9.5 sqft house\n",
    "tmp_x = 9.5\n",
    "y_hat = w * tmp_x + b\n",
    "\n",
    "# Plot the price of 9.5 sqft house\n",
    "plt.scatter(tmp_x, y_hat, marker='o', c='black',label='9.5 sqft house')\n",
    "plt.text(12, 75, f\"Price = {y_hat:.2f}\", fontsize=10)\n",
    "\n",
    "# Plot our model prediction\n",
    "plt.plot(x_train, tmp_f_wb, c='r',label='Our Prediction')\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(x_train, y_train, marker='x', c='b',label='Actual Values')\n",
    "plt.text(12, 80, f\"Cost = {cost:.2f}\", fontsize=10)\n",
    "\n",
    "# Set the title\n",
    "plt.title(\"Housing Prices\")\n",
    "# Set the y-axis label\n",
    "plt.ylabel('Price (in 10.000s of dollars)')\n",
    "# Set the x-axis label\n",
    "plt.xlabel('Size (in 100s sqft)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59069f9d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdf5dd19",
   "metadata": {},
   "source": [
    "Based on the results in the graph above, we can see a decrease in the error/loss that we got, from the previous amount of 31.641 to 27.94. In this way, the level of accuracy in predicting the price of the house we are looking for will increase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce82d759",
   "metadata": {},
   "source": [
    "You can also make predictions with this trained model. For example, if you want to predict the y value for x = 9.5, you can do it using the predict() method like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59cff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = np.array([9.5]).reshape((-1, 1))\n",
    "y_pred = model.predict(x_new)\n",
    "print(f\"Predicted value for x = 9.5: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7cab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Predicted value for x = 9.5: [110.29401321]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19daaa4d",
   "metadata": {},
   "source": [
    "But, how does the sklearn library calculate the best $w$ and $b$ values? The answer is using the Gradient Descent algorithm, which will be discussed in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771d1749",
   "metadata": {},
   "source": [
    "# Linear Regression with Multiple Input Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e9e573",
   "metadata": {},
   "source": [
    "# Multiple Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b871ff30",
   "metadata": {},
   "source": [
    "In real-world scenarios, an output variable or a result often depends on more than one factor or input variable. In machine learning, these factors are referred to as “features”. In the previous discussion, you have one feature $x$, the size of the house and you can predict $y$, the price of the house. But now, what if you don’t just use the size of the house as a feature that can be used to predict prices such as the number of bedrooms and bathrooms and the age of the house. Let’s look at the example below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324e5d4b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bbe2572",
   "metadata": {},
   "source": [
    "![Image](https://storage.googleapis.com/rg-ai-bootcamp/machine-learning/multiple-feature-min.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cb1556",
   "metadata": {},
   "source": [
    "| General Notation | Description |\n",
    "| --- | --- |\n",
    "| \\(x_1\\) | House Size |\n",
    "| \\(x_2\\) | Number of Bedrooms |\n",
    "| \\(x_3\\) | Number of Bathrooms |\n",
    "| \\(x_4\\) | House Age |\n",
    "| \\(y\\) | Price |\n",
    "| \\(x_{j}\\) | \\(j\\)Feature (red rectangle) |\n",
    "| \\(n\\) | Number of Feature |\n",
    "| \\(\\vec{x}^{(i)}\\) | Feature of\\(i^{th}\\)training example, also called a row vector (yellow rectangle) |\n",
    "| \\({\\vec{x}_{j}}^{(i)}\\) | Value of Feature\\(j\\)in\\(i^{th}\\)in the training example (blue rectangle) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee181c05",
   "metadata": {},
   "source": [
    "If you just used the size of a house to predict its price, it would be a simple linear regression problem. In the previous discussion, this is how we defined the model, with $x$ as a single feature:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf7e30b",
   "metadata": {},
   "source": [
    "\n",
    "\\[ f(x) = wx + b\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771832d8",
   "metadata": {},
   "source": [
    "But now with many features, we will define them differently. We still multiply $w$ with $x$, but since now we have multiples of $w.x$, we sum them up together. The model then look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b5adaa",
   "metadata": {},
   "source": [
    "\n",
    "\\[ f(\\vec{x}) = w_{1}x_{1} + w_{2}x_{2} + \\dots + w_{n}x_{n} + b\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2916b504",
   "metadata": {},
   "source": [
    "or in vector notation can be written like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2ff79f",
   "metadata": {},
   "source": [
    "\n",
    "\\[ f(\\vec{x}) = \\vec{w} \\cdot \\vec{x} + b\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c7e6e0",
   "metadata": {},
   "source": [
    "where $\\cdot$ is a vector dot product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e662b2e2",
   "metadata": {},
   "source": [
    "| General Notation | Description |\n",
    "| --- | --- |\n",
    "| \\(\\vec{w}\\) | [\\(w_{1}, w_{2}, \\dots , w_{n}\\)] |\n",
    "| \\(b\\) | Number of Bias Parameter |\n",
    "| \\(\\vec{x}\\) | [\\(x_{1}, x_{2}, \\dots , x_{n}\\)] |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadb468f",
   "metadata": {},
   "source": [
    "Here, $\\vec{w}$ are parameters of the model that it learns during the training phase. The $\\vec{x}$ are the features. Using multiple features usually gives a better prediction than using just one feature, as real-world scenarios are often influenced by more than just one factor. It’s worth noting, however, that adding more and more features can also lead to overfitting where the model learns the training data too well and performs poorly on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6fb42d",
   "metadata": {},
   "source": [
    "> Note: We’ll learn more aboutdot productin the next material on Matrix and Vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf9013f",
   "metadata": {},
   "source": [
    "Note: We’ll learn more about dot product in the next material on Matrix and Vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e8678",
   "metadata": {},
   "source": [
    "Let’s go back to our case, where now we have multiple features to predict house prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95410058",
   "metadata": {},
   "source": [
    "| House Size (100 sq.ft) (\\(x_1\\)) | Bedrooms (\\(x_2\\)) | Bathrooms (\\(x_3\\)) | Age (years) (\\(x_4\\)) | Price (10.000s USD) (\\(y\\)) |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 6.50 | 2 | 1 | 15 | 77.2 |\n",
    "| 7.60 | 3 | 2 | 5 | 99.8 |\n",
    "| 12.00 | 4 | 3 | 10 | 120.5 |\n",
    "| 7.20 | 2 | 1 | 7 | 89.5 |\n",
    "| 9.75 | 3 | 2 | 12 | 113.5 |\n",
    "| 10.75 | 3 | 2 | 3 | 124.2 |\n",
    "| 9.00 | 3 | 2 | 8 | 106.0 |\n",
    "| 11.50 | 4 | 3 | 10 | 133.0 |\n",
    "| 8.60 | 2 | 2 | 15 | 101.5 |\n",
    "| 13.25 | 4 | 3 | 1 | 148.2 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886705bf",
   "metadata": {},
   "source": [
    "In this example, we not only have the size of the house as the independent variable, but also the number of bedrooms and bathrooms as well as the age of the house itself and don’t forget the price of the house as the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86b6dd2",
   "metadata": {},
   "source": [
    "Let’s take what we learn and apply it to code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c12dd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([\n",
    "    [6.50, 2, 1, 15],\n",
    "    [7.60, 3, 2, 5],\n",
    "    [12.00, 4, 3, 10],\n",
    "    [7.20, 2, 1, 7],\n",
    "    [9.75, 3, 2, 12],\n",
    "    [10.75, 3, 2, 3],\n",
    "    [9.00, 3, 2, 8],\n",
    "    [11.50, 4, 3, 10],\n",
    "    [8.60, 2, 2, 15],\n",
    "    [13.25, 4, 3, 1]\n",
    "])\n",
    "\n",
    "y_train = np.array([77.2, 99.8, 120.5, 89.5, 113.5, 124.2, 106.0, 133.0, 101.5, 148.2])\n",
    "\n",
    "print(f\"X Shape: {X_train.shape}, X Type:{type(X_train)})\")\n",
    "print(X_train)\n",
    "print(f\"y Shape: {y_train.shape}, y Type:{type(y_train)})\")\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ff1fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X Shape: (10, 4), X Type:<class 'numpy.ndarray'>)\n",
    "[[ 6.5   2.    1.   15.  ]\n",
    " [ 7.6   3.    2.    5.  ]\n",
    " [12.    4.    3.   10.  ]\n",
    " [ 7.2   2.    1.    7.  ]\n",
    " [ 9.75  3.    2.   12.  ]\n",
    " [10.75  3.    2.    3.  ]\n",
    " [ 9.    3.    2.    8.  ]\n",
    " [11.5   4.    3.   10.  ]\n",
    " [ 8.6   2.    2.   15.  ]\n",
    " [13.25  4.    3.    1.  ]]\n",
    "y Shape: (10,), y Type:<class 'numpy.ndarray'>)\n",
    "[ 77.2  99.8 120.5  89.5 113.5 124.2 106.  133.  101.5 148.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9967ee3f",
   "metadata": {},
   "source": [
    "For parameter vector $w$ and $b$ can be written like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544e7606",
   "metadata": {},
   "source": [
    "- \\(w\\)is a vector with\\(n\\)elements.Each element contains the parameter associated with one feature.in our dataset, n is 4.notionally, we draw this as a column vector\n",
    "- Each element contains the parameter associated with one feature.\n",
    "- in our dataset, n is 4.\n",
    "- notionally, we draw this as a column vector\n",
    "- \\(b\\)is a scalar parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917b4307",
   "metadata": {},
   "source": [
    "- Each element contains the parameter associated with one feature.\n",
    "- in our dataset, n is 4.\n",
    "- notionally, we draw this as a column vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480fa452",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_init = 46.68178303822603\n",
    "w_init = np.array([ 7.05979876, -4.1870478, 8.30627675, -0.94230069])\n",
    "print(f\"w_init shape: {w_init.shape}, b_init type: {type(b_init)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af88dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init shape: (4,), b_init type: <class 'float'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599d14a1",
   "metadata": {},
   "source": [
    "Now, let’s look at a basic implementation using a for loop for computing the model’s prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73652185",
   "metadata": {},
   "source": [
    "\n",
    "\\[f(\\vec{x}) = \\sum\\limits_{j = 1}^{n} w_{j}x_{j} + b\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01ac755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_loop(x, w, b): \n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters    \n",
    "      b (scalar):  model parameter     \n",
    "      \n",
    "    Returns:\n",
    "      f (scalar):  prediction\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    f = 0\n",
    "    for i in range(n):\n",
    "        f_i = x[i] * w[i]  \n",
    "        f = f + f_i         \n",
    "    f = f + b                \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54826501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a row from our training data\n",
    "x_vec = X_train[0,:]\n",
    "print(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")\n",
    "\n",
    "# make a prediction\n",
    "f_x = predict_single_loop(x_vec, w_init, b_init)\n",
    "print(f\"f_wb shape {f_x.shape}, prediction: {f_x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2aa9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vec shape (4,), x_vec value: [ 6.5  2.   1.  15. ]\n",
    "4\n",
    "f_wb shape (), prediction: 78.36814577822602"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99170bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_for_multiple_variables(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the cost function for linear regression.\n",
    "\n",
    "    Args:\n",
    "      X : Data, m examples\n",
    "      y : target values\n",
    "      w,b : model parameters\n",
    "\n",
    "    Returns\n",
    "        total_cost (float): The cost of using w,b as the parameters for linear regression\n",
    "               to fit the data points in X and y\n",
    "    \"\"\"\n",
    "    m = len(y)  # number of training examples\n",
    "    \n",
    "    # hypothesis calculation - output prediction\n",
    "    fx = np.dot(X, w) + b\n",
    "    \n",
    "    # error calculation\n",
    "    errors = fx - y\n",
    "    \n",
    "    # calculate total cost\n",
    "    total_cost = (1 / m) * np.dot(errors.T, errors)\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97f4faa",
   "metadata": {},
   "source": [
    "Now, there is a very interesting trick called vectorization, which will make it easier to implement this algorithm and many other learning algorithms. And it will runs much faster too! Let’s go into the next discussion to see what vectorization is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95953cb0",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870b300f",
   "metadata": {},
   "source": [
    "Vectorization is a powerful ability within NumPy (and other similar libraries) to express operations as occurring on entire arrays, rather than their individual elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd713ce",
   "metadata": {},
   "source": [
    "When we loop over elements of an array and perform operations, this is called scalar computation. In Python, scalar computation is relatively slow compared to vectorized computation. This is where vectorization can be of great use in machine learning tasks which often require performing operations on large amounts of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f10ab3",
   "metadata": {},
   "source": [
    "Now, let’s look at how we can do this using vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9dba2f",
   "metadata": {},
   "source": [
    "\n",
    "\\[ f(\\vec{x}) = \\vec{w} \\cdot \\vec{x} + b\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9843d9b2",
   "metadata": {},
   "source": [
    "And now you can implement it with one line of code like below using dot() method from NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2cf7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b): \n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters   \n",
    "      b (scalar):             model parameter \n",
    "      \n",
    "    Returns:\n",
    "      f (scalar):  prediction\n",
    "    \"\"\"\n",
    "    f = np.dot(x, w) + b     \n",
    "    return f   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d37e19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a row from our training data\n",
    "x_vec = X_train[0,:]\n",
    "print(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")\n",
    "\n",
    "# make a prediction\n",
    "f_wb = predict(x_vec,w_init, b_init)\n",
    "print(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f22e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vec shape (4,), x_vec value: [ 6.5  2.   1.  15. ]\n",
    "f_wb shape (), prediction: 78.36814577822602"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9581ceb9",
   "metadata": {},
   "source": [
    "# How many features do we need?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ebfb76",
   "metadata": {},
   "source": [
    "In the previous discussion, we have discussed how to use multiple features to predict house prices. But, how many features do we need?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02189fdb",
   "metadata": {},
   "source": [
    "Should we add the following features to our model? - The number of floors - The size of the backyard - Is it near a school? - Is it near a mall? - Does it have a swimming pool?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad403476",
   "metadata": {},
   "source": [
    "More features are not always better. Adding more features can lead to overfitting, where the model learns the training data too well and performs poorly on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5d2dc5",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26339055",
   "metadata": {},
   "source": [
    "Feature choices can have a big impact on the performance of your learning algorithm. In fact, for many practical applications, selecting or including the right features is a critical step for an algorithm to function properly. In this topic, let’s see how we can select or engineer the most suitable features for our learning algorthm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76e1640",
   "metadata": {},
   "source": [
    "Feature Engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data. In other words, you’re creating new features from the existing ones, or even removing some that you deem unnecessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f2b9e4",
   "metadata": {},
   "source": [
    "Feature engineering can be considered as adding more useful information for your model to learn from. In the context of linear regression with multiple input variables, feature engineering can be crucial because the relationship between the response variable and predictors may not always be linear or may depend on multiple interacting variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1791400b",
   "metadata": {},
   "source": [
    "For a dataset describing different lands, let’s say we have three original features – Width, Depth, and Price:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f951636",
   "metadata": {},
   "source": [
    "| LandID | Width (m) | Depth (m) | Price ($) |\n",
    "| --- | --- | --- | --- |\n",
    "| 1 | 10 | 20 | 2000 |\n",
    "| 2 | 20 | 15 | 3000 |\n",
    "| 3 | 15 | 20 | 2500 |\n",
    "| 4 | 10 | 12 | 1200 |\n",
    "| 5 | 12 | 15 | 2250 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cadb263",
   "metadata": {},
   "source": [
    "The simple Machine learning model using Width and Depth to predict price could look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea5bf3d",
   "metadata": {},
   "source": [
    "\n",
    "\\[ f(\\vec{x}) = w_{1}x_{1} + w_{2}x_{2} + b\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c99450f",
   "metadata": {},
   "source": [
    "Now let’s see how feature engineering can enhance this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136b1f18",
   "metadata": {},
   "source": [
    "# Creating new features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e2a4e5",
   "metadata": {},
   "source": [
    "One simple feature engineering task on this dataset could be creating a new feature representing the total area. While the width and depth by themselves might not be the best indicators of price, land area (calculated as depth x width) could be a stronger feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5057f50",
   "metadata": {},
   "source": [
    "| LandID | Width (m) | Depth (m) | Area (m²) | Price ($) |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 1 | 10 | 20 | 200 | 2000 |\n",
    "| 2 | 20 | 15 | 300 | 3000 |\n",
    "| 3 | 15 | 20 | 300 | 2500 |\n",
    "| 4 | 10 | 12 | 120 | 1200 |\n",
    "| 5 | 12 | 15 | 180 | 2250 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ef1976",
   "metadata": {},
   "source": [
    "In certain situations, raw features as given in the dataset might not have a clear and direct relationship with the target variable. In the example of predicting land price, it’s possible that neither the depth nor the width of the land independently has a strong predictable effect on the price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e41cee8",
   "metadata": {},
   "source": [
    "However, when you combine them into a single feature (the total area, which is depth multiplied by width), you can create a new representation of your data that might have a better correlation or association with the target variable (price, in this case). It’s common that a property, such as area here, can have more correlation to price than either width or depth individually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612dd90e",
   "metadata": {},
   "source": [
    "In other words, even though the width and depth independently may not be good predictors, their product (which represents the total area) might prove to be a very good predictor. This can be understood intuitively too - larger lands (greater area) usually cost more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28850a18",
   "metadata": {},
   "source": [
    "This process of creating new and more effective features is a key part of feature engineering. Good feature engineering can often make the difference between a weak model and a highly accurate one. It’s an essential step in any machine learning project and often requires knowledge about the domain from which the data originates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6a6c4f",
   "metadata": {},
   "source": [
    "Now with the new feature, the Machine Learning model will look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81c0c89",
   "metadata": {},
   "source": [
    "\n",
    "\\[ f(\\vec{x}) = w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + b\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2204ba23",
   "metadata": {},
   "source": [
    "# Why Feature Engineering important in Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed91743",
   "metadata": {},
   "source": [
    "Linear regression aims to establish a linear relationship between the input variables (features) and the single output variable (target). Feature engineering aids in improving this relationship, leading to more accurate models. Good features capture important aspects of the data and provide information that helps the model make correct predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4c0f09",
   "metadata": {},
   "source": [
    "When it comes to linear regression, the importance of feature engineering arises from the fact that the model is quite simple. The model assumes a simple linear relationship between input and outputs. If the features can capture complex dependencies in simple forms, the model has a much easier time learning from them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03d2db7",
   "metadata": {},
   "source": [
    "It is also important to note that feature engineering often requires domain knowledge. In the land-price example, knowing that land price is often a function of total area is key to creating the “area” feature from the original “width” and “depth” features. As such, while Machine Learning provides many powerful tools, domain expertise is often key to applying these tools effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fc4a86",
   "metadata": {},
   "source": [
    "Here are some common methods used in feature engineering:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2aab2e",
   "metadata": {},
   "source": [
    "Feature scaling: This includes methods such as standardization and normalization. Feature scaling is used to standardize the range of features of data. This is important because features might have different units and may vary in range. Some algorithms, like gradient descent, converge faster when features are on a similar scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dc327d",
   "metadata": {},
   "source": [
    "Polynomial features: This is useful when the relationship between the independent and dependent variable is not linear. Polynomial features create interactions between features and also create features to the power of the exponent. For example, if we have a feature $x$, we can create a new feature that is $x^2$ or $x^3$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3fa963",
   "metadata": {},
   "source": [
    "# Feature Scalling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aca941",
   "metadata": {},
   "source": [
    "Feature Scalling is a technique that will enable gradient descent (the algorithm behind model.fit) to run much faster. Feature scaling is a step in data preprocessing that aims to standardize the range of features in the data. When the range of values in different features varies greatly, the ML algorithm might take longer to converge, or it may not effectively learn the correct weights for each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e703d23",
   "metadata": {},
   "source": [
    "Two common types of feature scaling are: - Mean Normalization - Standardization (Z-score Scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6c802b",
   "metadata": {},
   "source": [
    "> Notes: To find out what gradient descent is, it will be discussed in the next material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e5d645",
   "metadata": {},
   "source": [
    "Notes: To find out what gradient descent is, it will be discussed in the next material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccc23e5",
   "metadata": {},
   "source": [
    "# Mean Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a8750b",
   "metadata": {},
   "source": [
    "This method centers the data around 0. It scales the data between -1 and 1 by subtracting each observation by the average of observations and then dividing by the difference between the maximum and minimum value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fd1c95",
   "metadata": {},
   "source": [
    "\n",
    "\\[x_i := \\dfrac{x_i - \\mu_i}{max - min} \\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df03480",
   "metadata": {},
   "source": [
    "So, when do we use it? Mean Normalization is used when we want to center our variables around zero and vary between -1 and 1. Mean normalization can be useful in cases where we want to eliminate the effect of the units on an algorithm. It’s particularly good when our data has outliers, as mean normalization does not bound values to a specific range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd2227c",
   "metadata": {},
   "source": [
    "Below is a Python example to illustrate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30b541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Here is an array with some numbers\n",
    "x = np.array([\n",
    "    [650, 2, 1, 15],\n",
    "    [760, 3, 2, 5],\n",
    "    [1200, 4, 3, 10],\n",
    "    [720, 2, 1, 7],\n",
    "    [975, 3, 2, 12],\n",
    "    [1075, 3, 2, 3],\n",
    "    [900, 3, 2, 8],\n",
    "    [1150, 4, 3, 10],\n",
    "    [860, 2, 2, 15],\n",
    "    [1325, 4, 3, 1]\n",
    "], dtype=float)\n",
    "\n",
    "# Mean normalization\n",
    "x_mean_norm = (x - np.mean(x)) / (np.max(x) - np.min(x))\n",
    "print('Mean Normalized data:')\n",
    "print(x_mean_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e06476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean Normalized data:\n",
    "[[ 0.18518519 -0.48148148  0.51851852]\n",
    " [ 0.51851852 -0.14814815 -0.14814815]\n",
    " [-0.14814815  0.18518519 -0.48148148]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a361644",
   "metadata": {},
   "source": [
    "# Standardization (Z-score Scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef2e27d",
   "metadata": {},
   "source": [
    "This method transform the feature to have a mean of 0 and standard deviation of 1 by subtracting each value by the mean and then dividing by the standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f485f15",
   "metadata": {},
   "source": [
    "Why do we use standardization? If a feature in our dataset has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly, leading to sub-optimal performance. Standardization mitigates this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a171ef3c",
   "metadata": {},
   "source": [
    "To implement z-score normalization, adjust our input values as shown in this formula: \n",
    "\\[x^{(i)}_j = \\dfrac{x^{(i)}_j - \\mu_j}{\\sigma_j}\\]\n",
    " where $j$ selects a feature or a column in the $\\mathbf{X}$ matrix. $µ_j$ is the mean of all the values for feature (j) and $\\sigma_j$ is the standard deviation of feature (j). \n",
    "\\[\n",
    "\\begin{align}\n",
    "\\mu_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} x^{(i)}_j\\\\\n",
    "\\sigma^2_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} (x^{(i)}_j - \\mu_j)^2\n",
    "\\end{align}\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a32baa",
   "metadata": {},
   "source": [
    "Here’s an illustrative Python example using sklearn library’s StandardScaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acd4329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Here is an array with some random numbers\n",
    "x = np.array([\n",
    "    [650, 2, 1, 15],\n",
    "    [760, 3, 2, 5],\n",
    "    [1200, 4, 3, 10],\n",
    "    [720, 2, 1, 7],\n",
    "    [975, 3, 2, 12],\n",
    "    [1075, 3, 2, 3],\n",
    "    [900, 3, 2, 8],\n",
    "    [1150, 4, 3, 10],\n",
    "    [860, 2, 2, 15],\n",
    "    [1325, 4, 3, 1]\n",
    "], dtype=float)\n",
    "\n",
    "# Standardization\n",
    "std_scaler = StandardScaler()\n",
    "x_std = std_scaler.fit_transform(x)\n",
    "\n",
    "print('Standardized data:')\n",
    "print(x_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec40547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Standardized data:\n",
    "[[ 0.         -1.22474487  1.33630621]\n",
    " [ 1.22474487  0.         -0.26726124]\n",
    " [-1.22474487  1.22474487 -1.06904497]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d43c25",
   "metadata": {},
   "source": [
    "The output dimensions of standardization do not have a particular range, which means the data can be negative, zero, or positive. This kind of scaling is less affected by outliers or extreme values as it is based on the mean and standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc9c1cd",
   "metadata": {},
   "source": [
    "# Rule of Thumb Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727cd28d",
   "metadata": {},
   "source": [
    "| Feature Range | Status |\n",
    "| --- | --- |\n",
    "| 0 $ x_1 $ 3 | ok, no rescaling |\n",
    "| -2 $ x_2 $ 0.5 | ok, no rescaling |\n",
    "| -100 $ x_3 $ 100 | too large, rescaling |\n",
    "| -0.001 $ x_4 $ 0.001 | too small, rescaling |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fd838d",
   "metadata": {},
   "source": [
    "# Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de8a468",
   "metadata": {},
   "source": [
    "Now, we have learned that linear regression algorithm can find a linear line that best fits the data. But, what if the data is not linear? What if the data is curved?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ce38b0",
   "metadata": {},
   "source": [
    "Polynomial Regression is a form of regression analysis in which the relationship between the independent variable $x$ and the dependent variable $y$ is modeled as an nth degree polynomial. Polynomial regression fits a nonlinear relationship between the value of $x$ and the corresponding conditional mean of $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bbc005",
   "metadata": {},
   "source": [
    "For example, a simple linear regression equation represent like this: \n",
    "\\[ f(x) = wx + b\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2acbfef",
   "metadata": {},
   "source": [
    "However, in polynomial regression, you might have an equation that looks like this for a second degree (quadratic) polynomial:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c92789",
   "metadata": {},
   "source": [
    "\n",
    "\\[f(x)= w_0{x_0}^2 + b\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687a0719",
   "metadata": {},
   "source": [
    "This equation is still linear in the coefficients. The linearity in polynomial regression applies to the coefficients, and not the degree of the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0e6ae9",
   "metadata": {},
   "source": [
    "Polynomial regression can be very useful. When a linear regression model is not sufficient to capture the data relationships, it can be extended with polynomial features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af13f6de",
   "metadata": {},
   "source": [
    "Let’s consider a simple case study. Imagine we are running a small business selling ice cream. We notice that the sales of our ice cream seems to be heavily influenced by the temperature outside."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071a8d42",
   "metadata": {},
   "source": [
    "Let’s say we’ve collected the following data regarding outside temperature (in Celsius) and ice cream sales for a few days:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd4a5d",
   "metadata": {},
   "source": [
    "| Temperature (In Celsius) (\\(x\\)) | Ice Cream Sales (\\(y\\)) |\n",
    "| --- | --- |\n",
    "| 14 | 200 |\n",
    "| 16 | 225 |\n",
    "| 18 | 255 |\n",
    "| 20 | 290 |\n",
    "| 22 | 330 |\n",
    "| 24 | 370 |\n",
    "| 26 | 410 |\n",
    "| 28 | 440 |\n",
    "| 30 | 460 |\n",
    "| 32 | 470 |\n",
    "| 34 | 480 |\n",
    "| 36 | 485 |\n",
    "| 37 | 490 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b13e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = np.array([14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 37])\n",
    "y_train = np.array([200, 225, 255, 290, 330, 370, 410, 440, 460, 470, 480, 485, 490])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4530793",
   "metadata": {},
   "source": [
    "We’re going to visualize our data first to get a better understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7354a26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.xlabel('Temperature in Celsius')\n",
    "plt.ylabel('Ice Cream Sales')\n",
    "plt.title('Ice Cream Sales vs Temperature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af120a85",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bebd7181",
   "metadata": {},
   "source": [
    "Our data appears to show a non-linear trend (increases, then decreases). So, we decided to use quadratic Polynomial Regression to model this data. In this case we will use the sklearn library. sklearn itself provides the PolynomialFeatures class in the sklearn.preprocessing module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bbee43",
   "metadata": {},
   "source": [
    "The PolynomialFeatures class in the sklearn.preprocessing module is a feature transformation tool in Scikit-learn. It generates a new matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2940b3ce",
   "metadata": {},
   "source": [
    "If we have one feature (x), PolynomialFeatures would transform this based on the degree parameter. For a degree of 2, it would transform it to [1, x, x^2]. For a degree of 3, it would transform it to [1, x, x^2, x^3] and so on. The 1 is included as it acts as the intercept term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b055ddd8",
   "metadata": {},
   "source": [
    "Here’s an example of how to use PolynomialFeatures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8450c153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "\n",
    "# Given a simple 2D numpy array\n",
    "X = np.arange(6).reshape(3, 2)\n",
    "print(\"Original Data:\")\n",
    "print(X)\n",
    "\n",
    "# Create a PolynomialFeatures object and transform the data\n",
    "poly = PolynomialFeatures(2) # let's choose degree 2\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "print(\"Transformed Data:\")\n",
    "print(X_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369b6e50",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db9fdcd3",
   "metadata": {},
   "source": [
    "Now, we return to our case example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0ae737",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Transform our x input to 1D array for fitting\n",
    "x_train_fit = x_train.reshape(-1, 1)\n",
    "\n",
    "# Create Polynomial Features\n",
    "poly = PolynomialFeatures(degree=[2, 6])\n",
    "x_poly = poly.fit_transform(x_train_fit)\n",
    "\n",
    "# Fit the Polynomial Features to the Linear Regression\n",
    "poly_model = LinearRegression()\n",
    "poly_model.fit(x_poly, y_train)\n",
    "\n",
    "# Generate a sequence of temperatures for prediction \n",
    "num_samples = 100\n",
    "X_seq = np.linspace(x_train_fit.min(),x_train_fit.max(),num_samples).reshape(-1,1)\n",
    "\n",
    "# Use model to predict ice cream sales\n",
    "y_poly_pred = poly_model.predict(poly.fit_transform(X_seq))\n",
    "\n",
    "# Visualize the original data and the polynomial fit\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.plot(X_seq, y_poly_pred, color='r')\n",
    "plt.xlabel('Temperature in Celsius')\n",
    "plt.ylabel('Ice Cream Sales')\n",
    "plt.title('Ice Cream Sales vs Temperature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4460b098",
   "metadata": {},
   "source": [
    "Now we have a model that not only fits our training data but can predict the ice cream sales at any given temperature, which will help in managing our ice cream supply based on predicted sales."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
