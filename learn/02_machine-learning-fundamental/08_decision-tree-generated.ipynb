{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca3933c2",
   "metadata": {},
   "source": [
    "source: [https://ai-bootcamp.ruangguru.com/learn/02_machine-learning-fundamental/08_decision-tree.html](https://ai-bootcamp.ruangguru.com/learn/02_machine-learning-fundamental/08_decision-tree.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91531bc0",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813229be",
   "metadata": {},
   "source": [
    "Decision Tree is a one of a popular (and simple) machine learning model. It’s a supervised learning model that can be used for both classification and regression. The intuition behind the decision tree is simple, yet powerful. It’s a tree-like model that makes a decision based on the given features. The decision tree is a white-box model, which means that it’s easy to interpret the model’s decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2d38b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a sample dataset to illustrate GINI impurity\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a sample data set with 2 features and 2 classes and 100 samples\n",
    "# The GINI impurity should be = 0.5\n",
    "X = np.random.rand(100, 2)\n",
    "y = np.random.randint(0, 2, 100)\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "# Set gini number to the title\n",
    "plt.title('Gini = 0.5 with 2 classes')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c07779e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41db4ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a sample dataset to illustrate GINI impurity\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a sample data set with 2 features and 3 classes and 100 samples\n",
    "# The GINI impurity should be = 0.5\n",
    "\n",
    "X = np.random.rand(100, 2)\n",
    "y = np.random.randint(0, 3, 100)\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "# Set gini number to the title\n",
    "plt.title('Gini = 0.5 with 3 classes')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d317538d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46da483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a sample dataset to illustrate GINI impurity\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a sample data set with 2 features and 2 classes and 100 samples\n",
    "# The GINI impurity should be = 0\n",
    "\n",
    "X = np.random.rand(100, 2)\n",
    "y = np.zeros(100)\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "# Set gini number to the title\n",
    "plt.title('Gini = 0')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2790c84d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2d0d28f",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499b3e80",
   "metadata": {},
   "source": [
    "Go to this link and try to split the data manually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8d4c3d",
   "metadata": {},
   "source": [
    "Your goal: decide which feature the dataset should be split by to get better GINI value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f70021f",
   "metadata": {},
   "source": [
    "# Machine Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4575441",
   "metadata": {},
   "source": [
    "Now let the machine do it for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55801f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c00333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load assets/decision-tree-exercise.csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('assets/decision-tree-exercise.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cf5e1e",
   "metadata": {},
   "source": [
    "|  | Ticket Price | Holiday duration | Jakarta PM 2.5 | Temperature | Hectic in company? | Holiday to Bali |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| 0 | 1200000 | 10 | 198 | 35 | Y | True |\n",
    "| 1 | 641374 | 4 | 103 | 31 | Y | False |\n",
    "| 2 | 1381146 | 9 | 194 | 39 | Y | True |\n",
    "| 3 | 1478889 | 7 | 169 | 32 | N | False |\n",
    "| 4 | 897024 | 5 | 189 | 32 | N | False |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711439af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ece852",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ticket Price           int64\n",
    "Holiday duration       int64\n",
    "Jakarta PM 2.5         int64\n",
    "Temperature            int64\n",
    "Hectic in company?    object\n",
    "Holiday to Bali         bool\n",
    "dtype: object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0dc73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build decision tree model using sci-kit learn\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "columns = [\"Ticket Price\", \"Holiday duration\", \"Jakarta PM 2.5\", \"Temperature\", \"Hectic in company?\"]\n",
    "\n",
    "# convert Hectic in company? (y/n) to numeric\n",
    "df[\"Hectic in company?\"] = df[\"Hectic in company?\"].map({\"Y\": 1, \"N\": 0})\n",
    "\n",
    "# convert \"Holiday to Bali\" (TRUE/FALSE) to numeric\n",
    "df[\"Holiday to Bali\"] = df[\"Holiday to Bali\"].map({True: 1, False: 0})\n",
    "\n",
    "X = df[columns]\n",
    "y = df[\"Holiday to Bali\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfbc733",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491539d1",
   "metadata": {},
   "source": [
    "|  | Ticket Price | Holiday duration | Jakarta PM 2.5 | Temperature | Hectic in company? |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| 0 | 1200000 | 10 | 198 | 35 | 1 |\n",
    "| 1 | 641374 | 4 | 103 | 31 | 1 |\n",
    "| 2 | 1381146 | 9 | 194 | 39 | 1 |\n",
    "| 3 | 1478889 | 7 | 169 | 32 | 0 |\n",
    "| 4 | 897024 | 5 | 189 | 32 | 0 |\n",
    "| ... | ... | ... | ... | ... | ... |\n",
    "| 195 | 537683 | 10 | 124 | 37 | 1 |\n",
    "| 196 | 1194739 | 1 | 104 | 33 | 1 |\n",
    "| 197 | 1124824 | 5 | 161 | 32 | 1 |\n",
    "| 198 | 718404 | 5 | 177 | 37 | 1 |\n",
    "| 199 | 599056 | 5 | 185 | 39 | 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0abaaea",
   "metadata": {},
   "source": [
    "200 rows × 5 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ada45fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e20ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4525867c",
   "metadata": {},
   "outputs": [],
   "source": [
    "((150, 5), (50, 5), (150,), (50,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bae495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Draw decision tree\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "\n",
    "dot_data = export_graphviz(model, out_file=None, feature_names=columns, class_names=[\"No\", \"Yes\"], filled=True, rounded=True, special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9b3532",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3040a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fdf74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.96"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfd768e",
   "metadata": {},
   "source": [
    "# What is Gini?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def67c66",
   "metadata": {},
   "source": [
    "Gini is a measure of impurity. The lower the gini, the purer the node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaa9e72",
   "metadata": {},
   "source": [
    "\n",
    "\\[Gini = 1 - \\sum_{i=1}^{n} p_i^2\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8bc7c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "965d7255",
   "metadata": {},
   "source": [
    "![Image](https://storage.googleapis.com/lds-media/images/gini-impurity-diagram.width-1200.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4554d917",
   "metadata": {},
   "source": [
    "Source: Learndatasci.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8cd53e",
   "metadata": {},
   "source": [
    "Depends on how we split the data, we can get different purity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fa4597",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6658437f",
   "metadata": {},
   "source": [
    "![Image](https://ekamperi.github.io/images/decision_trees/pure_vs_impure_node.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca996305",
   "metadata": {},
   "source": [
    "Source: ekamperi.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25daa88",
   "metadata": {},
   "source": [
    "# Alternatives to Gini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d5c5ba",
   "metadata": {},
   "source": [
    "There are alternative metrics to using Gini impurity that can help us in building our Decision Tree:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e738d42",
   "metadata": {},
   "source": [
    "- Entropy: This is a measure of disorder or uncertainty. The entropy of a dataset is used for calculating the information gain, which is the reduction in entropy after a dataset is split on an attribute. Constructing a decision tree involves finding the attribute that returns the highest information gain (i.e., the most homogeneous branches).\n",
    "- Information Gain: This is the reduction in entropy or surprise that results from partitioning the data on an attribute. The decision tree will make the split where information gain is maximum, or equivalently, where entropy is minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198de885",
   "metadata": {},
   "source": [
    "Entropy: This is a measure of disorder or uncertainty. The entropy of a dataset is used for calculating the information gain, which is the reduction in entropy after a dataset is split on an attribute. Constructing a decision tree involves finding the attribute that returns the highest information gain (i.e., the most homogeneous branches)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f9c53d",
   "metadata": {},
   "source": [
    "Information Gain: This is the reduction in entropy or surprise that results from partitioning the data on an attribute. The decision tree will make the split where information gain is maximum, or equivalently, where entropy is minimum."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
