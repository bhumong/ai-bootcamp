{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c90c3965",
   "metadata": {},
   "source": [
    "# Covariance formula and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e4b36d",
   "metadata": {},
   "source": [
    "Before we’ve learned about variance which has below formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1625383e",
   "metadata": {},
   "source": [
    "\\[\n",
    "\\frac{\\sum{(x - \\text{mean of } x)^2}}{N}\n",
    "\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6577e5c1",
   "metadata": {},
   "source": [
    "Given \\(X\\) is a variable, \\(\\mu\\) is the mean of \\(X\\), and \\(N\\) is the number of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8162abe",
   "metadata": {},
   "source": [
    "Which basically means:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae745a95",
   "metadata": {},
   "source": [
    "One point that you need to know about variance it’s basically another name of covariance, but it’s just covariance of a variable with itself. Where does the “with itself” come from? The square of the difference:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e993b67f",
   "metadata": {},
   "source": [
    "\\[\n",
    "(x - \\text{mean of } x)^2\n",
    "\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe10d05a",
   "metadata": {},
   "source": [
    "How so? This is the formula of covariance: \\[\n",
    "Covariance(x,y) = \\frac{\\sum_{i=1}^{n}{(x_i - \\text{mean of } x)(y_i - \\text{mean of } y)}}{N}\n",
    "\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25226b74",
   "metadata": {},
   "source": [
    "So for \\(Covariance(x,x)\\) it’s the same as saying \\(Variace(x)\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf1c0a1",
   "metadata": {},
   "source": [
    "Now let’s expand a bit more on covariance. If we have 5 data points of \\(x\\) and \\(y\\): (3, 9), (4, 7), (5, 10), (8, 12), (10, 7), we need to find the mean of \\(x\\) and \\(y\\) first which is 6 and 9 respectively. Then we can calculate the covariance of \\(x\\) and \\(y\\):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0c5b11",
   "metadata": {},
   "source": [
    "\\[\n",
    "(3 - 6) \\times (9 - 9) = -3 \\times 0 = 0 \\\\\n",
    "(4 - 6) \\times (7 - 9) = -2 \\times -2 = 4 \\\\\n",
    "(5 - 6) \\times (10 - 9) = -1 \\times 1 = -1 \\\\\n",
    "(8 - 6) \\times (12 - 9) = 2 \\times 3 = 6 \\\\\n",
    "(10 - 6) \\times (7 - 9) = 4 \\times -2 = -8 \\\\\n",
    "\\] \\[\n",
    "0 + 4 - 1 + 6 - 8 = 1\n",
    "\\] \\[\n",
    "\\frac{1}{\\text{number of data points}} = \\frac{1}{5} = 0.2\n",
    "\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68da3e88",
   "metadata": {},
   "source": [
    "So basically we find the difference between each \\(x\\) and \\(y\\) data points with their respective means to know the variation of each data point from the every other data points, then multiply the differences of each data point to know how they relate to each other, then sum all the multiplication results, and divide the sum by the number of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9440307",
   "metadata": {},
   "source": [
    "# Covariance basic intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ab28a1",
   "metadata": {},
   "source": [
    "So there are three basic scenarios for covariance: - Positive covariance: if one variable increases, the other tends to increase as well - Negative covariance: if one variable increases, the other tends to decrease - Zero covariance: if one variable increases, the other doesn’t tend to increase or decrease"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9752c84c",
   "metadata": {},
   "source": [
    "We’ll use it later, for now let’s just keep it in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d24e7c",
   "metadata": {},
   "source": [
    "# Covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e15855",
   "metadata": {},
   "source": [
    "For covariance matrix it’s as easy as we’re making a matrix of covariance of each feature with each other feature. So if we have 3 features, we’ll have a 3x3 matrix. For our given example we have 3 features: math, reading, and writing. So it will be like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1605199",
   "metadata": {},
   "source": [
    "\\[\n",
    "\\begin{bmatrix}\n",
    "    Covariance(\\text{math, math}) & Covariance(\\text{math, reading}) & Covariance(\\text{math, writing}) \\\\\n",
    "    Covariance(\\text{reading, math}) & Covariance(\\text{reading, reading}) & Covariance(\\text{reading, writing}) \\\\\n",
    "    Covariance(\\text{writing, math}) & Covariance(\\text{writing, reading}) & Covariance(\\text{writing, writing})\n",
    "\\end{bmatrix}\n",
    "\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a4915c",
   "metadata": {},
   "source": [
    "Again for covariance of a variable with itself, it’s called variance. And covariance of \\(Covariance(\\text{math, reading})\\) is the same as \\(Covariance(\\text{reading, math})\\) because intuitively if math and reading are related, then reading and math are related as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5f4854",
   "metadata": {},
   "source": [
    "# Quick run through of PCA using basic numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5e4218",
   "metadata": {},
   "source": [
    "We can easily create our own PCA using numpy because it’s mostly just statistics and some linear algebra. Let’s try it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc1acfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'your_data_file.csv' with the actual path to your CSV file\n",
    "file_path = 'https://storage.googleapis.com/rg-ai-bootcamp/machine-learning/StudentsPerformance.csv'\n",
    "\n",
    "# Load the CSV data into a pandas DataFrame\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "numerical_data = data[['math score', 'reading score', 'writing score']]\n",
    "numerical_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e617f5b",
   "metadata": {},
   "source": [
    "1000 rows × 3 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f1da6d",
   "metadata": {},
   "source": [
    "First, we standardize the data by subtracting the mean from each data point (Standardizing data prior to PCA aids in capturing the underlying structure of the data by focusing on the relative variances of variables, making the analysis more meaningful) then we find the covariance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0d934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Your data\n",
    "X = data[['math score', 'reading score', 'writing score']].to_numpy()\n",
    "\n",
    "# Calculate the mean\n",
    "mean = np.mean(X, axis=0)\n",
    "\n",
    "# Center the data\n",
    "centered_X = X - mean\n",
    "\n",
    "# Compute the covariance matrix\n",
    "covariance_matrix = np.cov(centered_X, rowvar=False)\n",
    "\n",
    "# Create a DataFrame with labels\n",
    "column_labels = ['math score', 'reading score', 'writing score']\n",
    "covariance_df = pd.DataFrame(covariance_matrix, columns=column_labels, index=column_labels)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(covariance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58221bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "               math score  reading score  writing score\n",
    "math score     229.918998     180.998958     184.939133\n",
    "reading score  180.998958     213.165605     211.786661\n",
    "writing score  184.939133     211.786661     230.907992"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60b72ef",
   "metadata": {},
   "source": [
    "Then we create a transformation matrix from given covariance matrix. The math involved some intermediate steps about eigenvalues and eigenvectors that outside of our scope, but the main intuition is basically we’re trying to find lines we project our data points to that the spread of the projected data points is as big as possible (so we don’t lose much information after the projection). This projection lines dictated by using our covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eff07f8",
   "metadata": {},
   "source": [
    "This concept is a little bit hard to understand, but hopefully below illustration can help you to understand it better:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716097bf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b24f8954",
   "metadata": {},
   "source": [
    "Illustration from https://numxl.com/blogs/principal-component-analysis-pca-101/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ef7f0f",
   "metadata": {},
   "source": [
    "So as you can see above we have two different lines that we can project our data points to from two different angles so that the spread of the projected data points is as big as possible. The process of finding the best line fit can be seen below where the best line fit is when the projection line match with purple lines that you can see on the left and right plot:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a975d7af",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "713cd5b0",
   "metadata": {},
   "source": [
    "Illustration from https://medium.com/@ashwin8april/dimensionality-reduction-and-visualization-using-pca-principal-component-analysis-8489b46c2ae0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053f817e",
   "metadata": {},
   "source": [
    "It’s kind of finding the linear regression line, but after we found from one angle, we rotate it to find the best line fit from another angle. So basically we’re trying to find the best line fit from all angles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ab00dd",
   "metadata": {},
   "source": [
    "Now let’s find that best line fit, below \\(k\\) is the number of principal components that we want to have after the dimensionality reduction. So if we want to reduce our data from 3 dimensions to 2 dimensions, \\(k\\) will be 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf6a48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "k= 2\n",
    "# Step 3: Eigen decomposition\n",
    "eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "# Step 4: Sorting eigenvalues and eigenvectors\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues = eigenvalues[sorted_indices]\n",
    "eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "# Step 5: Selecting the top k eigenvectors\n",
    "top_k_eigenvectors = eigenvectors[:, :k]\n",
    "\n",
    "top_k_eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89413ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "array([[ 0.56264911,  0.82561176],\n",
    "       [ 0.57397682, -0.35329218],\n",
    "       [ 0.59495932, -0.43994302]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f912d0e6",
   "metadata": {},
   "source": [
    "Above we’re having several steps of PCA:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84a3f37",
   "metadata": {},
   "source": [
    "And we’ve got \\(3 \\times 2\\) matrix that each of the column is the “best line fit” that we’ve found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372d44a0",
   "metadata": {},
   "source": [
    "Now to project our data to this vector, we just need to multiply our data with our transformation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1104af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Projecting the data\n",
    "reduced_data = np.matmul(centered_X, top_k_eigenvectors)\n",
    "\n",
    "reduced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3cc31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "array([[  8.48837536,   1.26411978],\n",
    "       [ 25.46144129, -13.73117695],\n",
    "       [ 43.12175323,  -0.35950596],\n",
    "       ...,\n",
    "       [ -4.75467372,  -5.15605377],\n",
    "       [ 11.46651782,  -5.47790938],\n",
    "       [ 26.47680822,  -4.83322812]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de16906",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(reduced_data, columns=['X', 'Y'])\n",
    "\n",
    "# Create a scatter plot using Plotly\n",
    "fig = px.scatter(df, x='X', y='Y', title='Scatter Plot')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d188ebb4",
   "metadata": {},
   "source": [
    "Above scatter plot seems mirrored than using the scikit-learn’s PCA, but the main idea is basically the same, it’s fun isn’t it? 😁"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
