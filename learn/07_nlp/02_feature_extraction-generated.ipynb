{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51902ed2",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081277db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01ce768",
   "metadata": {},
   "source": [
    "In the previous session, we already getting teased at the concept of how our model can know which word is positive, and which is negative. When we‚Äôre working on a sentiment classification task with Naive Bayes, there is one step before we can pushed our input to Naive Bayes classifier: Feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49367997",
   "metadata": {},
   "source": [
    "Feature extraction, like what we might‚Äôve done on previous architecture like CNN, is a way to shape our features so our model can learn. In NLP it‚Äôs basically converting our input of sentences to numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87afb4db",
   "metadata": {},
   "source": [
    "Feature extraction can be viewed like word embedding but a lot more simpler, which we‚Äôll learn the difference later in ‚ÄúWord embedding section‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fdfcc8",
   "metadata": {},
   "source": [
    "# Sheets for the math intution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67645b5b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64b7952c",
   "metadata": {},
   "source": [
    "For our learning we‚Äôll use this sheets to make us learn the math intuition behind what we‚Äôll learn later faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21467b7a",
   "metadata": {},
   "source": [
    "https://docs.google.com/spreadsheets/d/1aGhICbKMUvzXjvHKd69sy0-cBdNqfcCMuz27AOLqZh8/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02f0979",
   "metadata": {},
   "source": [
    "# Bag-Of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2869c13c",
   "metadata": {},
   "source": [
    "Bag-of-words is a way to convert our tokenized input to matrix of numbers. The concept is really simple: Every word is having their own dimension in the matrix and for every sentence in our tokenized dataset will be converted to that matrix by counting how many times the word is occured on that sentenced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8925dbad",
   "metadata": {},
   "source": [
    "It‚Äôs really simple when we‚Äôre seeing the example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c211b1a6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4300c0bc",
   "metadata": {},
   "source": [
    "So for every word in our training set, it will have their own dedicated dimensions, and we just count in every sentence on how many each words occured."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969638ff",
   "metadata": {},
   "source": [
    "In reality, we can just pushed above scores right to Naive Bayes classifier, but we can too learn a simple update from BoW that can help Naive Bayes works better. The ‚Äúupgraded from BoW‚Äù feature extraction method name is TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2e2d82",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5ca8af",
   "metadata": {},
   "source": [
    "Let‚Äôs break the name first so we can unmythified this ‚Äúscary‚Äù term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbed20cd",
   "metadata": {},
   "source": [
    "TF - Term Frequency IDF - Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbd185e",
   "metadata": {},
   "source": [
    "TF, term frequency, is basically bag-of-words, we count how frequent the term is being use in a single document, basically? Bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a1df9a",
   "metadata": {},
   "source": [
    "IDF is where the twist is: If a term is being used so often in the whole dataset, it will lower the score of that words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aedd41e",
   "metadata": {},
   "source": [
    "I really like your performance on that stage, your words is so mesmerizing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223efcae",
   "metadata": {},
   "source": [
    "I don‚Äôt like pineapple on pizza, the idea of it even not sounding too exciting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c414d71",
   "metadata": {},
   "source": [
    "If we‚Äôre already removing stopwords, the concept of TF-IDF will help you further finding the ‚Äúneutral‚Äù words that won‚Äôt help on your text classification tasks. Just like the word ‚ÄúI‚Äù won‚Äôt help on our sentiment classification task, but might be not be removed by our stopwords removal process as it‚Äôs generally can be use for another task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab91985a",
   "metadata": {},
   "source": [
    "TF-IDF helps us let the model know which words should be higher in their scoring towards classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46369e7",
   "metadata": {},
   "source": [
    "The term ‚Äúinverse‚Äù in IDF itself is basically because we‚Äôre inversing the quantity: If a word comes too many times, the score would be lower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188acb4c",
   "metadata": {},
   "source": [
    "# TF-IDF Math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f4d53f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9bfc1ab",
   "metadata": {},
   "source": [
    "For our TF-IDF learning we‚Äôll use previous dataset that we use for our Bag-of-words, and let‚Äôs see how TF-IDF will help us reducing importance of the word ‚ÄúI‚Äôm‚Äù as it‚Äôs keep occuring throughout the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a0d992",
   "metadata": {},
   "source": [
    "# IDF calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69984fdb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66213361",
   "metadata": {},
   "source": [
    "The IDF calculation for a feature is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e322f8",
   "metadata": {},
   "outputs": [],
   "source": [
    " ln(Total number of documents (datasets) / Number of documents with the feature in it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9e6953",
   "metadata": {},
   "source": [
    "After you do above steps for every single feature, you can see IDF score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933b81cb",
   "metadata": {},
   "source": [
    "# TF-IDF = TF x IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f412c54c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97ea696c",
   "metadata": {},
   "source": [
    "The TF-IDF then can be calculated by simply multiplying the TF for all the inputs to the IDF. Then we‚Äôll see below score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74957d6c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "177c5b53",
   "metadata": {},
   "source": [
    "As we can see above that words like ‚ÄúI‚Äôm‚Äù, because it‚Äôs seen basically in every dataset, using this to analyze the sentiment won‚Äôt be useful. As for other words they‚Äôre now prioritized by their occurence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8292105",
   "metadata": {},
   "source": [
    "Words like ‚Äúfeeling‚Äù get higher value than ‚Äúexcited‚Äù, simply because our limited dataset has it less than word ‚Äúexcited‚Äù. So, for words like this that rarely occured but in reality shouldn‚Äôt affect a sentiment, our classification system should move forward to the next step: Using Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21277b84",
   "metadata": {},
   "source": [
    "# Naive bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e45ad8",
   "metadata": {},
   "source": [
    "Naive bayes is one of the most simplest way to do a text classification without neural network. The concept is pretty much like what our intuitions: Finding words that can give hints towards any classification, while shouldn‚Äôt give any classification to ‚Äúneutral words‚Äù (words that shouldn‚Äôt giving hints towards any classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82da5411",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad12dac7",
   "metadata": {},
   "source": [
    "The operation is simple for"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ab6d53",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12ab146d",
   "metadata": {},
   "source": [
    "Then we can divide the end score with the percentage of how many is the dataset of every classification is when compared to the total of the entire dataset. As for our current dataset we have 50% data on ‚Äúpositive‚Äù sentiment, and 50% data on ‚Äúnegative‚Äù sentiment (balanced dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f91ffe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10009af3",
   "metadata": {},
   "source": [
    "As you can see above using ‚ÄúNaive bayes classifier‚Äù we can get every words that are can hint that a sentiment is positive, negative, or neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ad85bd",
   "metadata": {},
   "source": [
    "Positive words would have higher score on the positive sentiment, negative words would have higher score on the negative sentiment, neutral words would have similar (or if there are difference, the difference won‚Äôt be significant) score both on positive and negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92466fe1",
   "metadata": {},
   "source": [
    "# Let‚Äôs input our data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c28279",
   "metadata": {},
   "source": [
    "We can now try to input a sentence that our Naive Bayes never seen before:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487a232a",
   "metadata": {},
   "source": [
    "I‚Äôm feeling really sad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af838708",
   "metadata": {},
   "source": [
    "The steps is as simple as extract our features to something like below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60c9a0a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64b154a0",
   "metadata": {},
   "source": [
    "Then after that we multiply with our TF-IDF for each classification, then we‚Äôll get the score for each of word existing in our input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b69f8c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f64bd04f",
   "metadata": {},
   "source": [
    "After that to know what classification is our sentence is classified into according to our Naive Bayes we can just sum the per feature scores to a single score for each classsification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d2d119",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d632329",
   "metadata": {},
   "source": [
    "So according to our Naive Bayes, the sentence ‚ÄúI‚Äôm feeling really sad‚Äù, classified into negative sentiment üí•."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39c93e1",
   "metadata": {},
   "source": [
    "# The ‚Äúnaive‚Äù part of ‚Äúnaive bayes‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c77631",
   "metadata": {},
   "source": [
    "Naive bayes called naive because it‚Äôs ‚Äúnaively‚Äù consider that every word is independent of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbd8273",
   "metadata": {},
   "source": [
    "Naive bayes will treat every word as independent, meaning it can‚Äôt really ‚Äúnegate‚Äù above ‚Äúhappy‚Äù word to ‚Äúnot happy‚Äù, as it doesn‚Äôt really understand between more than a single feature (it can‚Äôt create relation between ‚Äúnot‚Äù and ‚Äúhappy‚Äù, and can‚Äôt create conclusion that the word ‚Äúnot‚Äù meant to negate the ‚Äúhappy‚Äù word)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43acc710",
   "metadata": {},
   "source": [
    "There are of course other classification method other than ‚ÄúNaive bayes‚Äù, even our feature extraction can be improved with more complex method that can somewhat make relation between one word and another without using deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f01c86",
   "metadata": {},
   "source": [
    "But in the end, machine learning without deep learning will make us harder to make sure our model understand context, and relations, between words. This is the part where neural networks really excels at when handling NLP: Understanding complex pattern of human words and also acting upon it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6908df2b",
   "metadata": {},
   "source": [
    "As we can see when learning ‚ÄúNaive Bayes‚Äù, without neural network it‚Äôs really hard to make a model really understand pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7953703",
   "metadata": {},
   "source": [
    "So let‚Äôs try to upgrade our understanding and move on to NLP using deep learning!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
