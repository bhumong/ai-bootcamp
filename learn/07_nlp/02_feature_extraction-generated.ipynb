{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aa6c6c7",
   "metadata": {},
   "source": [
    "source: [https://ai-bootcamp.ruangguru.com/learn/07_nlp/02_feature_extraction.html](https://ai-bootcamp.ruangguru.com/learn/07_nlp/02_feature_extraction.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc3600c",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2870a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206889ff",
   "metadata": {},
   "source": [
    "In the previous session, we already getting teased at the concept of how our model can know which word is positive, and which is negative. When weâ€™re working on a sentiment classification task with Naive Bayes, there is one step before we can pushed our input to Naive Bayes classifier: Feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022881c4",
   "metadata": {},
   "source": [
    "Feature extraction, like what we mightâ€™ve done on previous architecture like CNN, is a way to shape our features so our model can learn. In NLP itâ€™s basically converting our input of sentences to numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f936b1e0",
   "metadata": {},
   "source": [
    "Feature extraction can be viewed like word embedding but a lot more simpler, which weâ€™ll learn the difference later in â€œWord embedding sectionâ€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2583e034",
   "metadata": {},
   "source": [
    "# Sheets for the math intution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c767abd1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12d1fec2",
   "metadata": {},
   "source": [
    "![Image](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/image_SG9wtLLnk.png?updatedAt=1695362767430)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc4623a",
   "metadata": {},
   "source": [
    "For our learning weâ€™ll use this sheets to make us learn the math intuition behind what weâ€™ll learn later faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c8071e",
   "metadata": {},
   "source": [
    "https://docs.google.com/spreadsheets/d/1aGhICbKMUvzXjvHKd69sy0-cBdNqfcCMuz27AOLqZh8/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a3fbe0",
   "metadata": {},
   "source": [
    "# Bag-Of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fadbaf0",
   "metadata": {},
   "source": [
    "Bag-of-words is a way to convert our tokenized input to matrix of numbers. The concept is really simple: Every word is having their own dimension in the matrix and for every sentence in our tokenized dataset will be converted to that matrix by counting how many times the word is occured on that sentenced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524254b3",
   "metadata": {},
   "source": [
    "Itâ€™s really simple when weâ€™re seeing the example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b71502",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38fa0662",
   "metadata": {},
   "source": [
    "![Image](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/image_HTrrXdT43.png?updatedAt=1695363358969)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761dcaec",
   "metadata": {},
   "source": [
    "So for every word in our training set, it will have their own dedicated dimensions, and we just count in every sentence on how many each words occured."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe985df",
   "metadata": {},
   "source": [
    "In reality, we can just pushed above scores right to Naive Bayes classifier, but we can too learn a simple update from BoW that can help Naive Bayes works better. The â€œupgraded from BoWâ€ feature extraction method name is TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bf10ef",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bea6b63",
   "metadata": {},
   "source": [
    "Letâ€™s break the name first so we can unmythified this â€œscaryâ€ term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85341d2",
   "metadata": {},
   "source": [
    "TF - Term Frequency IDF - Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22066e4",
   "metadata": {},
   "source": [
    "TF, term frequency, is basically bag-of-words, we count how frequent the term is being use in a single document, basically? Bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96d5810",
   "metadata": {},
   "source": [
    "IDF is where the twist is: If a term is being used so often in the whole dataset, it will lower the score of that words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83685b3e",
   "metadata": {},
   "source": [
    "> I really like your performance on that stage, your words is so mesmerizing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4eb46f",
   "metadata": {},
   "source": [
    "I really like your performance on that stage, your words is so mesmerizing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3a3842",
   "metadata": {},
   "source": [
    "> I donâ€™t like pineapple on pizza, the idea of it even not sounding too exciting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca2248d",
   "metadata": {},
   "source": [
    "I donâ€™t like pineapple on pizza, the idea of it even not sounding too exciting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c20aebd",
   "metadata": {},
   "source": [
    "If weâ€™re already removing stopwords, the concept of TF-IDF will help you further finding the â€œneutralâ€ words that wonâ€™t help on your text classification tasks. Just like the word â€œIâ€ wonâ€™t help on our sentiment classification task, but might be not be removed by our stopwords removal process as itâ€™s generally can be use for another task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a7ad4e",
   "metadata": {},
   "source": [
    "TF-IDF helps us let the model know which words should be higher in their scoring towards classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec15678",
   "metadata": {},
   "source": [
    "The term â€œinverseâ€ in IDF itself is basically because weâ€™re inversing the quantity: If a word comes too many times, the score would be lower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acab599",
   "metadata": {},
   "source": [
    "# TF-IDF Math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18d25b2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f14709d",
   "metadata": {},
   "source": [
    "![Image](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/image_xGBPsufXt.png?updatedAt=1695317984944)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f88e80",
   "metadata": {},
   "source": [
    "For our TF-IDF learning weâ€™ll use previous dataset that we use for our Bag-of-words, and letâ€™s see how TF-IDF will help us reducing importance of the word â€œIâ€™mâ€ as itâ€™s keep occuring throughout the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e75f9f1",
   "metadata": {},
   "source": [
    "# IDF calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697ce8c8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "653002da",
   "metadata": {},
   "source": [
    "![Image](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/image_SmK9SlPhX.png?updatedAt=1695319110470)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0651e664",
   "metadata": {},
   "source": [
    "The IDF calculation for a feature is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec53d5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    " ln(Total number of documents (datasets) / Number of documents with the feature in it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c0f669",
   "metadata": {},
   "source": [
    "After you do above steps for every single feature, you can see IDF score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc3c5e8",
   "metadata": {},
   "source": [
    "# TF-IDF = TF x IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722a5be3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82198290",
   "metadata": {},
   "source": [
    "![Image](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/image_FleZfOk9F.png?updatedAt=1695320198870)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41104921",
   "metadata": {},
   "source": [
    "The TF-IDF then can be calculated by simply multiplying the TF for all the inputs to the IDF. Then weâ€™ll see below score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae7d08c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2234cc4",
   "metadata": {},
   "source": [
    "![Image](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/image_M9prAVdJQn.png?updatedAt=1695320869332)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98b538f",
   "metadata": {},
   "source": [
    "As we can see above that words like â€œIâ€™mâ€, because itâ€™s seen basically in every dataset, using this to analyze the sentiment wonâ€™t be useful. As for other words theyâ€™re now prioritized by their occurence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d958fbb4",
   "metadata": {},
   "source": [
    "Words like â€œfeelingâ€ get higher value than â€œexcitedâ€, simply because our limited dataset has it less than word â€œexcitedâ€. So, for words like this that rarely occured but in reality shouldnâ€™t affect a sentiment, our classification system should move forward to the next step: Using Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233d0698",
   "metadata": {},
   "source": [
    "# Naive bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a22b04d",
   "metadata": {},
   "source": [
    "Naive bayes is one of the most simplest way to do a text classification without neural network. The concept is pretty much like what our intuitions: Finding words that can give hints towards any classification, while shouldnâ€™t give any classification to â€œneutral wordsâ€ (words that shouldnâ€™t giving hints towards any classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf5a3eb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dc7c949",
   "metadata": {},
   "source": [
    "![Image](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/image_ApIHWVTZc.png?updatedAt=1695355604042)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6febc10",
   "metadata": {},
   "source": [
    "The operation is simple for"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb478ae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5ec772c",
   "metadata": {},
   "source": [
    "![Image](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/image_8hwRd_gMH.png?updatedAt=1695355900645)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9de21e1",
   "metadata": {},
   "source": [
    "Then we can divide the end score with the percentage of how many is the dataset of every classification is when compared to the total of the entire dataset. As for our current dataset we have 50% data on â€œpositiveâ€ sentiment, and 50% data on â€œnegativeâ€ sentiment (balanced dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9e806a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b09965f",
   "metadata": {},
   "source": [
    "![Image](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/image_rddLANhSv.png?updatedAt=1695356383816)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca17341e",
   "metadata": {},
   "source": [
    "As you can see above using â€œNaive bayes classifierâ€ we can get every words that are can hint that a sentiment is positive, negative, or neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34313ff",
   "metadata": {},
   "source": [
    "Positive words would have higher score on the positive sentiment, negative words would have higher score on the negative sentiment, neutral words would have similar (or if there are difference, the difference wonâ€™t be significant) score both on positive and negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dcead4",
   "metadata": {},
   "source": [
    "# Letâ€™s input our data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49c16a5",
   "metadata": {},
   "source": [
    "We can now try to input a sentence that our Naive Bayes never seen before:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75699a5d",
   "metadata": {},
   "source": [
    "> Iâ€™m feeling really sad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64321b4",
   "metadata": {},
   "source": [
    "Iâ€™m feeling really sad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0528dac4",
   "metadata": {},
   "source": [
    "The steps is as simple as extract our features to something like below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22586f5a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b3a13ad",
   "metadata": {},
   "source": [
    "![Image](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/image_4RVkRT6-z.png?updatedAt=1695357154011)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d31fa4",
   "metadata": {},
   "source": [
    "Then after that we multiply with our TF-IDF for each classification, then weâ€™ll get the score for each of word existing in our input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ba7503",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b241879",
   "metadata": {},
   "source": [
    "![Image](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/image_sLQhz1jy8.png?updatedAt=1695361458532)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9290e034",
   "metadata": {},
   "source": [
    "After that to know what classification is our sentence is classified into according to our Naive Bayes we can just sum the per feature scores to a single score for each classsification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a07292",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e037c5ad",
   "metadata": {},
   "source": [
    "![Image](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/image__XiuEBA_s.png?updatedAt=1695361865254)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcff021",
   "metadata": {},
   "source": [
    "So according to our Naive Bayes, the sentence â€œIâ€™m feeling really sadâ€, classified into negative sentiment ğŸ’¥."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b1066a",
   "metadata": {},
   "source": [
    "# The â€œnaiveâ€ part of â€œnaive bayesâ€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9cb7e7",
   "metadata": {},
   "source": [
    "Naive bayes called naive because itâ€™s â€œnaivelyâ€ consider that every word is independent of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7337259f",
   "metadata": {},
   "source": [
    "- I am not happy because she just left me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb2bf2c",
   "metadata": {},
   "source": [
    "Naive bayes will treat every word as independent, meaning it canâ€™t really â€œnegateâ€ above â€œhappyâ€ word to â€œnot happyâ€, as it doesnâ€™t really understand between more than a single feature (it canâ€™t create relation between â€œnotâ€ and â€œhappyâ€, and canâ€™t create conclusion that the word â€œnotâ€ meant to negate the â€œhappyâ€ word)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fe75ff",
   "metadata": {},
   "source": [
    "There are of course other classification method other than â€œNaive bayesâ€, even our feature extraction can be improved with more complex method that can somewhat make relation between one word and another without using deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0902b33f",
   "metadata": {},
   "source": [
    "But in the end, machine learning without deep learning will make us harder to make sure our model understand context, and relations, between words. This is the part where neural networks really excels at when handling NLP: Understanding complex pattern of human words and also acting upon it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6b88e0",
   "metadata": {},
   "source": [
    "As we can see when learning â€œNaive Bayesâ€, without neural network itâ€™s really hard to make a model really understand pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfe894c",
   "metadata": {},
   "source": [
    "So letâ€™s try to upgrade our understanding and move on to NLP using deep learning!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
