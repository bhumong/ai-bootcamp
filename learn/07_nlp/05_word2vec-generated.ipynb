{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e529b609",
   "metadata": {},
   "source": [
    "source: [link](https://ai-bootcamp.ruangguru.com/learn/07_nlp/05_word2vec.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371fbcfe",
   "metadata": {},
   "source": [
    "# Generate Word Embedding With Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76af3ee1",
   "metadata": {},
   "source": [
    "In the previous session, we have seen the power of word embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac253aa",
   "metadata": {},
   "source": [
    "Now we will see how to generate word embedding using Word2Vec!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74318372",
   "metadata": {},
   "source": [
    "Word2Vec is the most popular word embedding technique, where we train a neural network on a simple two-layer network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3a24ff",
   "metadata": {},
   "source": [
    "# Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831d19ca",
   "metadata": {},
   "source": [
    "The concept is pretty simple if we try to delve in into the intuition first:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4c83d5",
   "metadata": {},
   "source": [
    "Our input will start as a one-hot encoded vector. We’ve learned it before: A vector which only has one element as 1 and the rest are 0s. This vector will represent our input word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dfd440",
   "metadata": {},
   "source": [
    "This one-hot encoded vector is fully connected to a hidden layer where every individual neuron represents different focused contexts gathered from training. One neuron might be focused on the tense of verbs, while another might focus on gender differences in pronouns, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1657d64a",
   "metadata": {},
   "source": [
    "The hidden layer is then fully connected to an output layer which uses softmax to produce probabilities for every word in the vocab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc677f72",
   "metadata": {},
   "source": [
    "This last step is crucial to understand how Word2Vec understand relations between words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe06b728",
   "metadata": {},
   "source": [
    "# The Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a7b434",
   "metadata": {},
   "source": [
    "If we read a lot of sentences like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b247a9",
   "metadata": {},
   "source": [
    "We shall see that the words “king” and “ruler” are used in similar contexts, i.e. “king” should be close to “ruler” in the vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac54b1f",
   "metadata": {},
   "source": [
    "“commanded” and “ordered” are also used in similar contexts, so they should be close to each other as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4ac222",
   "metadata": {},
   "source": [
    "Let’s try other examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd309035",
   "metadata": {},
   "source": [
    "We see that “fish” and “water” often appear together. Shall we conclude that “fish” and “water” are close to each other in the vector space?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfae69fb",
   "metadata": {},
   "source": [
    "Yes, we can!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c777a61",
   "metadata": {},
   "source": [
    "So, how can we use this information to build a word embedding?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0c2657",
   "metadata": {},
   "source": [
    "# Relating words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a145aee",
   "metadata": {},
   "source": [
    "How Word2Vec learning context of a word is that when a word is commonly found near another word, then these two words have a close relationship. For example, the words “fish” and “water” are often found together, so they have a close relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60487820",
   "metadata": {},
   "source": [
    "Let’s use the following sentence as an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ebdc6b",
   "metadata": {},
   "source": [
    "\\[\\text{\"I love to eat fish, but I hate to drink water\"}\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0496a8d6",
   "metadata": {},
   "source": [
    "When creating this relation, there are two ways to do it:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be27ed1",
   "metadata": {},
   "source": [
    "# Continuous Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e14251",
   "metadata": {},
   "source": [
    "Predicts words within a certain range before and after a word in a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3567d74a",
   "metadata": {},
   "source": [
    "So, given “eat” and window size = 2, the skip-gram model will predict “love”, “to”, “fish”, and “but” as the output words. (see 2nd row below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6813d6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skip_gram_pairs(sentence, window_size):\n",
    "    skip_gram_pairs = []\n",
    "    words = sentence.lower().split()\n",
    "    for i in range(len(words)):\n",
    "        predicted = []\n",
    "        for j in range(i - window_size, i + window_size + 1):\n",
    "            if j == i or j < 0 or j >= len(words):\n",
    "                continue\n",
    "            predicted.append(words[j])\n",
    "        skip_gram_pairs.append([words[i], predicted])\n",
    "    return skip_gram_pairs\n",
    "\n",
    "get_skip_gram_pairs(\"I love to eat fish, but I hate to drink water\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6208ad9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[['i', ['love', 'to']],\n",
    " ['love', ['i', 'to', 'eat']],\n",
    " ['to', ['i', 'love', 'eat', 'fish,']],\n",
    " ['eat', ['love', 'to', 'fish,', 'but']],\n",
    " ['fish,', ['to', 'eat', 'but', 'i']],\n",
    " ['but', ['eat', 'fish,', 'i', 'hate']],\n",
    " ['i', ['fish,', 'but', 'hate', 'to']],\n",
    " ['hate', ['but', 'i', 'to', 'drink']],\n",
    " ['to', ['i', 'hate', 'drink', 'water']],\n",
    " ['drink', ['hate', 'to', 'water']],\n",
    " ['water', ['to', 'drink']]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a3069",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29386fa4",
   "metadata": {},
   "source": [
    "![Image](https://storage.googleapis.com/rg-ai-bootcamp/nlp/skip-gram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d22372",
   "metadata": {},
   "source": [
    "# Continuous Bag of Words (CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71c8bbd",
   "metadata": {},
   "source": [
    "It’s quite the opposite of Skip-gram. It predicts a middle word given the context of a few words before and a few words after the target word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc81d116",
   "metadata": {},
   "source": [
    "So, given “love”, “to”, “fish”, “but”, the CBOW model will predict “eat” as the output word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c72d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cbow(sentence, window_size):\n",
    "    words = sentence.split()\n",
    "    cbow_pairs = []\n",
    "    for i in range(window_size, len(words) - window_size):\n",
    "        context_words = []\n",
    "        for j in range(i - window_size, i + window_size + 1):\n",
    "            if j == i or j < 0 or j >= len(words):\n",
    "                continue\n",
    "            context_words.append(words[j])\n",
    "        cbow_pairs.append((context_words, words[i]))\n",
    "    return cbow_pairs\n",
    "\n",
    "generate_cbow(\"I love to eat fish, but I hate to drink water\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c04ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(['I', 'love', 'eat', 'fish,'], 'to'),\n",
    " (['love', 'to', 'fish,', 'but'], 'eat'),\n",
    " (['to', 'eat', 'but', 'I'], 'fish,'),\n",
    " (['eat', 'fish,', 'I', 'hate'], 'but'),\n",
    " (['fish,', 'but', 'hate', 'to'], 'I'),\n",
    " (['but', 'I', 'to', 'drink'], 'hate'),\n",
    " (['I', 'hate', 'drink', 'water'], 'to')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae54bf8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53aded4e",
   "metadata": {},
   "source": [
    "![Image](https://storage.googleapis.com/rg-ai-bootcamp/nlp/cbow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2090380b",
   "metadata": {},
   "source": [
    "# Let’s build a Word2Vec model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44de51f9",
   "metadata": {},
   "source": [
    "# The easiest way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bde8b38",
   "metadata": {},
   "source": [
    "The easiest way to build a Word2Vec model is to use the gensim library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce52e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [['I', 'love', 'to','eat', 'ice', 'cream'],\n",
    "                ['The', 'ice', 'cream', 'is', 'delicious'],\n",
    "                ['Ice', 'cream', 'is', 'my', 'favorite'],\n",
    "                ['Ice', 'is', 'very', 'cold'],\n",
    "                ['South', 'Africa', 'is', 'the', 'house', 'of', 'various', 'animals'],\n",
    "                ['The', 'desert', 'is', 'very', 'hot']]\n",
    "model = Word2Vec(sentences, min_count=1, vector_size=100, window=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c7c765",
   "metadata": {},
   "source": [
    "We know have the word2vec model, yes it’s that simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5ef6db",
   "metadata": {},
   "source": [
    "Let’s see how to use it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5983ca7b",
   "metadata": {},
   "source": [
    "Let’s print the vector of the word “ice”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b63df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv['ice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4495ee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "array([ 9.3212293e-05,  3.0770719e-03, -6.8118651e-03, -1.3751196e-03,\n",
    "        7.6688202e-03,  7.3457859e-03, -3.6729246e-03,  2.6427959e-03,\n",
    "       -8.3167218e-03,  6.2047895e-03, -4.6373853e-03, -3.1648867e-03,\n",
    "        9.3104383e-03,  8.7417278e-04,  7.4903117e-03, -6.0727512e-03,\n",
    "        5.1610614e-03,  9.9233752e-03, -8.4570879e-03, -5.1350184e-03,\n",
    "       -7.0640068e-03, -4.8623742e-03, -3.7776425e-03, -8.5362354e-03,\n",
    "        7.9563707e-03, -4.8429691e-03,  8.4230686e-03,  5.2623590e-03,\n",
    "       -6.5500555e-03,  3.9582876e-03,  5.4709758e-03, -7.4261688e-03,\n",
    "       -7.4054217e-03, -2.4756726e-03, -8.6249216e-03, -1.5812701e-03,\n",
    "       -4.0279646e-04,  3.3000994e-03,  1.4431456e-03, -8.8017591e-04,\n",
    "       -5.5925641e-03,  1.7296794e-03, -8.9829665e-04,  6.7929067e-03,\n",
    "        3.9731935e-03,  4.5301151e-03,  1.4342893e-03, -2.6998674e-03,\n",
    "       -4.3663131e-03, -1.0323119e-03,  1.4375548e-03, -2.6464923e-03,\n",
    "       -7.0737889e-03, -7.8048133e-03, -9.1210250e-03, -5.9340443e-03,\n",
    "       -1.8465136e-03, -4.3233316e-03, -6.4603114e-03, -3.7162432e-03,\n",
    "        4.2880280e-03, -3.7385889e-03,  8.3772345e-03,  1.5335697e-03,\n",
    "       -7.2412803e-03,  9.4334288e-03,  7.6311510e-03,  5.4920013e-03,\n",
    "       -6.8473201e-03,  5.8228681e-03,  4.0087220e-03,  5.1837498e-03,\n",
    "        4.2560440e-03,  1.9400261e-03, -3.1702011e-03,  8.3524166e-03,\n",
    "        9.6113142e-03,  3.7917446e-03, -2.8362276e-03,  7.0220985e-06,\n",
    "        1.2179716e-03, -8.4580434e-03, -8.2226843e-03, -2.3149964e-04,\n",
    "        1.2369631e-03, -5.7432777e-03, -4.7246884e-03, -7.3462939e-03,\n",
    "        8.3279610e-03,  1.2049330e-04, -4.5093168e-03,  5.7014343e-03,\n",
    "        9.1802459e-03, -4.1006147e-03,  7.9636248e-03,  5.3757117e-03,\n",
    "        5.8797505e-03,  5.1249505e-04,  8.2120160e-03, -7.0181224e-03],\n",
    "      dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d9a8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('ice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9f7a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "[('very', 0.13172131776809692),\n",
    " ('delicious', 0.07499014586210251),\n",
    " ('cream', 0.06798356026411057),\n",
    " ('favorite', 0.04159315675497055),\n",
    " ('to', 0.04135243594646454),\n",
    " ('eat', 0.012988785281777382),\n",
    " ('I', 0.0066059790551662445),\n",
    " ('love', -0.009281391277909279),\n",
    " ('Ice', -0.013502932153642178),\n",
    " ('my', -0.013687963597476482)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beec9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('Africa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8676967",
   "metadata": {},
   "outputs": [],
   "source": [
    "[('is', 0.21887390315532684),\n",
    " ('my', 0.17480239272117615),\n",
    " ('hot', 0.16380424797534943),\n",
    " ('very', 0.10851778090000153),\n",
    " ('various', 0.10759598016738892),\n",
    " ('South', 0.06559502333402634),\n",
    " ('house', 0.059589654207229614),\n",
    " ('cold', 0.0490604005753994),\n",
    " ('Ice', 0.04764048010110855),\n",
    " ('cream', 0.02233739383518696)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13a82bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://gist.githubusercontent.com/phillipj/4944029/raw/75ba2243dd5ec2875f629bf5d79f6c1e4b5a8b46/alice_in_wonderland.txt\", \"alice.txt\")\n",
    "\n",
    "sentences = []\n",
    "with open('alice.txt', 'r') as f:\n",
    "    sentences = f.readlines()\n",
    "    sentences = [sentence.strip() for sentence in sentences]\n",
    "    sentences = [sentence for sentence in sentences if sentence != '']\n",
    "    sentences = [sentence.split() for sentence in sentences]\n",
    "\n",
    "    # remove punctuation\n",
    "    sentences = [[word for word in sentence if word.isalpha()] for sentence in sentences]\n",
    "    # lower case\n",
    "    sentences = [[word.lower() for word in sentence] for sentence in sentences]\n",
    "\n",
    "sentences[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95ed17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[['adventures', 'in', 'wonderland'],\n",
    " ['adventures', 'in', 'wonderland'],\n",
    " ['lewis', 'carroll'],\n",
    " ['the', 'millennium', 'fulcrum', 'edition'],\n",
    " ['chapter', 'i'],\n",
    " ['down', 'the'],\n",
    " ['alice',\n",
    "  'was',\n",
    "  'beginning',\n",
    "  'to',\n",
    "  'get',\n",
    "  'very',\n",
    "  'tired',\n",
    "  'of',\n",
    "  'sitting',\n",
    "  'by',\n",
    "  'her',\n",
    "  'sister'],\n",
    " ['on',\n",
    "  'the',\n",
    "  'and',\n",
    "  'of',\n",
    "  'having',\n",
    "  'nothing',\n",
    "  'to',\n",
    "  'once',\n",
    "  'or',\n",
    "  'twice',\n",
    "  'she',\n",
    "  'had']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cf70ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train sentences to Word2Vec model\n",
    "\n",
    "model = Word2Vec(sentences, min_count=1, vector_size=100, window=5)\n",
    "\n",
    "model.wv.most_similar('dark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1564959",
   "metadata": {},
   "outputs": [],
   "source": [
    "[('creatures', 0.8895953297615051),\n",
    " ('open', 0.8853762149810791),\n",
    " ('william', 0.8853062987327576),\n",
    " ('gave', 0.8836493492126465),\n",
    " ('extraordinary', 0.8834893703460693),\n",
    " ('shook', 0.8826101422309875),\n",
    " ('until', 0.8825846910476685),\n",
    " ('puzzled', 0.8824530839920044),\n",
    " ('half', 0.8820216059684753),\n",
    " ('whether', 0.8819820284843445)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9f669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"animal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6313dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "[('sit', 0.9368554353713989),\n",
    " ('yourself', 0.935146152973175),\n",
    " ('every', 0.9333352446556091),\n",
    " ('waving', 0.9328792691230774),\n",
    " ('walked', 0.9322237372398376),\n",
    " ('too', 0.9318441152572632),\n",
    " ('hatter', 0.9318233132362366),\n",
    " ('hands', 0.9317442178726196),\n",
    " ('right', 0.9315378069877625),\n",
    " ('go', 0.9311593174934387)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0328dbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aa9d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[('cook', 0.97368985414505),\n",
    " ('keep', 0.9732257723808289),\n",
    " ('voice', 0.9731258153915405),\n",
    " ('found', 0.9730350375175476),\n",
    " ('three', 0.9728507399559021),\n",
    " ('made', 0.9726716876029968),\n",
    " ('him', 0.9724807739257812),\n",
    " ('seen', 0.9724376201629639),\n",
    " ('tell', 0.9724341034889221),\n",
    " ('rabbit', 0.9724060893058777)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46041a2f",
   "metadata": {},
   "source": [
    "# Our own CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b632af20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pytorch to train word2vec model using CBOW\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * 2 * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = torch.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = torch.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "sentences = ['I love to eat ice cream',\n",
    "    'The ice cream is delicious',\n",
    "    'Ice cream is my favorite',\n",
    "    'Ice is very cold',\n",
    "    'South Africa is the house of various animals',\n",
    "    'The desert is very hot']\n",
    "\n",
    "\n",
    "vocab = set()\n",
    "for sentence in sentences:\n",
    "    for word in sentence.split():\n",
    "        vocab.add(word)\n",
    "\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "\n",
    "print(\"word_to_idx\", word_to_idx)\n",
    "print(\"idx_to_word\", idx_to_word)\n",
    "\n",
    "context_size = 2\n",
    "embedding_dim = 10\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "model = CBOW(vocab_size, embedding_dim, context_size)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        cbows = generate_cbow(sentence, context_size)\n",
    "        for cbow in cbows:\n",
    "            context, target = cbow\n",
    "            # print(\"context\", context, \"target\", target)\n",
    "            context_idxs = torch.tensor([word_to_idx[w] for w in context], dtype=torch.long)\n",
    "\n",
    "            model.zero_grad()\n",
    "            log_probs = model(context_idxs)\n",
    "            loss = loss_function(log_probs, torch.tensor([word_to_idx[target]], dtype=torch.long))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    losses.append(total_loss)\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch: {}, Loss: {:.4f}\".format(epoch + 1, total_loss))\n",
    "\n",
    "\n",
    "# Predict\n",
    "def predict(context):\n",
    "    print(\"context:\", context)\n",
    "    context_idxs = torch.tensor([word_to_idx[w] for w in context], dtype=torch.long)\n",
    "    log_probs = model(context_idxs)\n",
    "    _, predicted = torch.max(log_probs, 1)\n",
    "    print(\"predicted:\", idx_to_word[predicted.item()])\n",
    "\n",
    "predict(['I', 'love', 'eat', 'ice'])\n",
    "predict(['The', 'ice', 'cream', 'delicious'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32928b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx {'of': 0, 'the': 1, 'love': 2, 'eat': 3, 'is': 4, 'favorite': 5, 'ice': 6, 'cream': 7, 'South': 8, 'my': 9, 'Ice': 10, 'cold': 11, 'various': 12, 'I': 13, 'animals': 14, 'desert': 15, 'The': 16, 'hot': 17, 'very': 18, 'delicious': 19, 'Africa': 20, 'house': 21, 'to': 22}\n",
    "idx_to_word {0: 'of', 1: 'the', 2: 'love', 3: 'eat', 4: 'is', 5: 'favorite', 6: 'ice', 7: 'cream', 8: 'South', 9: 'my', 10: 'Ice', 11: 'cold', 12: 'various', 13: 'I', 14: 'animals', 15: 'desert', 16: 'The', 17: 'hot', 18: 'very', 19: 'delicious', 20: 'Africa', 21: 'house', 22: 'to'}\n",
    "Epoch: 1, Loss: 29.1734\n",
    "Epoch: 11, Loss: 26.6193\n",
    "Epoch: 21, Loss: 24.1955\n",
    "Epoch: 31, Loss: 21.8820\n",
    "Epoch: 41, Loss: 19.6662\n",
    "Epoch: 51, Loss: 17.5518\n",
    "Epoch: 61, Loss: 15.5483\n",
    "Epoch: 71, Loss: 13.6769\n",
    "Epoch: 81, Loss: 11.9517\n",
    "Epoch: 91, Loss: 10.3868\n",
    "context: ['I', 'love', 'eat', 'ice']\n",
    "predicted: to\n",
    "context: ['The', 'ice', 'cream', 'delicious']\n",
    "predicted: cream"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
