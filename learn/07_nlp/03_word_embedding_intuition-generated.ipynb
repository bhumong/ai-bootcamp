{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5950cc2e",
   "metadata": {},
   "source": [
    "source: [link](https://ai-bootcamp.ruangguru.com/learn/07_nlp/03_word_embedding_intuition.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619f0419",
   "metadata": {},
   "source": [
    "# Second architecture: Using Word Embedding for sentiment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25652e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e97b6c9",
   "metadata": {},
   "source": [
    "The goal of learning with this second architecture is to understand how word embedding can create semantic relations between word. We’ll use RNN architecture later to utilize word embedding to the fullest. From the intuition to the math, understanding word embedding concept and basic RNN architecture hopefully can cater your thirst on how can a model really understand a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e8c111",
   "metadata": {},
   "source": [
    "# Word Embedding: Every word has their own data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3301813e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ead51eef",
   "metadata": {},
   "source": [
    "![Image](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/image_fCEXzZbEY.png?updatedAt=1695050596556)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1301fe2",
   "metadata": {},
   "source": [
    "We have already learned above intuition that word embedding is like “scorecard” for every single word. Utilizing word embedding is really about understanding that every single words can contain their own information, whether it’s about gender, about their grammatical rules, about their meaning, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce9b152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Download and load the word embedding\n",
    "#@markdown We separated the download and the loading of the word embedding so you can execute below visualization, similarity calculation, etc faster without having to keep redownloading\n",
    "import os\n",
    "import numpy as np\n",
    "import requests, zipfile, io\n",
    "\n",
    "def download_and_unzip_embeddings(url, directory):\n",
    "    print(f'Downloading and unzipping embeddings...')\n",
    "    r = requests.get(url)\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall(path=directory)\n",
    "\n",
    "def load_glove_embeddings(path, url):\n",
    "    # If file doesn't exist, download and unzip it\n",
    "    if not os.path.isfile(path):\n",
    "        download_and_unzip_embeddings(url, path.rsplit('/', 1)[0])\n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "        embeddings = {}\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "        return embeddings\n",
    "\n",
    "# URL of GloVe embeddings and the path - replace with your actual URL\n",
    "url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "path = 'glove.6B/glove.6B.300d.txt'\n",
    "\n",
    "embeddings = load_glove_embeddings(path, url)\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def load_glove_model(glove_input_file):\n",
    "    glove_model = KeyedVectors.load_word2vec_format(glove_input_file, binary=False)\n",
    "    return glove_model\n",
    "\n",
    "# Convert the GloVe file format to word2vec file format\n",
    "glove_input_file = 'glove.6B/glove.6B.50d.txt'\n",
    "word2vec_output_file = 'glove.6B/glove.6B.50d.txt.word2vec'\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "model = load_glove_model(word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594baad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeprecationWarning:\n",
    "\n",
    "Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8490e469",
   "metadata": {},
   "source": [
    "# Scatterchart for words relation in GloVe word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591a7642",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30ffe56c",
   "metadata": {},
   "source": [
    "![Image](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/image_CiBKpRWIm.png?updatedAt=1695567726941)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6335ce8",
   "metadata": {},
   "source": [
    "One of the most popular word embedding dictionary is GloVe, Global Vectors for word representation. GloVe is basically an existing dictionary of word embeddings in English that collecting hundreds of thousands to millions of vocabularies and map every single one of them to their dedicated matrix of word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10750fd",
   "metadata": {},
   "source": [
    "# NLP and words: The problem of finding relations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7188cabf",
   "metadata": {},
   "source": [
    "We’ll talk more about how a word can be attached to a context later when we’re learning how to generate a word embedding, but in broad sense a word embedding is generated by learning word relations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0d1fd0",
   "metadata": {},
   "source": [
    "For now let’s see below scatterplot to see a single word and what words that GloVe learned that related to that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b597317",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Scatterplot of word relations\n",
    "word = \"king\" #@param\n",
    "find_nearest = 50\n",
    "\n",
    "# The most_similar method and extraction of word vectors is not mentioned.\n",
    "# You'll have to implement this yourself or use an API like gensim or Spacy that provides this functionality.\n",
    "\n",
    "\n",
    "result = model.most_similar(word, topn=find_nearest)\n",
    "word_labels = [word for word, similarity in result]\n",
    "similarity_scores = [similarity for word, similarity in result]\n",
    "word_labels.append(word)\n",
    "word_vectors = model[word_labels]\n",
    "\n",
    "# Below part of the code assumes word_labels and word_vectors are correctly fetched and prepared.\n",
    "tsne = TSNE(n_components=2)\n",
    "reduced_vectors = tsne.fit_transform(word_vectors)\n",
    "\n",
    "df = pd.DataFrame(reduced_vectors, columns=[\"tsne1\", \"tsne2\"])\n",
    "df['word'] = word_labels\n",
    "df['is_input_word'] = (df['word'] == word)\n",
    "\n",
    "df_input_word = df[df['is_input_word']]\n",
    "df_similar_words = df[~df['is_input_word']]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_similar_words[\"tsne1\"],\n",
    "    y=df_similar_words[\"tsne2\"],\n",
    "    mode='markers+text',\n",
    "    marker=dict(\n",
    "        size=8,\n",
    "        color='rgba(152, 0, 0, .8)',\n",
    "    ),\n",
    "    text=df_similar_words['word'],\n",
    "    textposition='top center',\n",
    "    name='Similar words'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_input_word[\"tsne1\"],\n",
    "    y=df_input_word[\"tsne2\"],\n",
    "    mode='markers+text',\n",
    "    marker=dict(\n",
    "        size=12,\n",
    "        color='rgba(0, 152, 0, .8)',\n",
    "    ),\n",
    "    text=df_input_word['word'],\n",
    "    textposition='top center',\n",
    "    name='Input word'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=f'2D visualization of word embeddings for \"{word}\" and its similar words',\n",
    "    xaxis=dict(title='t-SNE 1'),\n",
    "    yaxis=dict(title='t-SNE 2'))\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Similarity Bar chart only for similar words\n",
    "fig2 = go.Figure(data=[\n",
    "    go.Bar(x=word_labels[:find_nearest], y=similarity_scores,\n",
    "           text=similarity_scores, textposition='auto')\n",
    "])\n",
    "\n",
    "fig2.update_layout(\n",
    "    title_text=f'Bar chart showing similarity scores of top {find_nearest} words similar to {word}',  # 'words' is changed to 'word'\n",
    "    xaxis=dict(title='Words'),\n",
    "    yaxis=dict(title='Similarity Score'))\n",
    "\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56cb987",
   "metadata": {},
   "source": [
    "As you can see above word “king”, GloVe word embedding create relations from that word to other words that relate to that word:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039ef89a",
   "metadata": {},
   "source": [
    "And so on. Of course it’s hard to really pin point what exactly the context that GloVe understand for a single word, why exactly does GloVe “think” that a word is related to another word because GloVe word embeddings are created by a neural network - which means that a word embedding is mostly lack of explainability in their creation of relations, we can only guess."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43098477",
   "metadata": {},
   "source": [
    "# Similarity in context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24097a8",
   "metadata": {},
   "source": [
    "Before we continue, one term that you should know is that when we check how a word related to another word, the term is mostly referred to “similarity”. Similarity means how similar that the given word to the context for another word, in our given context on NLP similarity mostly means:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c0764",
   "metadata": {},
   "source": [
    "Of course we’ll dive in further into the concept later in their dedicated section. Reminder that we use data from Wikipedia to generate word embedding, more text to gather from can give better context and this lack of source can weakened the word embedding accuracy on giving context to each words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca10468",
   "metadata": {},
   "source": [
    "# Multiple words relation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baa6c36",
   "metadata": {},
   "source": [
    "We can also try to visualize the word relation for several words at once, so we can know what GloVe word embedding thinks how some words relate to the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e6bbd9",
   "metadata": {},
   "source": [
    "You can try below demonstration and feel free to play with the input. You might notice that you can add negative words as well to make sure some words won’t be included to your plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf96899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Multiple words similarity\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "words_str = \"dog,cat\"  # @param\n",
    "neg_words_str = \"man\"  # @param\n",
    "find_nearest = 50\n",
    "\n",
    "# Parse array from comma-delimited string\n",
    "words = words_str.split(',')\n",
    "neg_words = neg_words_str.split(',')\n",
    "\n",
    "# Filter out empty strings\n",
    "words = [word for word in words if word]\n",
    "neg_words = [word for word in neg_words if word]\n",
    "\n",
    "# Use the most_similar method to get the top 'find_nearest' words that are most similar to your list of words\n",
    "result_positive = model.most_similar(positive=words, topn=find_nearest, negative=neg_words)\n",
    "\n",
    "word_labels = [word for word, similarity in result_positive]\n",
    "similarity_scores = [similarity for word, similarity in result_positive]\n",
    "\n",
    "# Extend labels and vectors for positive results\n",
    "word_labels.extend(words)\n",
    "\n",
    "# Extract vectors for words\n",
    "word_vectors = model[word_labels]\n",
    "\n",
    "# Reduce dimensionality for visualization\n",
    "tsne = TSNE(n_components=2)\n",
    "reduced_vectors = tsne.fit_transform(word_vectors)\n",
    "\n",
    "# Prepare DataFrame\n",
    "df = pd.DataFrame(reduced_vectors, columns=[\"tsne1\", \"tsne2\"])\n",
    "df['word'] = word_labels\n",
    "df['is_input_word'] = df['word'].isin(words)\n",
    "\n",
    "df_input_word = df[df['is_input_word']]\n",
    "df_similar_words = df[~df['is_input_word']]\n",
    "\n",
    "# Word embedding scatter plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Similar words\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_similar_words[\"tsne1\"],\n",
    "    y=df_similar_words[\"tsne2\"],\n",
    "    mode='markers+text',\n",
    "    marker=dict(\n",
    "        size=8,\n",
    "        color='rgba(152, 0, 0, .8)',\n",
    "    ),\n",
    "    text=df_similar_words['word'],\n",
    "    textposition='top center',\n",
    "    name='Similar words'\n",
    "))\n",
    "\n",
    "# Input words\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_input_word[\"tsne1\"],\n",
    "    y=df_input_word[\"tsne2\"],\n",
    "    mode='markers+text',\n",
    "    marker=dict(\n",
    "        size=12,\n",
    "        color='rgba(0, 152, 0, .8)',\n",
    "    ),\n",
    "    text=df_input_word['word'],\n",
    "    textposition='top center',\n",
    "    name='Input words'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=f'2D visualization of word embeddings for {words} and their most similar words',\n",
    "    xaxis=dict(title='t-SNE 1'),\n",
    "    yaxis=dict(title='t-SNE 2'))\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Similarity Bar chart only for similar words\n",
    "fig2 = go.Figure(data=[\n",
    "    go.Bar(x=word_labels[:find_nearest], y=similarity_scores,\n",
    "           text=similarity_scores, textposition='auto')\n",
    "])\n",
    "\n",
    "fig2.update_layout(\n",
    "    title_text=f'Bar chart showing similarity scores of top {find_nearest} words similar to {words}',\n",
    "    xaxis=dict(title='Words'),\n",
    "    yaxis=dict(title='Similarity Score'))\n",
    "\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c033674f",
   "metadata": {},
   "source": [
    "# Seeing how several words is related"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc1b486",
   "metadata": {},
   "source": [
    "Another way to visualize a word embedding is to check the strengthness of a word relation to several other words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c641e65b",
   "metadata": {},
   "source": [
    "Say you want to know whether a queen is more related to the word “he” or “she”, you can check the strengthness using below form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d183c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Strengthness comparison\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "main_word_str = \"he\"  # @param\n",
    "related_words_str = \"queen,king,husband,wife,brother,sister,father,mother\"  # @param\n",
    "\n",
    "# Parse array from comma-delimited string\n",
    "main_word = [main_word_str]\n",
    "related_words = related_words_str.split(',')\n",
    "\n",
    "# Get vectors for all words\n",
    "main_word_vector = model[main_word]\n",
    "related_word_vectors = [model[word] for word in related_words]\n",
    "\n",
    "# Calculate pairwise cosine similarities\n",
    "similarities = [cosine_similarity(main_word_vector.reshape(1, -1), vec.reshape(1, -1))[0][0]\n",
    "                for vec in related_word_vectors]\n",
    "\n",
    "# Plot a bar chart\n",
    "fig = go.Figure([go.Bar(x=related_words, y=similarities)])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Relationship Strength Between \"{main_word_str}\" and Related Words',\n",
    "    xaxis=dict(title='Related Words'),\n",
    "    yaxis=dict(title='Cosine Similarity with Main Word'),\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=500,\n",
    "    margin=dict(l=50, r=50, b=100, t=100, pad=4)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e4e5cc",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f5d9da",
   "metadata": {},
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd7aff4",
   "metadata": {},
   "source": [
    "Remember that in reality word embedding’s dimension is a lot larger than 2 dimension (our above glove word embedding have 100 dimensions per word), what we did above is called dimentionality reduction using T-SNA. Dimensionality reduction can be simplified for the intuition using below analogy:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3e1527",
   "metadata": {},
   "source": [
    "We live in 3 dimension world, we often “reduce the dimension” of what we see in nature by taking a photo (the photo is 2 dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c80917c",
   "metadata": {},
   "source": [
    "Above analogy can help you to understand that: - We can capture higher dimension vector to lower dimension - We will lose lots of information along the way. 2 dimension can’t clearly provide context missing from the our real world such as depth, we can’t see what is behind objects on our photo, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb8191c",
   "metadata": {},
   "source": [
    "# Reducing dimensions == removing context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e18c538",
   "metadata": {},
   "source": [
    "Remember previous intuition that when we do embedding each dimension is embedded with different kind of context? If we reduce any dimension like previous scatter plot we’ll find that lots of context will be missing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03c6bea",
   "metadata": {},
   "source": [
    "# Curse of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8b7796",
   "metadata": {},
   "source": [
    "The problem in the finding similar data is not as simple as finding your key that were lost in your room. Come curse of dimensionality:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15816b7b",
   "metadata": {},
   "source": [
    "Imagine you’re playing hide-and-seek with your friend in a long straight hallway with doors on either side. Each door leads to a room. Although it might take some time, you have only one direction to go – you can walk one way, then back the other way to check each room systematically. This is the equivalent to a problem of one dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f7d52c",
   "metadata": {},
   "source": [
    "Now, imagine if your friend could be hiding in any room in a single floor of a building, but the floor has a maze of lots of directions to go, not a single straight hallway anymore. Now you have more places to potentially look for your friend because the hiding space is wider and longer. You are dealing with two dimensions in this case (length and width of the building)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d1dcfc",
   "metadata": {},
   "source": [
    "Let’s go a step further. Your friend could be in any room of a massive multi-storey building with numerous rooms on each floor. Now, your friend has a lot more possible hiding spots because you’re not only searching across the length and width of the building, but also high and low up the multiple floors. This is an example of a problem with three dimensions (length, width, and height of the building)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c47d5d",
   "metadata": {},
   "source": [
    "The curse of dimensionality creates a complex problem when we want to search for similarity because as you can see above, adding dimension adding multitude of complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da50de54",
   "metadata": {},
   "source": [
    "This is the reason if you click play on our visualizations above, the value that are near your requested query keeps changing: We (data scientist) found some ways to search similarity quickly, but it’s almost impossible to really know if it’s really the nearest - we just guessing that it’s the most likely to be the most similar, but the computational resource to ensure that it’s really the nearest neighbor is expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038a8388",
   "metadata": {},
   "source": [
    "This concept is one of the reason why when we talk to ChatGPT we might have different answers from the same query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb771694",
   "metadata": {},
   "source": [
    "# Context-aware embedding vs static embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa09482",
   "metadata": {},
   "source": [
    "The last thing that we’ll learn right now for the intuition on word embedding is the difference of context-aware embeddings and static embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdac98e9",
   "metadata": {},
   "source": [
    "Context-aware embedding is a word embedding that’s generated per sentence input, for example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30796be",
   "metadata": {},
   "source": [
    "“I love this bat, because its grip is perfect for my baseball match.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b504aa51",
   "metadata": {},
   "source": [
    "A “bat” can be referred to many context. A static word embedding might be able to contain several context of a “bat”, but context-aware embedding is focused on understanding the whole input, the whole sentence first then giving word embedding per word that is focused on that sentence, so the “bat” here will refer to a baseball bat, and the model won’t consider other context for “bat”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e24de3f",
   "metadata": {},
   "source": [
    "The example for static word embedding is GloVe, which we already learned. And the example for context-aware embedding is BERT, which we’ll dive in further in it’s dedicated section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdde6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Context-aware embedding demo\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased', use_fast=True)\n",
    "bert_model = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def embed_text(text):\n",
    "    # Encode text\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    # Compute token embeddings\n",
    "    outputs = bert_model(**inputs)\n",
    "    # Retrieve the embeddings of the mean tokens\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    return embeddings\n",
    "\n",
    "def calculate_similarity(embedding_1, embedding_2):\n",
    "    # Flatten the embeddings to 1D before comparing using cosine distance\n",
    "    return 1 - cosine(embedding_1.flatten(), embedding_2.flatten())\n",
    "\n",
    "def plot_comparison(word, compare_1, compare_2, similarity_1, similarity_2):\n",
    "    fig = go.Figure(data=[\n",
    "        go.Bar(name=compare_1, x=[word], y=[similarity_1]),\n",
    "        go.Bar(name=compare_2, x=[word], y=[similarity_2])\n",
    "    ])\n",
    "    # Customize aspect\n",
    "    fig.update_layout(barmode='group')\n",
    "    fig.show()\n",
    "\n",
    "# Your inputs\n",
    "#@markdown The text input\n",
    "text = \"I love this bat, because its grip is perfect for my baseball match.\" #@param\n",
    "#@markdown the word to compare\n",
    "word = \"bat\" #@param\n",
    "#@markdown context to compare\n",
    "compare_1 = \"animal bat\"#@param\n",
    "compare_2 = \"baseball bat\"#@param\n",
    "\n",
    "word_embedding = embed_text(text + \" \" + word)\n",
    "compare_1_embedding = embed_text(compare_1)\n",
    "compare_2_embedding = embed_text(compare_2)\n",
    "\n",
    "similarity_1 = calculate_similarity(word_embedding, compare_1_embedding)\n",
    "similarity_2 = calculate_similarity(word_embedding, compare_2_embedding)\n",
    "\n",
    "plot_comparison(word, compare_1, compare_2, similarity_1, similarity_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd815b88",
   "metadata": {},
   "source": [
    "# Up next: Understanding the math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ea8c36",
   "metadata": {},
   "source": [
    "We’ve already learned how basic machine learning model that doesn’t use deep learning understand how to classify a sentence, and we’re learning the lack of “context” understanding when we’re not using neural netwok. Then we’re learning about the intuition of how word embedding works."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
