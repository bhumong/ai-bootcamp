{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "782c3e8f",
   "metadata": {},
   "source": [
    "source: [link](https://ai-bootcamp.ruangguru.com/learn/07_nlp/09_understanding_different_architectures.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3041b00",
   "metadata": {},
   "source": [
    "# Understanding Different Transformer Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a19562c",
   "metadata": {},
   "source": [
    "One of the unique aspects of the Transformer architecture is that it has different variants. In this notebook, we will explore the different variants of the Transformer architecture and how they differ from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e680375",
   "metadata": {},
   "source": [
    "# Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8189f6e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2feb4024",
   "metadata": {},
   "source": [
    "![Image](https://storage.googleapis.com/rg-ai-bootcamp/nlp/image-24.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb218dd3",
   "metadata": {},
   "source": [
    "Let’s review our understanding in encoder-decoder architecture from RNN, as it will help our understanding how it’s influenced in the Transformer architecture and how can we better understand later when we split the encoder and decoder into different parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec5625c",
   "metadata": {},
   "source": [
    "# The essence of Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51cee65",
   "metadata": {},
   "source": [
    "Encoder is essentially the layer of understanding the input sequence, it’s converting the input sequence into a vector that’s meaningful, understand pretty well what’s the input is about, what the writer intend to, and then giving it’s undertanding by writing it into a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f6cfb6",
   "metadata": {},
   "source": [
    "# The essence of Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21437023",
   "metadata": {},
   "source": [
    "Decoder is essentially the layer of generating the output sequence, it’s converting the vector that was written by the encoder into a sequence of output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1941e7f",
   "metadata": {},
   "source": [
    "# Encoder-Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9ebef1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4a86f2e",
   "metadata": {},
   "source": [
    "![Image](https://storage.googleapis.com/rg-ai-bootcamp/nlp/image-7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedde86f",
   "metadata": {},
   "source": [
    "When we classify the Transformer architecture into the encoder and decoder, we can see that from above images is that the encoder is the one on the left, and the decoder is the one on the right. In decoder we can see an additional component called the “Encoder decoder’s Attention”, which is the one that’s responsible for bringing the encoder’s and decoder’s attention together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aa744a",
   "metadata": {},
   "source": [
    "# Encoder’s attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd5aaad",
   "metadata": {},
   "source": [
    "If we review our understanding of attention, it’s basically: If you’re currently looking at me, what other words in the sentence that you should be looking also? Because other words can hint you about the contexts that you should know about me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc9beb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizerFast, RobertaModel\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "# Load pre-trained model (weights)\n",
    "model = RobertaModel.from_pretrained('deepset/tinyroberta-squad2', output_attentions=True)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('deepset/tinyroberta-squad2')\n",
    "\n",
    "def visualize_self_attention_in_context(sentence_A, sentence_B, target_word):\n",
    "    # Tokenize sentences\n",
    "    inputs = tokenizer.encode_plus(sentence_A, sentence_B, return_tensors='pt', add_special_tokens=True)\n",
    "    input_ids = inputs['input_ids'].to('cpu')\n",
    "\n",
    "    # Model forward pass\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "    # Attention from all layers and all heads\n",
    "    attentions_layers = np.array([layer[0, :, :, :].detach().numpy() for layer in outputs.attentions])\n",
    "\n",
    "    # Mean attention across all heads for each layer\n",
    "    attentions_heads_mean = attentions_layers.mean(axis=1)\n",
    "\n",
    "    # Mean attention across all layers\n",
    "    attentions_all_mean = attentions_heads_mean.mean(axis=0)\n",
    "\n",
    "    token_list = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    sep_index = token_list.index('</s>')\n",
    "\n",
    "    # Focusing on self-attention within sentence_A (offsetting indices for </s> and <s> tags)\n",
    "    attentions_all_mean_modified = attentions_all_mean[1:sep_index, 1:sep_index]\n",
    "\n",
    "    # Normalize attention scores for each word\n",
    "    attentions_all_mean_modified = (attentions_all_mean_modified - attentions_all_mean_modified.min(axis=1, keepdims=True)) / \\\n",
    "                                   (attentions_all_mean_modified.max(axis=1, keepdims=True) - attentions_all_mean_modified.min(axis=1, keepdims=True))\n",
    "\n",
    "    token_list_A_modified = [t.lstrip('Ġ') for t in token_list[1:sep_index]]\n",
    "\n",
    "    # Extract index of the target word\n",
    "    try:\n",
    "        target_index = token_list_A_modified.index(target_word)\n",
    "    except ValueError:\n",
    "        print(f\"Target word '{target_word}' not found. Please ensure it is part of sentence_A.\")\n",
    "        return\n",
    "\n",
    "    # Selecting attention weights for the target word\n",
    "    target_word_attentions = attentions_all_mean_modified[target_index, :]\n",
    "\n",
    "    # The code below visualizes attention distribution using seaborn as a heat map.\n",
    "    plt.figure(figsize=(12, 2))\n",
    "    sns.heatmap(np.expand_dims(target_word_attentions, axis=0),\n",
    "                xticklabels=token_list_A_modified,\n",
    "                yticklabels=[target_word],\n",
    "                linewidths=0.1,\n",
    "                cmap='coolwarm')\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "sentence_A = \"I love swimming, I love coding, and my name is Imam\"\n",
    "sentence_B = \"what's my name?\"\n",
    "target_word = \"name\"\n",
    "visualize_self_attention_in_context(sentence_A, sentence_B, target_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb90bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Some weights of the model checkpoint at deepset/tinyroberta-squad2 were not used when initializing RobertaModel: ['qa_outputs.bias', 'qa_outputs.weight']\n",
    "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "Some weights of RobertaModel were not initialized from the model checkpoint at deepset/tinyroberta-squad2 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50316fd5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f629ac9",
   "metadata": {},
   "source": [
    "As you can see above, when we’re trying to check the attention of the word “name”, we can see that we’re being hinted at the word “my”, “is”, and “Imam”. We can see that if we want to get the whole context of the word “name”, these words are basically the ones that we should be looking at."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0744ef75",
   "metadata": {},
   "source": [
    "So basically the encoder layer is the one that’s responsible to create this relationship between the words in the sentence, that later we’ll pass to the decoder layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00501c91",
   "metadata": {},
   "source": [
    "# Encoder encode every word at the same time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d2e93d",
   "metadata": {},
   "source": [
    "One thing to note is that in the encoder layer, we’re encoding every word at the same time, so when we use the same example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9980867a",
   "metadata": {},
   "source": [
    "I love swimming, I love coding, and my name is Imam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c897aea3",
   "metadata": {},
   "source": [
    "We’re encoding the word “I” at the same time as the word “love”, and the word “love” at the same time as the word “swimming”, and so on. This is different from the RNN architecture, where we’re encoding the word “I” first, then the word “love”, and so on. This is one of the reason why the Transformer architecture is faster than the RNN architecture, because we’re encoding every word at the same time, enabling us to parallelize the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae2dd3a",
   "metadata": {},
   "source": [
    "# Decoder’s regressive property"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b010e10",
   "metadata": {},
   "source": [
    "The difference between the encoder and decoder is that the decoder is regressive, meaning that it’s generating the output sequence one word at a time, from left to right. It’s much like the RNN architecture, where we’re generating the output sequence one word at a time, the last word will be the input for the next word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a708b69",
   "metadata": {},
   "source": [
    "# Decoder’s attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71ff97f",
   "metadata": {},
   "source": [
    "Decoder’s attention follow the regressive property, meaning that attention scoring are generated from left to right. The idea of the decoder’s attention is simple, based on what we’ve generated so far, what other words in the output sequence that might give hint to the next word that we’re going to generate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2592e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\", output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c30a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_self_attention_in_context(english_sentence, german_translation_part, target_word):\n",
    "    # Encode input and target sentences\n",
    "    input_ids = tokenizer.encode(\"translate English to German: \"+english_sentence, return_tensors='pt')\n",
    "    decoder_input_ids = tokenizer.encode(german_translation_part, return_tensors='pt')\n",
    "\n",
    "    # Model forward pass\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "\n",
    "    # Attention from all layers and all heads\n",
    "    attentions_layers = np.array([layer[0, :, :, :].detach().numpy() for layer in outputs.decoder_attentions])\n",
    "\n",
    "    # Mean attention across all heads for each layer\n",
    "    attentions_heads_mean = attentions_layers.mean(axis=1)\n",
    "\n",
    "    # Mean attention across all layers\n",
    "    attentions_all_mean = attentions_heads_mean.mean(axis=0)\n",
    "\n",
    "    token_list = tokenizer.convert_ids_to_tokens(decoder_input_ids[0])\n",
    "\n",
    "    # Normalize attention scores for each word\n",
    "    attentions_all_mean = (attentions_all_mean - attentions_all_mean.min(axis=1, keepdims=True)) / \\\n",
    "                          (attentions_all_mean.max(axis=1, keepdims=True) - attentions_all_mean.min(axis=1, keepdims=True))\n",
    "\n",
    "    # Strip the special characters added by the tokenizer\n",
    "    token_list = [t.lstrip('Ġ') for t in token_list]\n",
    "\n",
    "# Exclude </s> from the encoder tokens and attentions\n",
    "    if '</s>' in token_list:\n",
    "        end_index = token_list.index('</s>')\n",
    "        token_list = token_list[:end_index]\n",
    "        attentions_all_mean = attentions_all_mean[:, :end_index]\n",
    "\n",
    "    # Find all indices of the target word\n",
    "    target_indices = [i for i, token in enumerate(token_list) if target_word in token]\n",
    "\n",
    "    # Ensure there's at least one occurrence of the target word\n",
    "    if not target_indices:\n",
    "        print(f\"Target word '{target_word}' not found. Please ensure it is part of german_translation_part.\")\n",
    "        return\n",
    "\n",
    "    # Selecting attention weights for the target word\n",
    "    target_word_attentions = np.mean([attentions_all_mean[i, :] for i in target_indices], axis=0)\n",
    "\n",
    "    print(\"Tokenized word list:\")\n",
    "    print([t.replace('▁', '') for t in token_list])\n",
    "\n",
    "    # The code below visualizes attention distribution using seaborn as a heat map.\n",
    "    plt.figure(figsize=(12, 2))\n",
    "    sns.heatmap(np.expand_dims(target_word_attentions, axis=0),\n",
    "                xticklabels=[t.rstrip('_') for t in token_list],\n",
    "                yticklabels=[target_word],\n",
    "                linewidths=0.1,\n",
    "                cmap='coolwarm')\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "english_sentence = \"I love swimming, I love coding, and my name is Imam\"\n",
    "german_translation_part = \"Ich liebe Schwimmen, ich liebe Codierung, und mein Name ist\"\n",
    "target_word = \"ist\"\n",
    "visualize_self_attention_in_context(english_sentence, german_translation_part, target_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d5beab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenized word list:\n",
    "['Ich', 'liebe', 'Schwimm', 'en', ',', '', 'ich', 'liebe', 'Cod', 'ierung', ',', 'und', 'mein', 'Name', 'ist']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8103134a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d664ffc",
   "metadata": {},
   "source": [
    "As you can see above this sentence:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b86bba",
   "metadata": {},
   "source": [
    "I love swimming, I love coding, and my name is Imam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9cfc3a",
   "metadata": {},
   "source": [
    "Is already translated into this sentence:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4368e234",
   "metadata": {},
   "source": [
    "Ich liebe Schwimmen, ich liebe Codierung, und mein Name ist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb21923",
   "metadata": {},
   "source": [
    "But we’re still missing one last translation, the name “Imam”. So above chart you can see that the model giving hint that the next word should be highly related to “Name”, “ist”, but the “mein”, and “und” might not be that needed, but can still give you hint, so it’s lower in score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f075ad2",
   "metadata": {},
   "source": [
    "So basically the decoder attention is the one that’s responsible to give hints of what might be needed to generate the next word in the output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5022717",
   "metadata": {},
   "source": [
    "# Encoder-decoder attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b69daf",
   "metadata": {},
   "source": [
    "Encoder-decoder attention purpose is aligning the encoder’s and decoder’s attention together, so we can get hints what the output should be. Remember again:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb1361b",
   "metadata": {},
   "source": [
    "So decoder has basically know what kind of the word should be in the next word, and then encoder-decoder attention will help to check from the encoder’s attention, what word should be next, with hints given by the decoder’s attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abbd7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('t5-small', output_attentions=True)\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-small')\n",
    "\n",
    "def visualize_cross_attention_in_context(english_sentence, german_translation_part, target_word):\n",
    "    # Encode input and target sentences\n",
    "    input_text = \"translate English to German: \" + english_sentence\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    decoder_input_ids = tokenizer.encode(german_translation_part, return_tensors='pt')\n",
    "\n",
    "    # Model forward pass\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "\n",
    "    # Cross-attention from all layers and all heads\n",
    "    cross_attentions_layers = np.array([layer[0, :, :, :].detach().numpy() for layer in outputs.cross_attentions])\n",
    "\n",
    "    # Mean cross-attention across all heads for each layer\n",
    "    cross_attentions_heads_mean = cross_attentions_layers.mean(axis=1)\n",
    "\n",
    "    # Mean cross-attention across all layers\n",
    "    cross_attentions_all_mean = cross_attentions_heads_mean.mean(axis=0)\n",
    "\n",
    "    encoder_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    decoder_tokens = tokenizer.convert_ids_to_tokens(decoder_input_ids[0])\n",
    "\n",
    "    task_prefix_token_count = len(tokenizer.encode(\"translate English to German: \", return_tensors='pt')[0]) \n",
    "    # Reduce tokens and attentions by the number of tokens in the task prefix\n",
    "    encoder_tokens = encoder_tokens[task_prefix_token_count:]\n",
    "    cross_attentions_all_mean = cross_attentions_all_mean[:, task_prefix_token_count:]\n",
    "\n",
    "    # Exclude </s> from the encoder tokens and attentions\n",
    "    if '</s>' in encoder_tokens:\n",
    "        end_index = encoder_tokens.index('</s>')\n",
    "        encoder_tokens = encoder_tokens[:end_index]\n",
    "        cross_attentions_all_mean = cross_attentions_all_mean[:, :end_index]\n",
    "\n",
    "    # Find all indices of the target word in the German sentence\n",
    "    target_indices = [i for i, token in enumerate(decoder_tokens) if target_word in token]\n",
    "\n",
    "    if not target_indices:\n",
    "        print(f\"Target word '{target_word}' not found. Please make sure it is part of german_translation_part.\")\n",
    "        return\n",
    "\n",
    "    # Selecting attention weights for the target word\n",
    "    target_word_attentions = np.mean([cross_attentions_all_mean[i, :] for i in target_indices], axis=0)\n",
    "\n",
    "    print(\"Tokenized word list:\")\n",
    "    print([t.replace('▁', '') for t in decoder_tokens])\n",
    "\n",
    "    # Visualize cross-attention distribution using seaborn as a heat map\n",
    "    plt.figure(figsize=(12, 2))\n",
    "    sns.heatmap(np.expand_dims(target_word_attentions, axis=0),\n",
    "                xticklabels=encoder_tokens,\n",
    "                yticklabels=[target_word],\n",
    "                linewidths=0.1,\n",
    "                cmap='coolwarm')\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "english_sentence = \"I love swimming, I love coding, and my name is Imam\"\n",
    "german_translation_part = \"Ich liebe Schwimmen, ich liebe Codierung, und mein\"\n",
    "target_word = \"mein\"\n",
    "visualize_cross_attention_in_context(english_sentence, german_translation_part, target_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8b9058",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenized word list:\n",
    "['Ich', 'liebe', 'Schwimm', 'en', ',', '', 'ich', 'liebe', 'Cod', 'ierung', ',', 'und', 'mein', '</s>']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d26297",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08ec303e",
   "metadata": {},
   "source": [
    "Ich liebe Schwimmen, ich liebe Codierung, und mein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef797916",
   "metadata": {},
   "source": [
    "As you can see above we just translated until “mein”, and on above chart we can see that the decoder’s attention is giving hint that the next word should be highly related to the English word “name”, and “my”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f26d58",
   "metadata": {},
   "source": [
    "# Encoder Only Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f574ca3c",
   "metadata": {},
   "source": [
    "As we already told above, the encoder is the one that’s responsible to understand the input sequence. This layer is the one who manage to create relation on the input, and in effect, the layer who really understand the input sequence is this layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dfb951",
   "metadata": {},
   "source": [
    "Most of the time if we’re using this encoder only architecture, we’re using it for task that highly need on understanding the input sequence, understand the relation between the words in the input sequence. Some example of the task that we can use this encoder only architecture is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d40874",
   "metadata": {},
   "source": [
    "# Decoder Only Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2a5fca",
   "metadata": {},
   "source": [
    "The decoder only is more tricky to understand. This layer is mostly used in a text generation task, where we’re generating the output sequence one word at a time. This layer basically has no way to “understand” the input sequence, and it’s only relying on the decoder’s attention to give hints on what the next word should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67280bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained model (weights) & tokenizer\n",
    "model = GPT2Model.from_pretrained('gpt2', output_attentions=True)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "def visualize_self_attention_in_context(sentence):\n",
    "    # Model forward pass\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(sentence, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids)\n",
    "    \n",
    "    # Extract the attention values\n",
    "    attentions_layers = np.array([layer[0, :, :, :].detach().numpy() for layer in outputs.attentions])\n",
    "    \n",
    "    # Compute the mean attention values across all heads for each layer\n",
    "    attentions_heads_mean = attentions_layers.mean(axis=1)\n",
    "\n",
    "    # Compute the mean attention values across all layers\n",
    "    attentions_all_mean = attentions_heads_mean.mean(axis=0)\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    # Compute attention values for the last token (the latest prediction)\n",
    "    attention_for_last_token = attentions_all_mean[-1, :]\n",
    "    \n",
    "    # Print out the tokens \n",
    "    print(\"Sentence tokenized:\")\n",
    "    print(tokens)\n",
    "\n",
    "    # Visualize using seaborn\n",
    "    plt.figure(figsize=(12, 2))\n",
    "    sns.heatmap(np.expand_dims(attention_for_last_token, axis=0), xticklabels=tokens, cmap='coolwarm')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Usage\n",
    "sentence = \"After several meeting with the client, the team finally decided to accept the\"\n",
    "visualize_self_attention_in_context(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28576725",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentence tokenized:\n",
    "['After', 'Ġseveral', 'Ġmeeting', 'Ġwith', 'Ġthe', 'Ġclient', ',', 'Ġthe', 'Ġteam', 'Ġfinally', 'Ġdecided', 'Ġto', 'Ġaccept', 'Ġthe']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6668375d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07dca5ec",
   "metadata": {},
   "source": [
    "On above example we see the visualization of the attention score of above GPT-2 model that it think will help the later fully connected layer to generate the next word. For GPT-2, for some reason it gives a hight attention score to the word “After” or any word that become the first word of the input. It’s likely to make sure the model later understand the need to know the first word by default before caring any other words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63473750",
   "metadata": {},
   "source": [
    "Let’s try to remove the first word just for the sake of understanding what other words will be given high attention score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488a1f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained model (weights) & tokenizer\n",
    "model = GPT2Model.from_pretrained('gpt2', output_attentions=True)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "def visualize_self_attention_in_context(sentence):\n",
    "    # Model forward pass\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(sentence, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids)\n",
    "    \n",
    "    # Extract the attention values\n",
    "    attentions_layers = np.array([layer[0, :, :, :].detach().numpy() for layer in outputs.attentions])\n",
    "    \n",
    "    # Compute the mean attention values across all heads for each layer\n",
    "    attentions_heads_mean = attentions_layers.mean(axis=1)\n",
    "\n",
    "    # Compute the mean attention values across all layers\n",
    "    attentions_all_mean = attentions_heads_mean.mean(axis=0)\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    # Compute attention values for the last token (the latest prediction)\n",
    "    attention_for_last_token = attentions_all_mean[-1, :]\n",
    "    \n",
    "    # Remove the attention to the first word and its corresponding token\n",
    "    attention_for_last_token = attention_for_last_token[1:]\n",
    "    tokens = tokens[1:]\n",
    "    \n",
    "    # Print out the tokens \n",
    "    print(\"Sentence tokenized:\")\n",
    "    print(tokens)\n",
    "\n",
    "    # Visualize using seaborn\n",
    "    plt.figure(figsize=(12, 2))\n",
    "    sns.heatmap(np.expand_dims(attention_for_last_token, axis=0), xticklabels=tokens, cmap='coolwarm')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Usage\n",
    "sentence = \"After several hours of discussion with the client, the team finally decided to accept the\"\n",
    "visualize_self_attention_in_context(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd64742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentence tokenized:\n",
    "['Ġseveral', 'Ġhours', 'Ġof', 'Ġdiscussion', 'Ġwith', 'Ġthe', 'Ġclient', ',', 'Ġthe', 'Ġteam', 'Ġfinally', 'Ġdecided', 'Ġto', 'Ġaccept', 'Ġthe']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8653ce28",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb09c5c7",
   "metadata": {},
   "source": [
    "We’ll see that the word “accept”, “the”, “to”, and “client” are given high attention score. So we can see that just by using this, without having the need of “understanding” the input sequence (encoder), we might still be able to generate a good output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619555f2",
   "metadata": {},
   "source": [
    "Like what’s told, this decoder only architecture is mostly used in text generation task, where we’re generating the output without having the need to understand the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390533e4",
   "metadata": {},
   "source": [
    "But in reality using something like GPT-3, there is a realization that we can still use this decoder only architecture to do some task classification or other task that seemingly need to understand the input sequence, as long as we’re using a big enough model, and the task is prepared in a structure of text generation task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c283e9b",
   "metadata": {},
   "source": [
    "Even though technically using decoder-only architecture, the score might not be as good as using architecture that’s using encoder, or even if it comes close it can be highly expensive to train or use the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac34a26",
   "metadata": {},
   "source": [
    "# Additional Materials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5d513f",
   "metadata": {},
   "source": [
    "Youtube Videos - Coding a Transformer from scratch on PyTorch, with full explanation, training and inference. - Illustrated Guide to Transformers Neural Network: A step by step explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e87ffb",
   "metadata": {},
   "source": [
    "Paper - Attention is all you need - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding - Language Models are Few-Shot Learners, Paper GPT-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6141e36",
   "metadata": {},
   "source": [
    "Excalidraw - Excalidraw Link"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
