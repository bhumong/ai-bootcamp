{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b06f04a",
   "metadata": {},
   "source": [
    "source: [link](https://ai-bootcamp.ruangguru.com/learn/07_nlp/08_attention.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c8fe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastbook\n",
    "from fastbook import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27cc9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bertviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4864dd2",
   "metadata": {},
   "source": [
    "Disclaimer: today’s lecture is HARD, we tried to make it as simple as possible, but it’s still hard 🙏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224368fe",
   "metadata": {},
   "source": [
    "# Problem With RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9221a0a2",
   "metadata": {},
   "source": [
    "Before we go on to learn about attention, let’s first understand the problem with RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba403ba7",
   "metadata": {},
   "source": [
    "# Recurrence can’t be parallelized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6303325d",
   "metadata": {},
   "source": [
    "RNNs are sequential models. They process the input one token at a time. This means that the next token can only be processed after the previous token has been processed. This makes it difficult to parallelize the training of RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a22189",
   "metadata": {},
   "source": [
    "Consider the following recurrent function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996b1939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_length(words: list) -> int:\n",
    "    if len(words) == 0:\n",
    "        return 0\n",
    "    return len(words[0]) + count_length(words[1:])\n",
    "\n",
    "count_length(['I', 'am', 'a', 'student'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54e5453",
   "metadata": {},
   "outputs": [],
   "source": [
    "11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc54a56",
   "metadata": {},
   "source": [
    "To calculate count_length(['I', 'am', 'a', 'student']) we need to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f221ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "gv('''\n",
    "    rankdir=\"TB\";\n",
    "    a [label=\"count_length(['I', 'am', 'a', 'student'])\"]\n",
    "    b [label=\"count_length(['am', 'a', 'student'])\"]\n",
    "    c [label=\"count_length(['a', 'student'])\"]\n",
    "    d [label=\"count_length(['student'])\"]\n",
    "    e [label=\"count_length([])\"]\n",
    "    a -> b [label=\"len('I') + \"]\n",
    "    b -> c [label=\"len('am') + \"]\n",
    "    c -> d [label=\"len('a') + \"]\n",
    "    d -> e [label=\"len('student') + \"]\n",
    "   ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df314efe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7eddd43f",
   "metadata": {},
   "source": [
    "We need to compute them sequentially, one after the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1655d1",
   "metadata": {},
   "source": [
    "Compare it to this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdf6245",
   "metadata": {},
   "outputs": [],
   "source": [
    "gv('''\n",
    "    rankdir=\"TB\";\n",
    "    a [label=\"count_length(['I', 'am', 'a', 'student'])\"]\n",
    "    b [label=\"count_length(['I'])\"]\n",
    "    c [label=\"count_length(['am'])\"]\n",
    "    d [label=\"count_length(['a'])\"]\n",
    "    e [label=\"count_length(['student'])\"]\n",
    "    a -> b\n",
    "    a -> c\n",
    "    a -> d\n",
    "    a -> e\n",
    "    b -> sum\n",
    "    c -> sum\n",
    "    d -> sum\n",
    "    e -> sum\n",
    "   ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab4d242",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abdedff8",
   "metadata": {},
   "source": [
    "The second function can run in parallel. We can calculate len('I'), len('am'), len('a'), and len('student') at the same time. We can then add them up and return the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aed0570",
   "metadata": {},
   "source": [
    "Let’s take a look at RNN diagram:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d808bdc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e35e3e2",
   "metadata": {},
   "source": [
    "![Image](https://storage.googleapis.com/rg-ai-bootcamp/nlp/image-24.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f63387",
   "metadata": {},
   "source": [
    "Does it ressemble the first function or the second? The answer is the first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e5be33",
   "metadata": {},
   "source": [
    "The output of the RNN at time step $t$ is dependent on the output of the RNN at time step $t-1$. This means that we can’t calculate the output of the RNN at time step $t$ until we have calculated the output of the RNN at time step $t-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f697aa50",
   "metadata": {},
   "source": [
    "# Vanishing Gradient and Exploding Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab6d1e7",
   "metadata": {},
   "source": [
    "Another problem with RNN is that it suffers from vanishing gradient. This is because the gradient is multiplied by the same weight matrix at each time step. If the weight matrix is small, the gradient will vanish. If the weight matrix is large, the gradient will explode. This problem, in effect, will have the effect of the model “forgetting” the first few words in the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5624a153",
   "metadata": {},
   "source": [
    "So if we have sentence like below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5d9a2d",
   "metadata": {},
   "source": [
    "> Do not eat that cake because it is poisonous and you will die if you eat that cake, so remember to be careful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b71a54",
   "metadata": {},
   "source": [
    "Do not eat that cake because it is poisonous and you will die if you eat that cake, so remember to be careful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa6832a",
   "metadata": {},
   "source": [
    "If we have a model that suffers from vanishing gradient, the important part such as “Do not” might be forgotten by the model, end up with “eat that cake” as the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b691e7d7",
   "metadata": {},
   "source": [
    "Another effect of vanishing gradient is that the model will have a hard time to learn long term dependency, making training the model to take longer time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239f5984",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15604655",
   "metadata": {},
   "source": [
    "Remember CNN architecture we learned earlier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfda2c7a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfd458f5",
   "metadata": {},
   "source": [
    "![Image](https://upload.wikimedia.org/wikipedia/commons/6/63/Typical_cnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4863a4",
   "metadata": {},
   "source": [
    "Compare it with RNN architecture. What do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f29634",
   "metadata": {},
   "source": [
    "CNN can be parallelized! We can calculate the function on different pixel at the same time!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a2d1be",
   "metadata": {},
   "source": [
    "Can we take this idea and apply it to RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7749a23",
   "metadata": {},
   "source": [
    "# Attention is all you need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788e09f2",
   "metadata": {},
   "source": [
    "We’ll learn about transformer, a model that made a breakthrough in the field of NLP, that we’ve been using in several lectures back. This model first introduced in the paper Attention is all you need in 2017. The name of the paper seems like a clickbait at first, but there are two reasons why this paper is named like that:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27f2ab6",
   "metadata": {},
   "source": [
    "Attention is a concept that has been around for a long time, even has it’s usage in RNN. It’s basically a way to weight a part of the input sequence, so that the model can focus on that part of the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd233c6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fa5223c",
   "metadata": {},
   "source": [
    "![Image](https://eleks.com/wp-content/uploads/neural-machine-translation-with-attention-mechanism.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99cff38",
   "metadata": {},
   "source": [
    "> Image source: https://eleks.com/research/neural-machine-translation-attention-mechanism/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21787300",
   "metadata": {},
   "source": [
    "Image source: https://eleks.com/research/neural-machine-translation-attention-mechanism/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1271c62b",
   "metadata": {},
   "source": [
    "In RNN, the concept of attention is rely on a decoder only focused on a part of the input sequence when want to output in a single time step. What does it mean is that some of the context will have more influence on the output than the others. For example when we want to continue below sentence:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e6f820",
   "metadata": {},
   "source": [
    "> I went to the __"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77247433",
   "metadata": {},
   "source": [
    "I went to the __"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a799010",
   "metadata": {},
   "source": [
    "The concept of attention will make the model to focus on certain words on the input sequence that will influence the output more than the others. For example, if the model focus on the blank space above, the attention will make the model to focus on the word “went” and “to” more than the word “I”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5b2bee",
   "metadata": {},
   "source": [
    "This is because the word “I” doesn’t really influence the output, as if we see that input, we can see that we need more words related to a place rather to a person to solve the next word. So making sure that the model focus on the word “went” and “to” will make the model to output a word related to a place, rather than a person."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe0de7e",
   "metadata": {},
   "source": [
    "This concept of attention is done in RNN with one of the reason being to solve the problem of vanishing gradient. By making the model to focus only on certain part of the input sequence, the model will not have to learn the whole input sequence, thus making the model to learn faster, and what’s being put into the hidden state will be more effective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9389107",
   "metadata": {},
   "source": [
    "Let’s use the following sentence as an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1a80b7",
   "metadata": {},
   "source": [
    "> “Father’s passport was stolen, so he went to the police station to get a new one.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ec04e7",
   "metadata": {},
   "source": [
    "“Father’s passport was stolen, so he went to the police station to get a new one.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f8006a",
   "metadata": {},
   "source": [
    "What we want: - “he” to have a high attention weight to “Father” - “passport” to have a high attention weight to “stolen” - “new one” to have a high attention weight to “passport” - “went” to have a high attention weight to “police station”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7376b133",
   "metadata": {},
   "source": [
    "How do we do that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d732791f",
   "metadata": {},
   "source": [
    "# Basic Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c672ce",
   "metadata": {},
   "source": [
    "We have learned about embedding before. And we also learned that if two tokens are related, they will have similar embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113039d1",
   "metadata": {},
   "source": [
    "Can we use this to calculate the attention weight? Let’s try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c26a4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of GloVe embeddings and the path - replace with your actual URL\n",
    "url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "path = 'glove.6B.300d.txt'\n",
    "\n",
    "# download the url above if not exist\n",
    "import os\n",
    "import urllib.request\n",
    "if not os.path.exists(path):\n",
    "    import urllib.request\n",
    "    urllib.request.urlretrieve(url, path)\n",
    "\n",
    "# load into GenSim\n",
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(path, binary=False, no_header=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc419d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "<gensim.models.keyedvectors.KeyedVectors at 0x115560c10>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab443616",
   "metadata": {},
   "source": [
    "Let’s print the similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdcfbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's remove stop words and punctuation so that we don't need to deal with them here\n",
    "\n",
    "sentence = \"father passport was stolen so he went to the police station to get a new one\"\n",
    "words = sentence.split(' ')\n",
    "\n",
    "for i in range(len(words)):\n",
    "    similarity_score = {}\n",
    "    for j in range(len(words)):\n",
    "        if i != j:\n",
    "            similarity_score[words[j]] = model.similarity(words[i], words[j])\n",
    "    # print similarity sorted by score take only the top 3\n",
    "    print(words[i], '=>', sorted(similarity_score.items(), key=lambda kv: kv[1], reverse=True)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dfe9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "father => [('he', 0.57918), ('was', 0.4167939), ('went', 0.41016555)]\n",
    "passport => [('stolen', 0.35170314), ('get', 0.22569308), ('police', 0.22211868)]\n",
    "was => [('he', 0.61911285), ('the', 0.5451479), ('so', 0.5369364)]\n",
    "stolen => [('passport', 0.35170314), ('police', 0.32043472), ('he', 0.27722472)]\n",
    "so => [('get', 0.7669047), ('he', 0.6482584), ('one', 0.63450015)]\n",
    "he => [('so', 0.6482584), ('was', 0.61911285), ('went', 0.6081544)]\n",
    "went => [('he', 0.6081544), ('one', 0.49178118), ('so', 0.4860344)]\n",
    "to => [('to', 1.0), ('get', 0.58743733), ('so', 0.57337403)]\n",
    "the => [('one', 0.6245334), ('was', 0.5451479), ('so', 0.54477024)]\n",
    "police => [('one', 0.3614448), ('he', 0.3481394), ('station', 0.3467765)]\n",
    "station => [('police', 0.3467765), ('one', 0.326971), ('was', 0.32232344)]\n",
    "to => [('to', 1.0), ('get', 0.58743733), ('so', 0.57337403)]\n",
    "get => [('so', 0.7669047), ('to', 0.58743733), ('one', 0.5550698)]\n",
    "a => [('one', 0.60356975), ('the', 0.5241736), ('was', 0.51922)]\n",
    "new => [('the', 0.5271288), ('one', 0.47153175), ('to', 0.4701864)]\n",
    "one => [('so', 0.63450015), ('the', 0.6245334), ('a', 0.60356975)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ce111d",
   "metadata": {},
   "source": [
    "Cool! Do we have the attention weight now?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cd18ae",
   "metadata": {},
   "source": [
    "> “Father’s passport was stolen, so he went to the police station to get a new one.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc6277",
   "metadata": {},
   "source": [
    "“Father’s passport was stolen, so he went to the police station to get a new one.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cae68a",
   "metadata": {},
   "source": [
    "- “passport” is correctly related to “stolen”\n",
    "- “father” is correctly related to “he”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902a3551",
   "metadata": {},
   "source": [
    "But what about “new one” and “passport”? They are not related at all!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710f92ee",
   "metadata": {},
   "source": [
    "# Better Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7a507c",
   "metadata": {},
   "source": [
    "Before explaining how, let’s try to use RoBERTa to calculate the attention weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025c957e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizerFast, RobertaModel\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "# Load pre-trained model (weights)\n",
    "model = RobertaModel.from_pretrained('deepset/tinyroberta-squad2', output_attentions=True)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('deepset/tinyroberta-squad2')\n",
    "\n",
    "def visualize_self_attention_in_context(sentence_A, target_word):\n",
    "    # Tokenize sentences\n",
    "    inputs = tokenizer.encode_plus(sentence_A, return_tensors='pt', add_special_tokens=True)\n",
    "    input_ids = inputs['input_ids'].to('cpu')\n",
    "\n",
    "    # Model forward pass\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "    # Attention from all layers and all heads\n",
    "    attentions_layers = np.array([layer[0, :, :, :].detach().numpy() for layer in outputs.attentions])\n",
    "\n",
    "    # Mean attention across all heads for each layer\n",
    "    attentions_heads_mean = attentions_layers.mean(axis=1)\n",
    "\n",
    "    # Mean attention across all layers\n",
    "    attentions_all_mean = attentions_heads_mean.mean(axis=0)\n",
    "\n",
    "    token_list = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    sep_index = token_list.index('</s>')\n",
    "\n",
    "    # Focusing on self-attention within sentence_A (offsetting indices for </s> and <s> tags)\n",
    "    attentions_all_mean_modified = attentions_all_mean[1:sep_index, 1:sep_index]\n",
    "\n",
    "    # Normalize attention scores for each word\n",
    "    attentions_all_mean_modified = (attentions_all_mean_modified - attentions_all_mean_modified.min(axis=1, keepdims=True)) / \\\n",
    "                                   (attentions_all_mean_modified.max(axis=1, keepdims=True) - attentions_all_mean_modified.min(axis=1, keepdims=True))\n",
    "\n",
    "    token_list_A_modified = [t.lstrip('Ġ') for t in token_list[1:sep_index]]\n",
    "\n",
    "    # Extract index of the target word\n",
    "    try:\n",
    "        target_index = token_list_A_modified.index(target_word)\n",
    "    except ValueError:\n",
    "        print(f\"Target word '{target_word}' not found. Please ensure it is part of sentence_A.\")\n",
    "        return\n",
    "\n",
    "    # Selecting attention weights for the target word\n",
    "    target_word_attentions = attentions_all_mean_modified[target_index, :]\n",
    "\n",
    "    # The code below visualizes attention distribution using seaborn as a heat map.\n",
    "    plt.figure(figsize=(12, 2))\n",
    "    sns.heatmap(np.expand_dims(target_word_attentions, axis=0),\n",
    "                xticklabels=token_list_A_modified,\n",
    "                yticklabels=[target_word],\n",
    "                linewidths=0.1,\n",
    "                cmap='coolwarm')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aae976",
   "metadata": {},
   "outputs": [],
   "source": [
    "Some weights of the model checkpoint at deepset/tinyroberta-squad2 were not used when initializing RobertaModel: ['qa_outputs.weight', 'qa_outputs.bias']\n",
    "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "Some weights of RobertaModel were not initialized from the model checkpoint at deepset/tinyroberta-squad2 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4b0011",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Father's passport was stolen, so he went to the police station to get a new one\"\n",
    "target_word = \"he\"\n",
    "visualize_self_attention_in_context(sentence, target_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d27e2f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91186b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Father's passport was stolen, so he went to the police station to get a new one\"\n",
    "target_word = \"stolen\"\n",
    "visualize_self_attention_in_context(sentence, target_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89492160",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b863f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Father's passport was stolen, so he went to the police station to get a new one\"\n",
    "target_word = \"get\"\n",
    "visualize_self_attention_in_context(sentence, target_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79531c36",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "109dc91b",
   "metadata": {},
   "source": [
    "So, how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ead2ce",
   "metadata": {},
   "source": [
    "# Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7721af93",
   "metadata": {},
   "source": [
    "Self attention works by calculating the attention weight of each token in the input sequence to every other token in the input sequence. This means that the attention weight of each token will be calculated to every other token in the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0ace0f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "145adb0b",
   "metadata": {},
   "source": [
    "![Image](https://peterbloem.nl/files/transformers/self-attention.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4f6f6a",
   "metadata": {},
   "source": [
    "Source: Peterbloem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6ae0f9",
   "metadata": {},
   "source": [
    "$x_1$ is embedding vector of the first token in the input sequence. $x_2$ is embedding vector of the second token in the input sequence. And soon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49af614",
   "metadata": {},
   "source": [
    "While $y_2$ is the attention weight of the second token in the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb34afcd",
   "metadata": {},
   "source": [
    "In the above diagram, to get the value of $y_2$ we need to calculate: \n",
    "\\[w_{21} = x_2^T \\cdot x_1\\]\n",
    " \n",
    "\\[w_{22} = x_2^T \\cdot x_2\\]\n",
    " \n",
    "\\[w_{23} = x_2^T \\cdot x_3\\]\n",
    " \n",
    "\\[w_{24} = x_2^T \\cdot x_4\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bb0d85",
   "metadata": {},
   "source": [
    "Then sum them up: \n",
    "\\[y_2 = w_{21} + w_{22} + w_{23} + w_{24}\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fe8057",
   "metadata": {},
   "source": [
    "Let’s take a look at $w_{21}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4f2cb3",
   "metadata": {},
   "source": [
    "$w_{21}$ is the dot product of the embedding vector of the second token and the embedding vector of the first token. This is similar to our first naive approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dda0cea",
   "metadata": {},
   "source": [
    "Then those score is multiplied by $x_1$. So the more closely related $x_1$ and $x_2$ are, the higher the affect of $x_1$ to $y_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009400ea",
   "metadata": {},
   "source": [
    "Then we do the same thing for $x_3$ and $x_4$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4183dd86",
   "metadata": {},
   "source": [
    "# Weight Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e2e39d",
   "metadata": {},
   "source": [
    "To make the model able to adapt to the context of a sentence, weight matrixes were introduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c247ef4b",
   "metadata": {},
   "source": [
    "So, the weight matrix make it possible for the model to learn different relation between tokens in different context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ddf4c7",
   "metadata": {},
   "source": [
    "e.g."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dfe828",
   "metadata": {},
   "source": [
    "> I hate soccer because it is boring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174c0822",
   "metadata": {},
   "source": [
    "I hate soccer because it is boring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27d850c",
   "metadata": {},
   "source": [
    "The weight matrix allows the model to learn that “it” refers to “soccer” in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cad2489",
   "metadata": {},
   "source": [
    "To be able to do that, Transformer needs 3 weight matrixes, $W_Q$, $W_K$, and $W_V$. What are they?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d777fa8d",
   "metadata": {},
   "source": [
    "# Key, Query, Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63960d58",
   "metadata": {},
   "source": [
    "> “The cat sat on the mat.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc70dd6a",
   "metadata": {},
   "source": [
    "“The cat sat on the mat.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b18689",
   "metadata": {},
   "source": [
    "Imagine we’re processing the word “sat” and trying to determine its context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6b0b36",
   "metadata": {},
   "source": [
    "- Query (Q): This would be generated from the word “sat”. The Query is like asking a question: “Which words in the sentence are most relevant to me (the word ‘sat’)?”\n",
    "- Keys (K): These are generated for every word in the sentence. They serve as unique “labels” or “identifiers” for each word. In this context, the Keys for “The”, “cat”, “on”, “the”, and “mat” will each provide a means to match how relevant they are to the Query.\n",
    "- Values (V): These are also generated for every word in the sentence. You can think of them as the actual “content” or “information” from each word. Once the model determines which words are most relevant to the Query using the Keys, it will fetch the corresponding Values to produce the output representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41edd711",
   "metadata": {},
   "source": [
    "Query (Q): This would be generated from the word “sat”. The Query is like asking a question: “Which words in the sentence are most relevant to me (the word ‘sat’)?”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb4e07",
   "metadata": {},
   "source": [
    "Keys (K): These are generated for every word in the sentence. They serve as unique “labels” or “identifiers” for each word. In this context, the Keys for “The”, “cat”, “on”, “the”, and “mat” will each provide a means to match how relevant they are to the Query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0e3108",
   "metadata": {},
   "source": [
    "Values (V): These are also generated for every word in the sentence. You can think of them as the actual “content” or “information” from each word. Once the model determines which words are most relevant to the Query using the Keys, it will fetch the corresponding Values to produce the output representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2446358",
   "metadata": {},
   "source": [
    "Those representation are represented by different weight matrixes: $W_Q$, $W_K$, and $W_V$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cdd896",
   "metadata": {},
   "source": [
    "Those weight matrixes are multiplied by the embedding vector of each token to get the query, key, and value of each token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bbd691",
   "metadata": {},
   "source": [
    "\n",
    "\\[q = W_Q \\cdot x\\]\n",
    " \n",
    "\\[k = W_K \\cdot x\\]\n",
    " \n",
    "\\[v = W_V \\cdot x\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b769de9b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd19b517",
   "metadata": {},
   "source": [
    "![Image](https://peterbloem.nl/files/transformers/key-query-value.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4216a053",
   "metadata": {},
   "source": [
    "Source: Peterbloem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba074bd",
   "metadata": {},
   "source": [
    "Please compare the above diagram with the previous one. The only difference is that now we have 3 weight matrixes, $W_Q$, $W_K$, and $W_V$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5b7d95",
   "metadata": {},
   "source": [
    "# Multi Head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de69477e",
   "metadata": {},
   "source": [
    "> Cat hates dog because it is scary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a782514",
   "metadata": {},
   "source": [
    "Cat hates dog because it is scary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc212c7",
   "metadata": {},
   "source": [
    "The weight matrix allows the model to learn that “it” refers to “dog” in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2b66b2",
   "metadata": {},
   "source": [
    "… but what if we want to learn that “it” refers to “cat” in this context?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47be0e40",
   "metadata": {},
   "source": [
    "We can’t do that with only one weight matrix. We need multiple weight matrixes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0c0017",
   "metadata": {},
   "source": [
    "This is where multi head attention comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c7b411",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8d48bf5",
   "metadata": {},
   "source": [
    "![Image](https://peterbloem.nl/files/transformers/multi-head.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22935b82",
   "metadata": {},
   "source": [
    "Source: Peterbloem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbc64d3",
   "metadata": {},
   "source": [
    "In the multi head attention, we have multiple weight matrixes, $W_Q$, $W_K$, and $W_V$. Each of them is called a head. And each head will learn different relation between tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe85089",
   "metadata": {},
   "source": [
    "To visualize multi head attention, please run this Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e55c5e4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8b2963d",
   "metadata": {},
   "source": [
    "![Image](https://storage.googleapis.com/rg-ai-bootcamp/nlp/image-29.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eaa610",
   "metadata": {},
   "source": [
    "Source: Attention is All you Need paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd0f1ff",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c430748b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21c1ad58",
   "metadata": {},
   "source": [
    "![Image](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8f/The-Transformer-model-architecture.png/600px-The-Transformer-model-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66be54d",
   "metadata": {},
   "source": [
    "The self attention layer doesn’t take into account the position of each token in the input sequence. This is a problem because the position of each token in the input sequence is important. For example, in the sentence “I went to the store”, the word “I” is the subject, while the word “store” is the object. If we change the position of the word “I” and “store”, the meaning of the sentence will change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6c8fe4",
   "metadata": {},
   "source": [
    "To solve this problem, we need to add positional encoding to the input sequence. This is done by adding a positional encoding vector to the embedding vector of each token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0af2bd",
   "metadata": {},
   "source": [
    "The formula is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62287c66",
   "metadata": {},
   "source": [
    "\n",
    "\\[PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})\\]\n",
    " \n",
    "\\[PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dc0f66",
   "metadata": {},
   "source": [
    "Where $pos$ is the position of the token in the input sequence, $i$ is the dimension of the embedding vector, and $d_{model}$ is the dimension of the embedding vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01c1a6b",
   "metadata": {},
   "source": [
    "It’s a bit complicated, but the idea is that we want to make sure that the positional encoding vector is different for each position. We do this by using a sine and cosine function with different frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf9833d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbe45118",
   "metadata": {},
   "source": [
    "![Image](https://storage.googleapis.com/rg-ai-bootcamp/nlp/image-28.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6524e36d",
   "metadata": {},
   "source": [
    "Source Coding a Transformer from scratch on PyTorch, with full explanation, training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af37cdd",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7ddb56",
   "metadata": {},
   "source": [
    "Let’s revisit the architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae02ddb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51409ba1",
   "metadata": {},
   "source": [
    "![Image](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8f/The-Transformer-model-architecture.png/600px-The-Transformer-model-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a81cb10",
   "metadata": {},
   "source": [
    "The decoder is similar to the encoder, but with one difference: the decoder has an extra self attention layer that takes the output of the encoder as input: $W_K$ and $W_V$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d1e109",
   "metadata": {},
   "source": [
    "The idea is that, the learned $W_K$ and $W_V$ is passed to the decoder, so that the decoder can learn the relation between the input sequence and the output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297322fd",
   "metadata": {},
   "source": [
    "# Linear Layer & Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8ab923",
   "metadata": {},
   "source": [
    "The final layer is a linear layer and a softmax layer. The linear layer is used to convert the output of the decoder to the same dimension as the vocabulary size. The softmax layer is used to convert the output of the linear layer to a probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddabbd6",
   "metadata": {},
   "source": [
    "e.g."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db04f82d",
   "metadata": {},
   "source": [
    "> “I went to the __”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e292b3e7",
   "metadata": {},
   "source": [
    "“I went to the __”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dc02fd",
   "metadata": {},
   "source": [
    "The output of the decoder will be a probability distribution of the next token in the output sequence. e.g. “store”, “park”, “school”, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd11a27",
   "metadata": {},
   "source": [
    "$"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
