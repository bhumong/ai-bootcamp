{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f51a37da",
   "metadata": {},
   "source": [
    "source: [link](https://ai-bootcamp.ruangguru.com/learn/07_nlp/01_preprocess.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017524a1",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525dfefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d8f39f",
   "metadata": {},
   "source": [
    "# Overview for our first architecture - NLP Without Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4dfe1d",
   "metadata": {},
   "source": [
    "For our first architecture we‚Äôll learn how to do sentiment classification using ‚ÄúNaive Bayes‚Äù. We‚Äôll delve in several key concepts of NLP that will help us later understand more complex architecture like Seq2Seq and Transformers such as Preprocessing, Word Embedding, tokenization, and more!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cc1804",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c3209d6",
   "metadata": {},
   "source": [
    "![Image](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/image_R2iHDwSe_.png?updatedAt=1695078679500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b74c9dc",
   "metadata": {},
   "source": [
    "As you can see on above diagram, we‚Äôll learn the steps of asking, converting that question to numbers, and then making sure our model understand that question. For modern architecture the concept of understanding is mostly using an encoder layer, but for methods like ‚ÄúNaive Bayes‚Äù it‚Äôs kinda like encoder layer, but much more traditional than that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e6c564",
   "metadata": {},
   "source": [
    "Note: ‚ÄúNaive Bayes‚Äù method will be covered in the next lesson. Look forward to it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed14efa",
   "metadata": {},
   "source": [
    "# Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ea5f11",
   "metadata": {},
   "source": [
    "One of the task that we can use for NLP without neural network is ‚ÄúText classification‚Äù. This task is as simple as it sound: What current input should be classified to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5703198e",
   "metadata": {},
   "source": [
    "For today we‚Äôll learn how to classify a tweet if it can be count as positive tweet or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d522de1",
   "metadata": {},
   "source": [
    "Imagine this tweet:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746937db",
   "metadata": {},
   "source": [
    "I‚Äôm really excited towards tomorrow for our shop opening, see you guys at xxx!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7ae41a",
   "metadata": {},
   "source": [
    "We as a human can know from above tweet that the person who tweeted currently being positive (being excited, being happy), and so the conclusion is that above tweet is considered as a ‚Äúpositive tweet‚Äù."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9a62a0",
   "metadata": {},
   "source": [
    "So in our first architecture we‚Äôll learn how we can conclude a tweet is either positive or negative by checking every word and see if there are any hints that tweet have either positive, or negative sentiment. For above tweet the hint would be the word ‚ÄúExcited‚Äù."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420e5cfa",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396a959f",
   "metadata": {},
   "source": [
    "For our learning to classify tweets into it‚Äôs sentiment, we will use this dataset https://www.kaggle.com/datasets/ferno2/training1600000processednoemoticoncsv. It‚Äôs a dataset of 1,6 million of tweets that‚Äôs already classified as either positive tweet for negative tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60fb62d",
   "metadata": {},
   "source": [
    "# Preprocessing - Cleaning noises, and consolidating words - Human part before we input to the machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcc0adf",
   "metadata": {},
   "source": [
    "One of the place where human can ‚Äúhelp‚Äù the machine learning model is in preprocessing. One of the task that are done in preprocessing is to make sure that our model won‚Äôt be distracted by several things that we as a human might figure out that the model shouldn‚Äôt care about, and transform several things to make sure our model can works better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c961c9",
   "metadata": {},
   "source": [
    "When we‚Äôre working on classification (especially when working with model but without neural network), we should consider our goal when we‚Äôre looking at our dataset: What words do our model really need to consider when classifying our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d03eae6",
   "metadata": {},
   "source": [
    "As for NLP using something like naive bayes mostly depends on understanding if a tweet contain certain words that can help it‚Äôs understanding if the tweet is either positive or negative. So there are basicallly two things that we should do before feeding our input to our model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8878ba",
   "metadata": {},
   "source": [
    "# Removing noises, which is words or characters that shouldn‚Äôt give any effect in our classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec6f3f",
   "metadata": {},
   "source": [
    "Example: üòÉ Super excited to share my latest article! @OpenAI üëÄüëâ http://ai.newpost.com #AI #OpenAI üòé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd864ae",
   "metadata": {},
   "source": [
    "If we‚Äôre currently doing sentiment classification we might not need to include urls, mentions, hashtags, etc. If we include those into our model, our model might hint those noises as something that geared the tweet sentiment towards either positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d146eeb0",
   "metadata": {},
   "source": [
    "Another example for sentiment classification tasks is removing stopwords. Stopwords are words that occur so frequently in sentences that they contain little meaningful information. Examples of common stopwords in the English language include: ‚Äúthe‚Äù, ‚Äúis‚Äù, ‚Äúat‚Äù, ‚Äúwhich‚Äù, ‚Äúon‚Äù."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8d26fd",
   "metadata": {},
   "source": [
    "Other things that we might considering removing is symbols like ‚Äú?‚Äù, ‚Äú!‚Äù, etc.as -at least when we‚Äôre not using neural network- understanding sentiment from symbols might be cout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a73ab20",
   "metadata": {},
   "source": [
    "# Consolidating words that have similar meaning, by removing their tenses, plurality, prefix, suffix, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec12e0a5",
   "metadata": {},
   "source": [
    "Words like ‚ÄúExciting‚Äù is consolidated with ‚Äúexcited‚Äù, ‚Äúexcitement‚Äù, ‚Äúexcite‚Äù, etc. so we can consider words that have the same root (‚ÄúExciting‚Äù, ‚ÄúExcited‚Äù, ‚ÄúExcite‚Äù, are have the same root word: ‚ÄúExcite‚Äù) to be processed together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965df61e",
   "metadata": {},
   "source": [
    "Another thing we might consider is to lowercasing so ‚ÄúExciting‚Äù, ‚Äúexciting‚Äù, and ‚ÄúEXCITING‚Äù can be considered the same so our model won‚Äôt differentiate between those three words when learning the sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db770212",
   "metadata": {},
   "source": [
    "# Let‚Äôs remove all noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb47a7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Remove noises\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    hashtag_pattern = re.compile(r'#\\S+')\n",
    "    return hashtag_pattern.sub(r'', text)\n",
    "\n",
    "def remove_mentions(text):\n",
    "    mention_pattern = re.compile(r'@\\S+')\n",
    "    return mention_pattern.sub(r'', text)\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_symbols(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def preprocess_sentence(text):\n",
    "    text = remove_urls(text)\n",
    "    text = remove_hashtags(text)\n",
    "    text = remove_mentions(text)\n",
    "    text = remove_emojis(text)\n",
    "    text = remove_symbols(text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Example usage:\n",
    "text = \"Hey @user, check out the webpage: https://example.com. I found it awesome! üòé #exciting\" # @param {text: \"string\"}\n",
    "print(preprocess_sentence(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0c92fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hey  check out the webpage  I found it awesome  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f087a29a",
   "metadata": {},
   "source": [
    "# Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb546d9e",
   "metadata": {},
   "source": [
    "When consolidating words that have the same root, there are two strategies that can be used: Stemming and Lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbfaa2a",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa19ebe",
   "metadata": {},
   "source": [
    "Exciting, excited. Happy, happiness. Sad, sadden, sadness. Worrying, worried, worry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7356bba",
   "metadata": {},
   "source": [
    "The way stemming handle words consolidating is by removing the suffixes (and sometimes prefixes) of the words, leaving only the word ‚Äústem‚Äù (the part of the word that is common to all its inflected variants). It‚Äôs easier to learn by example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8547c1",
   "metadata": {},
   "source": [
    "exciting -> excit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d7fb6e",
   "metadata": {},
   "source": [
    "The unique thing about stemming is that it reduce to several characters that are unique to other words, but sometimes it doesn‚Äôt really ‚Äúmake sense‚Äù in the meaning of the word. As long as it can manage to group several pattern of the same words as one, lots of task can be enough to use this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e1b15",
   "metadata": {},
   "source": [
    "went != go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38579dff",
   "metadata": {},
   "source": [
    "It‚Äôs only caring about reducing the words to the most basic letter that unique from other words, not caring to their synonyms, tenses, or likewise. For example ‚Äúwent‚Äù and ‚Äúgo‚Äù would be different in stem even though ‚Äúwent‚Äù is just a past tense of ‚Äúgo‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df46dcb8",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538a51d3",
   "metadata": {},
   "source": [
    "Lemmatization is different from stemming such that it emphasizes a heavy consideration for grammar rules in its approach. While both methodologies aim to reduce words to their base or root form, lemmatization performs this task by taking into account the morphological analysis of the words. This means that it understands the context and proper grammatical elements such as verb tenses, plural forms, and even gender to extract the correct linguistic base form of a word, known as ‚Äòlemma‚Äô."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71501e8",
   "metadata": {},
   "source": [
    "Better -> Good. Geese -> goose. Went -> Go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965b3d27",
   "metadata": {},
   "source": [
    "As we can see from above examples, lemmatization profoundly recognizes and accurately transforms words into their dictionary or base form, considering their tenses, their plurality, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85498464",
   "metadata": {},
   "source": [
    "This can‚Äôt be achieved with stemming as stemming is merely ‚ÄúChopping off‚Äù words rather than considering dictionary at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f17890f",
   "metadata": {},
   "source": [
    "# Quick library note: NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd69c2a1",
   "metadata": {},
   "source": [
    "Going forward, we‚Äôll use NLTK a lot. NLTK is short for Natural Language Toolkit, a python library that has a lot of functionality to work with NLP in Python. You can use this library for lots of thing such as removing stopwords, tokenizing, stemming, lemmatizing, and more. You can learn more on https://www.nltk.org/ and check what capabilities that this library has by checking https://www.nltk.org/py-modindex.html ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0d5b22",
   "metadata": {},
   "source": [
    "# Stemming in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223b15a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Stemming\n",
    "# Import the necessary libraries\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download required datasets from nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "text = \"The striped bats were hanging on their feet and eating best batches of juicy leeches\" #@param {type: \"string\"}\n",
    "\n",
    "# Tokenize the text\n",
    "token_list = word_tokenize(text)\n",
    "\n",
    "# Apply stemming on the tokens\n",
    "stemmed_output = ' '.join([stemmer.stem(token) for token in token_list])\n",
    "\n",
    "print(text)\n",
    "print(stemmed_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06b08a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
    "[nltk_data]   Unzipping tokenizers/punkt.zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abe25b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "the stripe bat were hang on their feet and eat best batch of juici leech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baf96d1",
   "metadata": {},
   "source": [
    "# Lemmatization In Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7784213",
   "metadata": {},
   "source": [
    "The process of lemmatization is a little bit more complex than stemming because we need every words ‚ÄúPOS tag‚Äù to make sure that the lemmatization lemmatize to the correct part of speech."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb8abb5",
   "metadata": {},
   "source": [
    "# POS (Part of speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65b6450",
   "metadata": {},
   "source": [
    "Part of speech is as simple as asking to each words: Is it a noun? Is it a verb? Is it an adjective? Etc. This helps in making sure that every word converted to the correct lemma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ce0f5e",
   "metadata": {},
   "source": [
    "Of course, different from stemming, for lemmatization to work correctly we must ensure that our input still contains stopwords to ensure the POS is correct. So if you want to do lemmatization ensure that POS is done before removing all stopwords, or removing any words at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d9bace",
   "metadata": {},
   "source": [
    "Below is the code for lemmatization, feel free to change the input text to any sentence that you want to see lemmatization on play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dba3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title POS\n",
    "# Import the necessary libraries\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download required datasets from nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def get_human_readable_pos(treebank_tag):\n",
    "    \"\"\"Map `treebank_tag` to equivalent human readable POS tag.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return \"Adjective\"\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return \"Verb\"\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return \"Noun\"\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return \"Adverb\"\n",
    "    else:\n",
    "        return \"Others\"\n",
    "\n",
    "text = \"The striped bats were hanging on their feet and eating best batches of juicy leeches\" # @param {text: \"string\"}\n",
    "\n",
    "# Tokenize the text\n",
    "token_list = word_tokenize(text)\n",
    "\n",
    "# POS tagging on the tokens\n",
    "pos_tokens = pos_tag(token_list)\n",
    "\n",
    "# Print word with its POS tag\n",
    "for word, pos in pos_tokens:\n",
    "    print(f\"{word} : {get_human_readable_pos(pos)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a04f8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "The : Others\n",
    "striped : Adjective\n",
    "bats : Noun\n",
    "were : Verb\n",
    "hanging : Verb\n",
    "on : Others\n",
    "their : Others\n",
    "feet : Noun\n",
    "and : Others\n",
    "eating : Verb\n",
    "best : Adjective\n",
    "batches : Noun\n",
    "of : Others\n",
    "juicy : Noun\n",
    "leeches : Noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735821fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
    "[nltk_data]     /root/nltk_data...\n",
    "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
    "[nltk_data]       date!\n",
    "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
    "[nltk_data]   Package punkt is already up-to-date!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e212aec",
   "metadata": {},
   "source": [
    "# Let‚Äôs lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9e9c0e",
   "metadata": {},
   "source": [
    "Now after POS tagging are done, we can pass the POS tagging along with every words to our lemmatization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f84b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Lemmatization\n",
    "# Import the necessary libraries\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "\n",
    "# Download required datasets from nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Map `treebank_tag` to equivalent WordNet POS tag.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # As default pos in lemmatization is Noun\n",
    "        return wordnet.NOUN\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text = \"The striped bats were hanging on their feet and eating best batches of juicy leeches\" #@param {text: \"string\"}\n",
    "\n",
    "# Tokenize the text\n",
    "token_list = word_tokenize(text)\n",
    "\n",
    "# POS tagging on the tokens\n",
    "pos_tokens = pos_tag(token_list)\n",
    "\n",
    "# Lemmatize with POS tagging\n",
    "lemmatized_output = ' '.join([lemmatizer.lemmatize(token, get_wordnet_pos(pos)) for token, pos in pos_tokens])\n",
    "\n",
    "print(lemmatized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe088c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
    "[nltk_data]     /root/nltk_data...\n",
    "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
    "[nltk_data]       date!\n",
    "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
    "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
    "[nltk_data]   Package punkt is already up-to-date!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cf4ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "The striped bat be hang on their foot and eat best batch of juicy leech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6ec51",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab5bfef",
   "metadata": {},
   "source": [
    "Tokenization is one of the latest part of preprocessing in NLP. The definition is simple: It‚Äôs a process to breakdown our preprocessed words into array of features that already preprocessed so we can feed it to our process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc08cde5",
   "metadata": {},
   "source": [
    "Why we called it features? For our current architecture, a feature is basically a single pre-processed word. But later when we‚Äôre using neural networks, a feature might be refer to sub-words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34fa6b3",
   "metadata": {},
   "source": [
    "Words such as ‚Äúeating‚Äù, when we tokenized into sub-words, might be tokenized into something like ‚Äúeat-ing‚Äù. But sub-words as features mostly held place when we need semantic relation between words, but for learning how NLP works without neural network it‚Äôs basically harder and mostly we can just refer to neural network for tasks that require these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4c4cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Basic tokenization\n",
    "# Import required library\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample text\n",
    "text = \"This is an example sentence for basic tokenization.\" #@param {text:\"string\"}\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Output the tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9d7969",
   "metadata": {},
   "outputs": [],
   "source": [
    "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
    "[nltk_data]   Unzipping tokenizers/punkt.zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492f5f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "['This', 'is', 'an', 'example', 'sentence', 'for', 'basic', 'tokenization', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa5ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Sub-words tokenizer that is used by BERT model\n",
    "# Import required library\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the tokenizer with a pretrained model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Sample text\n",
    "text = \"The striped bats were hanging on their feet and eating best batches of juicy leeches\" #@param\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Output the tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575fada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "['the', 'striped', 'bats', 'were', 'hanging', 'on', 'their', 'feet', 'and', 'eating', 'best', 'batch', '##es', 'of', 'juicy', 'lee', '##ches']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0b98b9",
   "metadata": {},
   "source": [
    "As you can see above that some of the word is splitted to sub-words: ‚Äúbatch + ##es‚Äù and ‚Äúlee + ##ches‚Äù. What to split to subwords is depend on the task at hand of course, and for BERT cases, lot‚Äôs of verb still considered a single token rather that splitting it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e11b8d5",
   "metadata": {},
   "source": [
    "# How our model will understand which sentiment to assign our tweet to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e50facd",
   "metadata": {},
   "source": [
    "Let‚Äôs get a while back and try to understand below tweet:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cfcad3",
   "metadata": {},
   "source": [
    "I‚Äôm really excited towards tomorrow for our shop opening, see you guys at xxx!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f663a78a",
   "metadata": {},
   "source": [
    "How can we conclude that tweet is positive again? It‚Äôs because it‚Äôs having the word excited, as we know that the word excited are more likely hinting to a sentence that is positive, but is unlikely to be existing on a sentence that is negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeb6eee",
   "metadata": {},
   "source": [
    "So how can a model know, especially when we‚Äôre not doing deep-learning, how to differentiate a sentiment of a tweet? By checking if a sentence containing words that give hint towards one of the sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e2ac9d",
   "metadata": {},
   "source": [
    "# Excited are unlikely to be occuring on negative tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8d1d6a",
   "metadata": {},
   "source": [
    "So how can we teach a machine that certain words should give a great hint that a tweet is positive while certain words can give a great hint for otherwise?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f9fcd7",
   "metadata": {},
   "source": [
    "We can of course just feed ‚Äúexcited‚Äù, ‚Äúhappy‚Äù, ‚Äúsad‚Äù, etc, then tag them to be one way or another, but imagine if we don‚Äôt have the dictionary for all positive words and negative words, how can we compile them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639f7e2a",
   "metadata": {},
   "source": [
    "‚ÄúExcited are unlikely to be occuring on negative tweet‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645b61be",
   "metadata": {},
   "source": [
    "So if we can gather lots of tweets that already tagged as positive and negative, we can compile every word that are positive by checking all positive tweets and if there are lots of tweets that hinting that this word is positive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d120dca1",
   "metadata": {},
   "source": [
    "‚ÄúWord like ‚Äòtechnology‚Äô can be occuring on positive and negative tweet, and shouldn‚Äôt affect a sentiment‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc28e8f",
   "metadata": {},
   "source": [
    "But don‚Äôt forget that some words are neutral. It depends on your dataset, but let‚Äôs say if we‚Äôre scraping tweets from tech reviewer, the word ‚Äútechnology‚Äù would appear on positive sentiment, while still of course shown on negative sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4feefd",
   "metadata": {},
   "source": [
    "So our first formula might be: - If a word often shown on a sentiment, it might be hinting that it‚Äôs classify as that sentiment - But if a word geared towards both sentiment, it most likely hinting that it‚Äôs a neutral word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385f7ed5",
   "metadata": {},
   "source": [
    "Above concept will be our baseline to understand two methods of Feature Extraction: Bag-of-words, and TF-IDF, which we‚Äôll learn in our next session!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6c835a",
   "metadata": {},
   "source": [
    "# Isn‚Äôt NLP exciting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bc1be3",
   "metadata": {},
   "source": [
    "There are lots of challenge when it comes to a task as simple as sentiment analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bf2ee7",
   "metadata": {},
   "source": [
    "For sentence like that, we have to make sure that our model knowing to use that ‚Äúnot‚Äù and negate anything after"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42acd8e9",
   "metadata": {},
   "source": [
    "For sarcasm, it‚Äôs a whole another level. How to solve something like that? How a model can know which sentence is sarcasm, and which are not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc177f5a",
   "metadata": {},
   "source": [
    "Of course we won‚Äôt give you the answer right away üòõ, stay tune and stay curious!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
