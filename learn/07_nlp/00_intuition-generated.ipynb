{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8ef36f3",
   "metadata": {},
   "source": [
    "source: [link](https://ai-bootcamp.ruangguru.com/learn/07_nlp/00_intuition.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c33102",
   "metadata": {},
   "source": [
    "# Intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78770bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b5840c",
   "metadata": {},
   "source": [
    "Welcome to NLP learning, we‚Äôll start warming up with just mostly about intuition that you should understand about how natural language processing works, what is preprocessing and how to preprocess our data, then we‚Äôll see the intuition about how several architecture works behind the scene, and much more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9b1d9e",
   "metadata": {},
   "source": [
    "# The Black Box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b7f978",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93783966",
   "metadata": {},
   "source": [
    "![Image](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/output_IVkit2X9C.gif?updatedAt=1695021798371)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3982f18",
   "metadata": {},
   "source": [
    "Natural language processing is a concept of understanding ‚Äúnatural language‚Äù ( which basically means ‚Äúhuman language‚Äù) and making sure a machine can understand that ‚Äúnatural language‚Äù. But how can a machine understand humand language, let alone answering it like ChatGPT above?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac65ac2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0457024",
   "metadata": {},
   "source": [
    "![Image](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/image_6vrPLsRVx.png?updatedAt=1695020969115)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa507a6c",
   "metadata": {},
   "source": [
    "# Machine only understand numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10ccc66",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "320270cc",
   "metadata": {},
   "source": [
    "![Image](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/image_F42l4buMr.png?updatedAt=1695027219913)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380694a5",
   "metadata": {},
   "source": [
    "We‚Äôve learned machine learning so far, and what is one of the most oversimplification but kinda true about machine learning? When it comes to inferring, it‚Äôs basically just a giant calculator that keep doing matrix multiplication, activation, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e6a4d6",
   "metadata": {},
   "source": [
    "So when we‚Äôre facing our computer to learn about language, we need to convert them into numbers first. For that later we‚Äôll learn about:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cf57ac",
   "metadata": {},
   "source": [
    "After the ‚ÄúMagic‚Äù process (that of course we‚Äôll learn more later), we‚Äôll need to convert the output process back to human language. This process of converting numbers back to human language is mostly using fully-connected layer, things that we have learned again and again, not that complex right? üòâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72e0664",
   "metadata": {},
   "source": [
    "# The process of answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d111c367",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dad0cbde",
   "metadata": {},
   "source": [
    "![Image](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/image_XE289cX9B.png?updatedAt=1695027950063)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b95b21",
   "metadata": {},
   "source": [
    "Now let‚Äôs uncover more ‚Äúmagic‚Äù. In general the magic itself can be divided into two categories: Encoder (The part of where the model try to understanding the question) and decoder (The part where the model finding the answer and converting the answer which previously only can be understand by our modal back to human words)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e3fc9f",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8a4560",
   "metadata": {},
   "source": [
    "Encoder‚Äôs position in NLP is to ‚Äúencode context from human input‚Äù, which translates to ‚Äúmaking human language to make it digestable by our model. So for lots of architecture -despite the modern architecture that lacking encoder layer- encoder is a part of‚Äùconverting‚Äù input to a vector, array of numbers that can be processed further after that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a82589d",
   "metadata": {},
   "source": [
    "So a sentence can be summarized into an array of numbers that later can be processed further by the machine, but if zoomed in further even every single words can be summarized into an array of numbers, which is called ‚ÄúWord embedding‚Äù which we‚Äôll learn right after this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18a8cc7",
   "metadata": {},
   "source": [
    "A sentence have lots of words, every single words have their own array of numbers that can be understood by a machine, the encoder is a way to summarize those words into another array of numbers that encapsulate the whole sentence meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bfaa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Encoded text\n",
    "from transformers import BartTokenizer, BartModel\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "model = BartModel.from_pretrained('facebook/bart-large')\n",
    "\n",
    "# Input text\n",
    "text = \"Hey, how are you doing today?\" #@param\n",
    "\n",
    "# Encode text\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# Get embeddings\n",
    "embeddings = model(**encoded_input)['last_hidden_state']\n",
    "\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9714c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor([[[ 0.3529,  0.5034, -1.6365,  ...,  0.4680,  0.7314,  0.4178],\n",
    "         [ 0.3529,  0.5034, -1.6365,  ...,  0.4680,  0.7314,  0.4178],\n",
    "         [ 1.7981,  1.9646, -0.3734,  ...,  0.1334, -0.3966, -0.4826],\n",
    "         ...,\n",
    "         [-0.1854,  4.0069, -0.2969,  ...,  0.2142,  0.0490,  0.8782],\n",
    "         [ 0.2307,  4.7543, -0.8947,  ...,  1.2735,  1.1160, -0.8490],\n",
    "         [-0.4442,  1.4159, -0.4588,  ...,  0.8986,  0.8376,  0.3417]]],\n",
    "       grad_fn=<NativeLayerNormBackward0>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3959eb",
   "metadata": {},
   "source": [
    "# NLP model == Complex calculator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e9571b",
   "metadata": {},
   "source": [
    "If you run above code, you‚Äôll see that above sentence are translated into numbers that can later be processed further into our model. So by seeing array of matrix above hopefully you can be more understand that the basis of NLP is basically lots and lots of matrix operation: It‚Äôs basically a really complex calculator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678880ad",
   "metadata": {},
   "source": [
    "# Word embedding - Where every words containing a lot of contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a614ba6c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81847734",
   "metadata": {},
   "source": [
    "![Image](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/image_fCEXzZbEY.png?updatedAt=1695050596556)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab96d3ae",
   "metadata": {},
   "source": [
    "Every single words in NLP refer to a word embedding, a word embedding basically an array of numbers that represent the word context, that every single word embedding can contains hundreds or thousands of dimension that are containing different ‚Äúcontext‚Äù."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c7b689",
   "metadata": {},
   "source": [
    "So word like ‚Äúhorse‚Äù for example, have a word embedding that refer to that exact word that more or less look like: ‚Äú[0, 3213, 732 ..,943]‚Äù."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fef6c7",
   "metadata": {},
   "source": [
    "To not be ahead of ourselves, consider when we want to complete below sentence:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6908ee6",
   "metadata": {},
   "source": [
    "Andi walks into his own ___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b212d8",
   "metadata": {},
   "source": [
    "When we‚Äôre seeing above sentence we might talk more about: - Places - Vehicles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63938af7",
   "metadata": {},
   "source": [
    "So we‚Äôll find in words that contains high value in dimension that refer to that. We might find that the word book, glass, microphone, ear, eye, is really unlikely to be the answer of above sentence, as it‚Äôs not related to places, or vehicles, so the scores should be really low or even negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedae5cd",
   "metadata": {},
   "source": [
    "But for the words like garden, car, house, would be more likely to complete above sentence, so the score would be higher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cdd8a4",
   "metadata": {},
   "source": [
    "So in NLP, multidimensionality refers to every word have their own ‚Äúscore-card‚Äù which behave like scoring system how a dimension reflect to certain context so when we need an answer we can just ‚Äúfound them in the referred context‚Äù."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd09623",
   "metadata": {},
   "source": [
    "Of course it‚Äôs a really high-level intuition which we‚Äôll learn more about when we touch word embedding üòÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac8d390",
   "metadata": {},
   "source": [
    "Heads up: In reality word-embedding might not only be a single word have single embedding, but can be divided more to ‚Äúsub-words‚Äù, which is ‚Äúparts-of-words‚Äù, a word divided to several parts such as ‚Äúhiking‚Äù can be divided to ‚Äúhike-ing‚Äù."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80aaf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Get a word embedding\n",
    "import os\n",
    "import numpy as np\n",
    "import requests, zipfile, io\n",
    "\n",
    "def download_and_unzip_embeddings(url, directory):\n",
    "    print(f'Downloading and unzipping embeddings...')\n",
    "    r = requests.get(url)\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall(path=directory)\n",
    "\n",
    "def load_glove_embeddings(path, url):\n",
    "    # If file doesn't exist, download and unzip it\n",
    "    if not os.path.isfile(path):\n",
    "        download_and_unzip_embeddings(url, path.rsplit('/', 1)[0])\n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "        embeddings = {}\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "        return embeddings\n",
    "\n",
    "# URL of GloVe embeddings and the path - replace with your actual URL\n",
    "url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "path = 'glove.6B/glove.6B.50d.txt'\n",
    "\n",
    "embeddings = load_glove_embeddings(path, url)\n",
    "\n",
    "# To get a word's embedding\n",
    "word = 'computer' #@param\n",
    "embedding_vector = embeddings.get(word)\n",
    "\n",
    "# If the word doesn't exist in the dictionary, `get` method will return None.\n",
    "if embedding_vector is not None:\n",
    "    print(embedding_vector)\n",
    "else:\n",
    "    print(f\"'{word}' not found in the dictionary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00793b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "[ 0.079084 -0.81504   1.7901    0.91653   0.10797  -0.55628  -0.84427\n",
    " -1.4951    0.13418   0.63627   0.35146   0.25813  -0.55029   0.51056\n",
    "  0.37409   0.12092  -1.6166    0.83653   0.14202  -0.52348   0.73453\n",
    "  0.12207  -0.49079   0.32533   0.45306  -1.585    -0.63848  -1.0053\n",
    "  0.10454  -0.42984   3.181    -0.62187   0.16819  -1.0139    0.064058\n",
    "  0.57844  -0.4556    0.73783   0.37203  -0.57722   0.66441   0.055129\n",
    "  0.037891  1.3275    0.30991   0.50697   1.2357    0.1274   -0.11434\n",
    "  0.20709 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6b4063",
   "metadata": {},
   "source": [
    "As we see above, a ‚Äúcomputer‚Äù can be embedded into numbers that each dimension contain some context of ‚Äúcomputer‚Äù. To show above example we use 50 dimension from word embedding named ‚ÄúGloVe‚Äù, but in reality even in GloVe they have variation of number of dimensions like 100 dimensions, 200 dimensions, even 300 dimensions, when the more dimension can contain more context of a word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8041e6",
   "metadata": {},
   "source": [
    "# One word at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b39879c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cf63068",
   "metadata": {},
   "source": [
    "![Image](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/output_IVkit2X9C.gif?updatedAt=1695021798371)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815e5847",
   "metadata": {},
   "source": [
    "When we‚Äôre talking about question and answering like GPT model, we can see that our input is being answered one at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da4adfa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d970d178",
   "metadata": {},
   "source": [
    "![Image](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/image_Etk8eVNLg.png?updatedAt=1695051947700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5402adae",
   "metadata": {},
   "source": [
    "It‚Äôs simply because decoder mostly work at one-word-at-a-time. So after they got their encoded input, it will process what output that it thinks the best, one-word-at-a-time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb07fc7",
   "metadata": {},
   "source": [
    "After a single word is finished, the model will check if it‚Äôs the last word or not, if not, then the next decoder process is continue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c57aef",
   "metadata": {},
   "source": [
    "# End-to-end Process Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aeab11",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67b028c4",
   "metadata": {},
   "source": [
    "![Image](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/image_Ya5YuwbHF.png?updatedAt=1695076739564)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee7f102",
   "metadata": {},
   "source": [
    "So above diagram is a really high overview of NLP‚Äôs intuition. Our journey in learning NLP, most of it can be categorized into any of above steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755ae987",
   "metadata": {},
   "source": [
    "Remember that some of NLP architecture only use above categorization until ‚ÄúUndestanding the question‚Äù."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb93e79",
   "metadata": {},
   "source": [
    "In more classic architecture the concept of encoder isn‚Äôt really exist, so the concept of ‚ÄúUnderstanding the question‚Äù won‚Äôt use encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083c661d",
   "metadata": {},
   "source": [
    "In modern architecture there‚Äôs a concept called ‚ÄúSelf-Attention‚Äù that even can skip the whole concept of ‚ÄúEncoder‚Äù and just straight to ‚ÄúDecoder‚Äù."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
